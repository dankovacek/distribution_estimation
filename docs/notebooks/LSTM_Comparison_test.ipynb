{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "558cd32f-b562-4502-ab63-826aede8f321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"c6c7217e-7ddc-4287-b3dd-90407f29bc7e\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"c6c7217e-7ddc-4287-b3dd-90407f29bc7e\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.7.3.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"c6c7217e-7ddc-4287-b3dd-90407f29bc7e\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import xyzservices.providers as xyz\n",
    "from scipy.stats import linregress\n",
    "\n",
    "from bokeh.plotting import figure, output_file, save, show\n",
    "from bokeh.layouts import gridplot, row, column\n",
    "\n",
    "# from bokeh.models import ColumnDataSource, LinearAxis, Range1d, HoverTool, Div\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import Div\n",
    "# from bokeh.palettes import Sunset10, Vibrant7, Category20, Bokeh6, Bokeh7, Bokeh8, Greys256, Blues256\n",
    "\n",
    "# from shapely.geometry import Polygon, Point\n",
    "# from shapely.ops import unary_union\n",
    "# from scipy.spatial import Voronoi\n",
    "\n",
    "# from kde_estimator import KDEEstimator\n",
    "from fdc_estimator_context import FDCEstimationContext\n",
    "from fdc_data import StationData\n",
    "\n",
    "import data_processing_functions as dpf\n",
    "\n",
    "import xyzservices.providers as xyz\n",
    "tiles = xyz['USGS']['USTopo']\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9525e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Watershed_ID</th>\n",
       "      <th>Source</th>\n",
       "      <th>Name</th>\n",
       "      <th>Official_ID</th>\n",
       "      <th>Centroid_Lat_deg_N</th>\n",
       "      <th>Centroid_Lon_deg_E</th>\n",
       "      <th>Drainage_Area_km2</th>\n",
       "      <th>Drainage_Area_GSIM_km2</th>\n",
       "      <th>Flag_GSIM_boundaries</th>\n",
       "      <th>Flag_Artificial_Boundaries</th>\n",
       "      <th>...</th>\n",
       "      <th>Land_Use_Wetland_frac</th>\n",
       "      <th>Land_Use_Water_frac</th>\n",
       "      <th>Land_Use_Urban_frac</th>\n",
       "      <th>Land_Use_Shrubs_frac</th>\n",
       "      <th>Land_Use_Crops_frac</th>\n",
       "      <th>Land_Use_Snow_Ice_frac</th>\n",
       "      <th>Flag_Land_Use_Extraction</th>\n",
       "      <th>Permeability_logk_m2</th>\n",
       "      <th>Porosity_frac</th>\n",
       "      <th>Flag_Subsoil_Extraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>847</td>\n",
       "      <td>HYDAT</td>\n",
       "      <td>CROWSNEST RIVER AT FRANK</td>\n",
       "      <td>05AA008</td>\n",
       "      <td>49.59732</td>\n",
       "      <td>-114.4106</td>\n",
       "      <td>402.6522</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>0.0785</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>1</td>\n",
       "      <td>-15.543306</td>\n",
       "      <td>0.170479</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>850</td>\n",
       "      <td>HYDAT</td>\n",
       "      <td>CASTLE RIVER NEAR BEAVER MINES</td>\n",
       "      <td>05AA022</td>\n",
       "      <td>49.48866</td>\n",
       "      <td>-114.1444</td>\n",
       "      <td>820.6510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.1156</td>\n",
       "      <td>0.0246</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>-15.929747</td>\n",
       "      <td>0.150196</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Watershed_ID Source                            Name Official_ID  \\\n",
       "846           847  HYDAT        CROWSNEST RIVER AT FRANK     05AA008   \n",
       "849           850  HYDAT  CASTLE RIVER NEAR BEAVER MINES     05AA022   \n",
       "\n",
       "     Centroid_Lat_deg_N  Centroid_Lon_deg_E  Drainage_Area_km2  \\\n",
       "846            49.59732           -114.4106           402.6522   \n",
       "849            49.48866           -114.1444           820.6510   \n",
       "\n",
       "     Drainage_Area_GSIM_km2  Flag_GSIM_boundaries  Flag_Artificial_Boundaries  \\\n",
       "846                     NaN                     0                           0   \n",
       "849                     NaN                     0                           0   \n",
       "\n",
       "     ...  Land_Use_Wetland_frac  Land_Use_Water_frac  Land_Use_Urban_frac  \\\n",
       "846  ...                 0.0103               0.0065               0.0328   \n",
       "849  ...                 0.0058               0.0023               0.0105   \n",
       "\n",
       "     Land_Use_Shrubs_frac  Land_Use_Crops_frac  Land_Use_Snow_Ice_frac  \\\n",
       "846                0.0785               0.0015                  0.0002   \n",
       "849                0.1156               0.0246                  0.0000   \n",
       "\n",
       "     Flag_Land_Use_Extraction  Permeability_logk_m2  Porosity_frac  \\\n",
       "846                         1            -15.543306       0.170479   \n",
       "849                         1            -15.929747       0.150196   \n",
       "\n",
       "     Flag_Subsoil_Extraction  \n",
       "846                        1  \n",
       "849                        1  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_DIR = Path(os.getcwd())\n",
    "\n",
    "attr_fpath = 'data/BCUB_watershed_attributes_updated_20250227.csv'\n",
    "attr_df = pd.read_csv(attr_fpath, dtype={'Official_ID': str})\n",
    "station_ids = sorted(attr_df['official_id'].unique().tolist())\n",
    "\n",
    "# streamflow folder from (updated) HYSETS\n",
    "HYSETS_DIR = Path('/home/danbot/code/common_data/HYSETS')\n",
    "hs_df = pd.read_csv('data/HYSETS_watershed_properties.txt', sep=';')\n",
    "hs_df = hs_df[hs_df['Official_ID'].isin(station_ids)]\n",
    "hs_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1356ccc6-3d6d-4e08-bdbc-53a798187d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 723 monitored basins concurrent with LSTM ensemble results.\n"
     ]
    }
   ],
   "source": [
    "# retrieve LSTM ensemble predictions\n",
    "lstm_result_base_folder = Path('/home/danbot/code/neuralhydrology/data/')\n",
    "results_revisions = ['20250514', '20250627']\n",
    "lstm_result_files = os.listdir(lstm_result_base_folder / f'ensemble_results_{results_revisions[0]}')\n",
    "lstm_result_stns = [e.split('_')[0] for e in lstm_result_files]\n",
    "\n",
    "# filter for the common stations between BCUB region and LSTM-compatible (i.e. 1980-)\n",
    "daymet_concurrent_stations = list(set(station_ids) & set(lstm_result_stns))\n",
    "# assert '012414900' in daymet_concurrent_stations\n",
    "print(f'There are {len(daymet_concurrent_stations)} monitored basins concurrent with LSTM ensemble results.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85045132",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create dicts for easier access to 'official_id': 'drainage area', geometry, regulation status\n",
    "da_dict = attr_df[['official_id', 'drainage_area_km2']].copy().set_index('official_id').to_dict()['drainage_area_km2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9c026ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "watershed_id_dict = {row['Watershed_ID']: row['Official_ID'] for _, row in hs_df.iterrows()}\n",
    "# and the inverse\n",
    "official_id_dict = {row['Official_ID']: row['Watershed_ID'] for _, row in hs_df.iterrows()}\n",
    "# also for drainage areas\n",
    "da_dict = {row['Official_ID']: row['Drainage_Area_km2'] for _, row in hs_df.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b7a4e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_filter_hysets_data(station_ids, hs_df):\n",
    "    hs_df = hs_df[hs_df['Official_ID'].isin(station_ids)]\n",
    "\n",
    "    # load the updated HYSETS data\n",
    "    updated_filename = 'HYSETS_2023_update_QC_stations.nc'\n",
    "    ds = xr.open_dataset(HYSETS_DIR / updated_filename)\n",
    "\n",
    "    # Get valid IDs as a NumPy array\n",
    "    selected_ids = hs_df['Watershed_ID'].values\n",
    "\n",
    "    # Get boolean index where watershedID in selected_set\n",
    "    # safely access watershedID as a variable first\n",
    "    ws_ids = ds['watershedID'].data  # or .values if you prefer\n",
    "    mask = np.isin(ws_ids, selected_ids)\n",
    "\n",
    "    # Apply mask to data\n",
    "    ds = ds.sel(watershed=mask)\n",
    "    # Step 1: Promote 'watershedID' to a coordinate on the 'watershed' dimension\n",
    "    ds = ds.assign_coords(watershedID=(\"watershed\", ds[\"watershedID\"].data))\n",
    "\n",
    "    # Step 2: Set 'watershedID' as the index for the 'watershed' dimension\n",
    "    return ds.set_index(watershed=\"watershedID\")\n",
    "\n",
    "\n",
    "ds = load_and_filter_hysets_data(station_ids, hs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc287104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-12.02375108873622 10.819778284410283 0.005578395451317775 0.005578395451315998\n"
     ]
    }
   ],
   "source": [
    "def set_grid(global_min, global_max, n_grid_points=2**12):\n",
    "    # self.baseline_log_grid = np.linspace(np.log(adjusted_min_uar), np.log(max_uar), self.n_grid_points)\n",
    "    baseline_log_grid = np.linspace(\n",
    "        global_min, global_max, n_grid_points\n",
    "    )\n",
    "    baseline_lin_grid = np.exp(baseline_log_grid)\n",
    "    log_dx = np.gradient(baseline_log_grid)\n",
    "    max_step_size = baseline_lin_grid[-1] - baseline_lin_grid[-2]\n",
    "    # print(f'    max step size = {max_step_size:.1f} L/s/km^2 for n={n_grid_points:.1e} grid points')\n",
    "    min_step_size = baseline_lin_grid[1] - baseline_lin_grid[0]\n",
    "    # print(f'    min step size = {min_step_size:.2e} L/s/km^2 for n={n_grid_points:.1e} grid points')\n",
    "    return baseline_lin_grid, baseline_log_grid, log_dx\n",
    "\n",
    "_, baseline_log_grid, log_dx = set_grid(np.log(6e-6), np.log(5e4), n_grid_points=2**12)\n",
    "print(baseline_log_grid[0], baseline_log_grid[-1], log_dx[0], log_dx[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76c2cea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the complete years previously processed\n",
    "import json\n",
    "with open('data/complete_years.json', 'r') as f:\n",
    "    complete_year_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ce3b3b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from kde_estimator import KDEEstimator\n",
    "from scipy.stats import wasserstein_distance\n",
    "from jax import numpy as jnp\n",
    "\n",
    "def compute_kl_divergence(p, q):\n",
    "    \"\"\"Compute the KL divergence between two probability distributions.\"\"\"\n",
    "    mask = (p > 0) & (q > 0)  # Avoid log(0)\n",
    "    return jnp.sum(jnp.where(mask, p * jnp.log(p / q), 0.0))\n",
    "\n",
    "\n",
    "def compute_emd(p, q, baseline_log_grid):\n",
    "    assert np.isclose(np.sum(p), 1, atol=1e-3), f'sum P = {np.sum(p)}'\n",
    "    assert np.all(q >= 0), f'min q_i < 0: {np.min(q)}'\n",
    "    linear_grid = np.exp(baseline_log_grid)\n",
    "    emd = wasserstein_distance(linear_grid, linear_grid, p, q)\n",
    "    return float(round(emd, 4))#, {'bias': None, 'unsupported_mass': None, 'pct_of_signal': None}\n",
    "\n",
    "\n",
    "def compute_nse(obs, sim):\n",
    "    \"\"\"Compute the Nash-Sutcliffe Efficiency (NSE) between observed and simulated values.\"\"\"\n",
    "    assert not np.isnan(obs).any(), f'NaN values in obs: {obs}'\n",
    "    assert not np.isnan(sim).any(), f'NaN values in sim: {sim}'\n",
    "    assert (obs >= 0).all(), f'Negative values in obs: {obs}'\n",
    "    assert (sim >= 0).all(), f'Negative values in sim: {sim}'\n",
    "    # Compute the NSE\n",
    "    numerator = np.sum((obs - sim) ** 2)\n",
    "    denominator = np.sum((obs - obs.mean()) ** 2)\n",
    "    nse = 1 - (numerator / denominator)\n",
    "    return nse\n",
    "\n",
    "\n",
    "def compute_relative_error(obs, sim):\n",
    "    \"\"\"Compute the relative error between observed and simulated values.\"\"\"\n",
    "    assert not np.isnan(obs).any(), f'NaN values in obs: {obs}'\n",
    "    assert not np.isnan(sim).any(), f'NaN values in sim: {sim}'\n",
    "    assert (obs >= 0).all(), f'Negative values in obs: {obs}'\n",
    "    assert (sim >= 0).all(), f'Negative values in sim: {sim}'\n",
    "    # Compute the relative error\n",
    "    return (obs - sim) / obs\n",
    "\n",
    "\n",
    "def compute_RMSE(obs, sim):\n",
    "    \"\"\"Compute the Root Mean Square Error (RMSE) between observed and simulated values.\"\"\"\n",
    "    assert not np.isnan(obs).any(), f'NaN values in obs: {obs}'\n",
    "    assert not np.isnan(sim).any(), f'NaN values in sim: {sim}'\n",
    "    assert (obs >= 0).all(), f'Negative values in obs: {obs}'\n",
    "    assert (sim >= 0).all(), f'Negative values in sim: {sim}'\n",
    "    # Compute the RMSE\n",
    "    return np.sqrt(np.mean((obs - sim) ** 2))\n",
    "\n",
    "\n",
    "def compute_KGE(obs, sim):\n",
    "    \"\"\"Compute the Kling-Gupta Efficiency (KGE) between observed and simulated values.\"\"\"\n",
    "    assert not np.isnan(obs).any(), f\"NaN values in obs: {obs}\"\n",
    "    assert not np.isnan(sim).any(), f\"NaN values in sim: {sim}\"\n",
    "    assert (obs >= 0).all(), f\"Negative values in obs: {obs}\"\n",
    "    assert (sim >= 0).all(), f\"Negative values in sim: {sim}\"\n",
    "    if obs.size == 0:\n",
    "        return np.nan\n",
    "    # Compute the KGE\n",
    "    r = np.corrcoef(obs, sim)[0, 1]\n",
    "    alpha = sim.mean() / obs.mean()\n",
    "    beta = sim.std() / obs.std()\n",
    "    kge = 1 - np.sqrt((r - 1) ** 2 + (alpha - 1) ** 2 + (beta - 1) ** 2)\n",
    "    return kge\n",
    "\n",
    "\n",
    "def evaluate_fdc_metrics_from_pmf(pmf_est, baseline_pmf, baseline_log_grid):\n",
    "    \"\"\"\n",
    "    Evaluate RMSE, relative error, NSE, and KGE between two FDCs represented by discrete PMFs.\n",
    "    Note these are evaluated over the log_grid which is set in the context.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pmf_est : np.ndarray\n",
    "        Discrete PMF representing the estimated FDC, over `log_grid`.\n",
    "    log_grid : np.ndarray\n",
    "        Grid of log-transformed flow values corresponding to PMF bins.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary of RMSE, RelativeError, NSE, and KGE computed over p=1,...,99 quantiles.\n",
    "    \"\"\"\n",
    "    assert (\n",
    "        len(baseline_pmf) == len(pmf_est) == len(baseline_log_grid)\n",
    "    ), \"Array length mismatch\"\n",
    "\n",
    "    # Convert log flow grid back to linear runoff space\n",
    "    linear_grid = np.exp(baseline_log_grid)\n",
    "\n",
    "    # Compute CDFs\n",
    "    cdf_true = np.cumsum(baseline_pmf)\n",
    "    cdf_true /= cdf_true[-1]\n",
    "    cdf_est = np.cumsum(pmf_est)\n",
    "    cdf_est /= cdf_est[-1]\n",
    "    \n",
    "    assert np.isfinite(cdf_true).all(), \"Non-finite values in cdf_true\"\n",
    "    assert np.diff(cdf_true).sum() > 0, \"cdf_true has no spread\"\n",
    "\n",
    "    # Percentiles (1 to 99)\n",
    "    probs = np.linspace(0.01, 0.99, 99)\n",
    "\n",
    "    # Interpolate inverse CDF (flow values at given probabilities)\n",
    "    q_true = np.interp(\n",
    "        probs, cdf_true, linear_grid, left=linear_grid[0], right=linear_grid[-1]\n",
    "    )\n",
    "    q_est = np.interp(\n",
    "        probs, cdf_est, linear_grid, left=linear_grid[0], right=linear_grid[-1]\n",
    "    )\n",
    "    assert np.all(q_true > 0), \"Zero or negative values in q_true — invalid for relative error\"\n",
    "    assert np.all(q_est > 0), \"Zero or negative values in q_est — unexpected for flow\"\n",
    "\n",
    "    # Metrics\n",
    "    rmse = np.sqrt(np.mean((q_true - q_est) ** 2))\n",
    "    rel_error = np.mean(np.abs((q_est - q_true) / q_true))\n",
    "    nse = compute_nse(q_true, q_est)\n",
    "    kge = compute_KGE(q_true, q_est)\n",
    "\n",
    "    kld = compute_kl_divergence(baseline_pmf, pmf_est)\n",
    "    emd = compute_emd(baseline_pmf, pmf_est, baseline_log_grid)\n",
    "\n",
    "    return {\n",
    "        \"FDC_RMSE\": float(rmse), \n",
    "        \"FDC_RelativeError\": float(rel_error), \n",
    "        \"FDC_NSE\": float(nse), \n",
    "        \"FDC_KGE\": float(kge),\n",
    "        \"FDC_KLD\": float(kld),\n",
    "        \"FDC_EMD\": float(emd),\n",
    "    }\n",
    "\n",
    "\n",
    "def retrieve_timeseries_discharge(stn, ds):\n",
    "    watershed_id = official_id_dict[stn]\n",
    "    df = ds['discharge'].sel(watershed=str(watershed_id)).to_dataframe(name='discharge').reset_index()\n",
    "    df = df.set_index('time')[['discharge']]\n",
    "    df.dropna(inplace=True)\n",
    "    # clip minimum flow to 1e-4\n",
    "    df['discharge'] = np.clip(df['discharge'], 1e-4, None)\n",
    "    df.rename(columns={'discharge': stn}, inplace=True)\n",
    "    df[f'{stn}_uar'] = 1000 * df[stn] / da_dict[stn]\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compare_results_and_input(stn, sim_df, ds):\n",
    "    \"\"\"Compare the input streamflow timeseries with the observed streamflow timeseries.\n",
    "    Check that the dates in the output match the common dates between Daymet and the input data.\n",
    "    \"\"\"\n",
    "    input_df = retrieve_timeseries_discharge(stn, ds)\n",
    "    # clip the 'discharge' column to 1e-4, convert to unit area runoff (L/s/km2), and take the log\n",
    "    input_df = input_df[input_df.index >= '1980-01-01']\n",
    "\n",
    "    df = pd.concat([input_df, sim_df], axis=1, join='inner')\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    df['streamflow_obs'] = np.exp(df['streamflow_obs'])\n",
    "    sim_cols = [c for c in sim_df.columns if c.startswith('streamflow_sim')]\n",
    "    df[sim_cols] = np.exp(df[sim_cols])\n",
    "    # assert that the 'log_obs' and the 'streamflow_obs' columns are approximately equal\n",
    "\n",
    "    # set tolerance in the order of 1 L/s/km2\n",
    "    if not np.allclose(df[f'{stn}_uar'], df['streamflow_obs'], atol=1): \n",
    "        max_diff = np.abs(df[f'{stn}_uar'] - df['streamflow_obs']).max()\n",
    "        print(f'    Warning: {stn} has a max difference of {max_diff:.2f} between the input and output streamflow timeseries.')\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_ensemble_results(stn, folder):\n",
    "    \"\"\"Plot the ensemble results for a given station.\"\"\"\n",
    "    p = figure(title=f'Ensemble results for {stn}', x_axis_type='datetime', \n",
    "               y_axis_type='log', width=800, height=400)\n",
    "\n",
    "    for date, clr in zip(['20250514', '20250627'], ['black', 'red']):\n",
    "        fpath = folder / f'ensemble_results_{date}' / f'{stn}_ensemble.csv'\n",
    "        df = pd.read_csv(fpath)\n",
    "        df.rename(columns={'Unnamed: 0': 'time'}, inplace=True)\n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "        df.set_index('time', inplace=True)\n",
    "        df = np.exp(df)\n",
    "        if 'streamflow_obs' in df.columns:\n",
    "            p.line(df.index, df['streamflow_obs'], color=clr, legend_label=f'{date} Obs', line_width=2)\n",
    "        \n",
    "        sim_cols = [c for c in df.columns if c.startswith('streamflow_sim')]\n",
    "        mean_sim = df[sim_cols].mean(axis=1)\n",
    "        # compute the 5% quantiles on the simulation columns\n",
    "        lb = df[sim_cols].quantile(0.05, axis=1)\n",
    "        ub = df[sim_cols].quantile(0.95, axis=1)\n",
    "\n",
    "        p.varea(df.index, lb, ub, color=clr, alpha=0.2, legend_label=f'{date} 90% CI')\n",
    "        p.line(df.index, mean_sim, color=clr, legend_label=f'{date} Mean', line_dash='dashed', line_width=2)\n",
    "\n",
    "    p.legend.location = 'top_left'\n",
    "    p.xaxis.axis_label = 'Time'\n",
    "    p.yaxis.axis_label = 'Streamflow (L/s/km2)'\n",
    "    p.legend.click_policy= 'hide'\n",
    "    return p\n",
    "\n",
    "\n",
    "def filter_by_complete_years(stn, folder):\n",
    "    all_dfs = []\n",
    "    for date, clr in zip(['20250514', '20250627'], ['black', 'red']):\n",
    "        fpath = folder / f'ensemble_results_{date}' / f'{stn}_ensemble.csv'\n",
    "        if not os.path.exists(fpath):\n",
    "            return pd.DataFrame()\n",
    "        df = pd.read_csv(fpath)\n",
    "        df.rename(columns={'Unnamed: 0': 'time'}, inplace=True)\n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "        df.set_index('time', inplace=True)\n",
    "        df.columns = [f'{c}_{date}' for c in df.columns]\n",
    "        df = np.exp(df)\n",
    "        all_dfs.append(df)\n",
    "    result = pd.concat(all_dfs, axis=1, join='inner')\n",
    "    result = result.dropna(how='any', axis=0)\n",
    "    complete_years = complete_year_dict.get(stn, None).get('complete_years', [])\n",
    "    print(f'    Found {len(complete_years)} complete years for {stn}: {complete_years}')\n",
    "    return result[result.index.year.isin(complete_years)]\n",
    "\n",
    "\n",
    "def get_original_timeseries(stn, ds):\n",
    "    \"\"\"Retrieve the original timeseries for a given station.\"\"\"\n",
    "    watershed_id = official_id_dict[stn]\n",
    "    df = ds['discharge'].sel(watershed=str(watershed_id)).to_dataframe(name='discharge').reset_index()\n",
    "    df = df.set_index('time')[['discharge']]\n",
    "    df.dropna(inplace=True)\n",
    "    # clip minimum flow to 1e-4\n",
    "    df['discharge'] = np.clip(df['discharge'], 1e-4, None)\n",
    "    df.rename(columns={'discharge': stn}, inplace=True)\n",
    "    df[f'{stn}_uar'] = 1000 * df[stn] / da_dict[stn]\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_afdcs_from_sorted_flows(df, years, da, date):\n",
    "    \"\"\"Compute AFDCs from sorted daily flows, rather than PDFs.\"\"\"\n",
    "    \n",
    "    obs_cols = [c for c in df.columns if c.startswith('streamflow_obs') and c.endswith(date)]\n",
    "    assert len(obs_cols) == 1, f'Expected one observed column, found {len(obs_cols)}'\n",
    "    sim_cols = [c for c in df.columns if c.startswith('streamflow_sim') and c.endswith(date)]\n",
    "    assert len(sim_cols) == 10, f'Expected 10 simulated columns, found {len(sim_cols)}'\n",
    "\n",
    "    afdc_obs, afdc_sim = [], []\n",
    "\n",
    "    for year in years:\n",
    "        year_df = df[df.index.year == year]\n",
    "\n",
    "        # Observed\n",
    "        obs_values = year_df[obs_cols[0]].dropna().values\n",
    "        if len(obs_values) > 0:\n",
    "            sorted_obs = np.sort(obs_values)[::-1]  # descending\n",
    "            afdc_obs.append(pd.Series(sorted_obs, name=f\"{year}_obs\"))\n",
    "\n",
    "        # Simulated ensemble mean\n",
    "        sim_ensemble = year_df[sim_cols].dropna(how='all')  # drop rows with all NaNs\n",
    "        if not sim_ensemble.empty:\n",
    "            sim_mean = sim_ensemble.mean(axis=1).dropna().values\n",
    "            sorted_sim = np.sort(sim_mean)[::-1]\n",
    "            afdc_sim.append(pd.Series(sorted_sim, name=f\"{year}_sim\"))\n",
    "\n",
    "    # Align lengths: trim to shortest year\n",
    "    min_len = min(len(s) for s in afdc_obs + afdc_sim)\n",
    "    afdc_obs_trimmed = [s.iloc[:min_len].reset_index(drop=True) for s in afdc_obs]\n",
    "    afdc_sim_trimmed = [s.iloc[:min_len].reset_index(drop=True) for s in afdc_sim]\n",
    "\n",
    "    # Combine into DataFrames\n",
    "    obs_df = pd.concat(afdc_obs_trimmed, axis=1)\n",
    "    sim_df = pd.concat(afdc_sim_trimmed, axis=1)\n",
    "\n",
    "    # Compute percentile summary\n",
    "    afdc_summary = pd.DataFrame(index=np.arange(1, min_len + 1))\n",
    "    afdc_summary[f'AFDC50_obs_{date}'] = obs_df.median(axis=1)\n",
    "    afdc_summary[f'AFDC10_obs_{date}'] = obs_df.quantile(0.10, axis=1)\n",
    "    afdc_summary[f'AFDC90_obs_{date}'] = obs_df.quantile(0.90, axis=1)\n",
    "\n",
    "    afdc_summary[f'AFDC50_sim_{date}'] = sim_df.median(axis=1)\n",
    "    afdc_summary[f'AFDC10_sim_{date}'] = sim_df.quantile(0.10, axis=1)\n",
    "    afdc_summary[f'AFDC90_sim_{date}'] = sim_df.quantile(0.90, axis=1)\n",
    "\n",
    "    afdc_summary.index.name = 'Rank'\n",
    "    return afdc_summary\n",
    "\n",
    "\n",
    "def compute_ensemble_pmfs(df, sim_cols, kde, da):\n",
    "    \"\"\"Compute the frequency mean PMF for the simulated ensemble.\"\"\"\n",
    "    sim_ensemble_pmfs = []\n",
    "    for sim_col in sim_cols:\n",
    "        sim_vals = df[sim_col].dropna().values\n",
    "        assert len(sim_vals) > 0, f'No valid values found for {sim_col}'\n",
    "        sim_pmf, _ = kde.compute(sim_vals, da=da)\n",
    "        sim_ensemble_pmfs.append(pd.Series(sim_pmf, index=baseline_log_grid, name=sim_col))\n",
    "    # concatenate all PMFs and compute the mean\n",
    "    return pd.concat(sim_ensemble_pmfs, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ed302d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data_processing_functions as dpf\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "# ---------- Kernel Functions ----------\n",
    "\n",
    "@jax.jit\n",
    "def gaussian_kernel(u):\n",
    "    return jnp.exp(-0.5 * u**2) / jnp.sqrt(2 * jnp.pi)\n",
    "\n",
    "@jax.jit\n",
    "def epanechnikov_kernel(u):\n",
    "    return jnp.where(jnp.abs(u) <= 1, 0.75 * (1 - u**2), 0.0)\n",
    "\n",
    "@jax.jit\n",
    "def top_hat_kernel(u):\n",
    "    return jnp.where(jnp.abs(u) <= 1, 0.5, 0.0)\n",
    "\n",
    "# ---------- Bandwidth Strategies ----------\n",
    "def silverman_bandwidth(log_data: jnp.ndarray) -> float:\n",
    "    q75, q25 = jnp.percentile(log_data, jnp.array((75, 25)))\n",
    "    stdev = jnp.std(log_data)\n",
    "    A = jnp.min(jnp.array([stdev, (q75 - q25) / 1.34]))\n",
    "    return 1.06 * A / log_data.shape[0] ** 0.2\n",
    "\n",
    "\n",
    "def measurement_error_bandwidth_function(x: jnp.ndarray) -> jnp.ndarray:\n",
    "    error_points = jnp.array([1e-4, 1e-3, 1e-2, 1e-1, 1., 1e1, 1e2, 1e3, 1e4, 1e5])\n",
    "    error_values = jnp.array([1.0, 0.5, 0.2, 0.1, 0.1, 0.1, 0.1, 0.15, 0.2, 0.25])\n",
    "    return jnp.interp(x, error_points, error_values, left=1.0, right=0.25)\n",
    "\n",
    "\n",
    "def adaptive_bandwidths(uar: jnp.ndarray, da: float) -> jnp.ndarray:\n",
    "    flow_data = uar * da / 1000\n",
    "    unique_q = jnp.unique(flow_data)\n",
    "    \n",
    "    # compute the measurement error informed bandwidth\n",
    "    # units must be volumetric flow\n",
    "    error_model = measurement_error_bandwidth_function(unique_q)\n",
    "    unique_UAR = (1000 / da) * unique_q\n",
    "    upper_err_UAR = unique_UAR * (1 + error_model)\n",
    "    err_widths_UAR = jnp.log(upper_err_UAR) - jnp.log(unique_UAR)\n",
    "\n",
    "    # compute the basic Silverman bandwidth\n",
    "    # silverman_bw = silverman_bandwidth(jnp.log(unique_UAR))\n",
    "\n",
    "    # if there are not enough unique values, add a small amount of noise to the data\n",
    "    if len(unique_UAR) < 2:\n",
    "        print(f'    not enough unique values in runoff data ({len(unique_UAR)}), adding noise to the data according to the measurement error model.')\n",
    "        noise_bounds = (unique_UAR * (1 - error_model), unique_UAR * (1 + error_model))\n",
    "        flow_data += np.random.uniform(*noise_bounds, size=flow_data.shape)\n",
    "        unique_q = jnp.unique(flow_data)\n",
    "        unique_UAR = (1000 / da) * unique_q\n",
    "\n",
    "    # compute the log midpoints and bandwidths to address the issue\n",
    "    # of sparse data points in the log space\n",
    "    log_midpoints = jnp.log((unique_UAR[:-1] + unique_UAR[1:]) / 2)\n",
    "    left_mirror = unique_UAR[0] - (log_midpoints[0] - unique_UAR[0])\n",
    "    right_mirror = unique_UAR[-1] + (unique_UAR[-1] - log_midpoints[-1])\n",
    "    log_midpoints = jnp.concatenate((jnp.array([left_mirror]), log_midpoints, jnp.array([right_mirror])))\n",
    "    log_diffs = jnp.diff(log_midpoints)  / 2 / 1.15\n",
    "\n",
    "    bw_vals = jnp.where(log_diffs > err_widths_UAR, log_diffs, err_widths_UAR)\n",
    "    idx = jnp.searchsorted(unique_UAR, uar)\n",
    "    return bw_vals[idx]\n",
    "\n",
    "\n",
    "def kde_kernel(log_data, bw_values, log_grid):\n",
    "    H = bw_values[:, None]  # (N, 1)\n",
    "    U = (log_grid[None, :] - log_data) / H  # (N, M)\n",
    "    K = jnp.exp(-0.5 * U**2) / (H * jnp.sqrt(2 * jnp.pi))\n",
    "    return K.sum(axis=0) / log_data.shape[0]\n",
    "\n",
    "\n",
    "class KDEEstimator:\n",
    "    \"\"\"\n",
    "    Adaptive kernel density estimator using a measurement-error-informed bandwidth.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    log_grid : jnp.ndarray\n",
    "        Grid in log space over which to evaluate the KDE.\n",
    "    dx : jnp.ndarray\n",
    "        Spacing between grid points (gradient of log_grid).\n",
    "    cache : dict\n",
    "        Optional cache to store previously computed KDE results.\n",
    "    \"\"\"\n",
    "    def __init__(self, log_grid, dx, cache=None):\n",
    "        self.log_grid = jnp.asarray(log_grid, dtype=jnp.float32)\n",
    "        self.dx = jnp.asarray(dx, dtype=jnp.float32)\n",
    "\n",
    "\n",
    "    def compute(self, uar_data, da):\n",
    "        \"\"\"Compute the adaptive KDE and PMF for given unit area runoff data.\"\"\"\n",
    "        uar_data = jnp.asarray(uar_data)\n",
    "        da = float(da)\n",
    "\n",
    "        bw_values = adaptive_bandwidths(uar_data, da)\n",
    "        log_data = jnp.log(uar_data)[:, None]\n",
    "        pdf = kde_kernel(log_data, bw_values, self.log_grid)\n",
    "\n",
    "        # Normalize PDF\n",
    "        pdf /= jnp.trapezoid(pdf, x=self.log_grid)\n",
    "\n",
    "        # Convert to PMF\n",
    "        pmf = pdf * self.dx\n",
    "        pmf /= jnp.sum(pmf)\n",
    "        \n",
    "        return pmf, pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2173e371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_observed_and_simulated_pdf(stn, pmf_dfs, og_df, date, pdf_plots=[]):\n",
    "    \"\"\"Plot the observed and simulated PDFs for a given station.\"\"\"\n",
    "\n",
    "    baseline_lin_grid = np.exp(baseline_log_grid)\n",
    "    if date == '20250514':\n",
    "        title = f'{stn}: LSTM PDFs Mean NSE Objective'\n",
    "    elif date == '20250627':\n",
    "        title = f'{stn}: LSTM PDFs 95% Quantile Objective'\n",
    "    if len(pdf_plots) > 0:\n",
    "        p = figure(title=title, x_axis_type='log',\n",
    "            width=800, height=350, x_range=pdf_plots[0].x_range,\n",
    "            y_range=pdf_plots[0].y_range)\n",
    "    else:\n",
    "        p = figure(title=title, x_axis_type='log',\n",
    "            width=800, height=350)\n",
    "\n",
    "    # plot the observed values as quad glyphs\n",
    "    observed_vals = og_df[f'{stn}_uar'].dropna().values\n",
    "    observed_log_vals = np.log(observed_vals)\n",
    "    min_q, max_q = observed_log_vals.min(), observed_log_vals.max()\n",
    "    obs_log_dx = np.linspace(min_q - 0.1, max_q + 0.1, num=128)\n",
    "    hist, edges = np.histogram(observed_log_vals, bins=obs_log_dx, density=True)\n",
    "    edges = np.exp(edges)  # convert edges back to linear space\n",
    "    # convert to probbility mass function (PMF)\n",
    "    p.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:],\n",
    "            fill_color='dodgerblue', alpha=0.5, legend_label='Observed')\n",
    "    \n",
    "    df = pmf_dfs[date].copy()\n",
    "    for i in range(10):\n",
    "        sim_col = f'streamflow_sim_{i}_{date}'\n",
    "        if sim_col in df.columns:\n",
    "            p.line(baseline_lin_grid, df[sim_col].values / log_dx, color='grey', alpha=0.5, \n",
    "                    legend_label=f'LSTM Ensemble')\n",
    "\n",
    "    # convert pmfs to pdfs\n",
    "    p.line(baseline_lin_grid, df[f'POR_obs_{date}'].values / log_dx, \n",
    "           color='black', line_width=2.5, legend_label=f'POR Observed', line_dash='dotted')\n",
    "    p.line(baseline_lin_grid, df[f'POR_sim_timeEnsemble_{date}'].values / log_dx, \n",
    "           line_width=2.5, color='green', legend_label=f'timeEnsemble')\n",
    "    p.line(baseline_lin_grid, df[f'POR_sim_freqEnsemble_{date}'].values / log_dx, \n",
    "           line_width=2.5, color='red', legend_label=f'freqEnsemble')\n",
    "\n",
    "    p.xaxis.axis_label = 'Log Unit Area Runoff (L/s/km2)'\n",
    "    p.xaxis.axis_label = 'Unit Area Runoff (L/s/km2)'\n",
    "    p.yaxis.axis_label = 'Probability Density'\n",
    "    p.legend.location = 'top_left'\n",
    "    p.legend.click_policy = 'hide'\n",
    "    p.add_layout(p.legend[0], 'right')\n",
    "    return p\n",
    "\n",
    "\n",
    "def plot_observed_and_simulated_fdc(stn, lstm_df, og_df, date, fdc_plots=[]):\n",
    "    \"\"\"Plot the observed and simulated FDCs for a given station.\"\"\"\n",
    "\n",
    "    # baseline_lin_grid = np.exp(baseline_log_grid)\n",
    "    if date == '20250514':\n",
    "        title = f'{stn}: LSTM FDCs Mean NSE Objective'\n",
    "    elif date == '20250627':\n",
    "        title = f'{stn}: LSTM FDCs 95% Quantile Objective'\n",
    "\n",
    "    if len(fdc_plots) > 0:\n",
    "        fdc_plot = figure(title=title, #x_axis_type='log',\n",
    "            width=400, height=350, x_range=fdc_plots[0].x_range,\n",
    "            y_range=fdc_plots[0].y_range)\n",
    "    else:\n",
    "        fdc_plot = figure(title=title, #x_axis_type='log', \n",
    "                          width=800, height=350)\n",
    "        \n",
    "    # plot the observed duration curve\n",
    "    pcts = np.linspace(0.01, 0.99, 99)\n",
    "    observed_vals = og_df[f'{stn}_uar'].dropna().values\n",
    "    obs_fdc = np.percentile(observed_vals, pcts * 100)[::-1]\n",
    "\n",
    "    fdc_plot.line(pcts, obs_fdc, color='dodgerblue', line_width=2.5, legend_label=f'Observed')\n",
    "\n",
    "    sim_cols = [c for c in lstm_df.columns if c.startswith('streamflow_sim') and c.endswith(date)]\n",
    "    assert len(sim_cols) == 10, f'Expected 10 simulated columns, found {len(sim_cols)}'\n",
    "\n",
    "    sim_fdcs = pd.DataFrame(index=pcts)\n",
    "    for i, sim_col in enumerate(sim_cols):\n",
    "        sim_vals = lstm_df[sim_col].dropna().values\n",
    "        sim_fdc = np.percentile(sim_vals, pcts * 100)[::-1]\n",
    "        sim_fdcs[f'LSTM Simulation {i+1}'] = sim_fdc\n",
    "        fdc_plot.line(pcts, sim_fdc, color='grey', alpha=0.5, \n",
    "                      legend_label=f'LSTM Simulation')\n",
    "        \n",
    "    # compute the temporal ensemble mean FDC\n",
    "    temporal_mean_fdc = lstm_df[sim_cols].mean(axis=1).dropna().values\n",
    "    fdc_plot.line(pcts, np.percentile(temporal_mean_fdc, pcts * 100)[::-1], \n",
    "                  color='green', line_width=2.5, \n",
    "                  legend_label=f'Temporal Ensemble Mean')\n",
    "    \n",
    "    # compute the frequency ensemble mean FDC\n",
    "    freq_ensemble_fdc = sim_fdcs.mean(axis=1).values\n",
    "    fdc_plot.line(pcts, freq_ensemble_fdc, color='red', line_width=2.5, \n",
    "                  legend_label=f'Frequency Ensemble Mean')\n",
    "    \n",
    "    fdc_plot.xaxis.axis_label = 'Exceedance Probability (%)'\n",
    "    fdc_plot.yaxis.axis_label = 'Unit Area Runoff (L/s/km2)'\n",
    "    fdc_plot.legend.location = 'top_right'\n",
    "    fdc_plot.legend.click_policy = 'hide'\n",
    "    \n",
    "    return fdc_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "39962a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "def check_if_station_in_hydat(stn, conn):\n",
    "    # Query to check if the station exists\n",
    "    check_query = \"\"\"\n",
    "    SELECT STATION_NUMBER, STATION_NAME\n",
    "    FROM STATIONS\n",
    "    WHERE STATION_NUMBER = ?\n",
    "    \"\"\"\n",
    "    # Run the query\n",
    "    station_check = pd.read_sql_query(check_query, conn, params=(stn,))\n",
    "\n",
    "    # Test if any result was returned\n",
    "    if station_check.empty:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "    \n",
    "def query_data_symbols(conn):\n",
    "    # Query all data symbols\n",
    "    query = \"SELECT SYMBOL_ID, SYMBOL_EN FROM DATA_SYMBOLS ORDER BY SYMBOL_ID\"\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    return df.set_index('SYMBOL_ID')['SYMBOL_EN']\n",
    "\n",
    "\n",
    "def reshape_hydat_wide(df):\n",
    "    # First, ensure all FLOW_SYMBOL columns exist and are named correctly\n",
    "    id_vars = [\"STATION_NUMBER\", \"YEAR\", \"MONTH\", \"NO_DAYS\"]\n",
    "    \n",
    "    # Melt flows\n",
    "    flow_df = df.melt(id_vars=id_vars, \n",
    "                      value_vars=[f\"FLOW{i}\" for i in range(1, 32)],\n",
    "                      var_name=\"day\", value_name=\"flow\")\n",
    "\n",
    "    # Melt symbols\n",
    "    sym_df = df.melt(id_vars=id_vars, \n",
    "                     value_vars=[f\"FLOW_SYMBOL{i}\" for i in range(1, 32)],\n",
    "                     var_name=\"day\", value_name=\"flow_symbol\")\n",
    "\n",
    "    # Extract day number\n",
    "    flow_df[\"day\"] = flow_df[\"day\"].str.extract(r\"(\\d+)$\").astype(int)\n",
    "    sym_df[\"day\"] = sym_df[\"day\"].str.extract(r\"(\\d+)$\").astype(int)\n",
    "\n",
    "    # Merge on ID columns + day\n",
    "    merged = pd.merge(flow_df, sym_df, on=id_vars + [\"day\"])\n",
    "\n",
    "    # Construct date\n",
    "    merged[\"date\"] = pd.to_datetime(dict(year=merged[\"YEAR\"], \n",
    "                                         month=merged[\"MONTH\"], \n",
    "                                         day=merged[\"day\"]), errors='coerce')\n",
    "\n",
    "    # Filter out invalid days (e.g., day > NO_DAYS)\n",
    "    merged = merged[merged[\"day\"] <= merged[\"NO_DAYS\"]]\n",
    "    formatted_df = merged[[\"STATION_NUMBER\", \"date\", \"flow\", \"flow_symbol\"]].dropna(subset=[\"flow\"])\n",
    "    formatted_df.set_index('date', inplace=True)\n",
    "    return formatted_df\n",
    "\n",
    "\n",
    "def query_hydat_database(stn):\n",
    "    \"\"\"Query the HYDAT database for a given station and date range.\"\"\"\n",
    "    hydat_path = Path('/home/danbot/code/common_data/HYDAT') / 'Hydat_20250415.sqlite3'\n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(hydat_path)\n",
    "\n",
    "    quality_symbols = query_data_symbols(conn)\n",
    "\n",
    "    station_in_hydat = check_if_station_in_hydat(stn, conn)\n",
    "    if station_in_hydat is False:\n",
    "        print(f'Station {stn} not found in HYDAT database.')\n",
    "        return pd.DataFrame(), quality_symbols\n",
    "    \n",
    "    base_columns = [\"STATION_NUMBER\", \"YEAR\", \"MONTH\"]\n",
    "    flow_columns = [f\"FLOW{i}, FLOW_SYMBOL{i}\" for i in range(1, 32)]\n",
    "    end_columns = [\"NO_DAYS\"]\n",
    "\n",
    "    all_columns = base_columns + flow_columns + end_columns\n",
    "    column_str = \",\\n    \".join(all_columns)\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        {column_str}\n",
    "    FROM DLY_FLOWS\n",
    "    WHERE STATION_NUMBER = ?\n",
    "    ORDER BY YEAR, MONTH;\n",
    "    \"\"\"\n",
    "    df = pd.read_sql_query(query, conn, params=(stn,))\n",
    "\n",
    "    if df.empty:\n",
    "        print(f'No data found for {stn} in HYDAT.')\n",
    "        return pd.DataFrame(), quality_symbols\n",
    "    df = reshape_hydat_wide(df)\n",
    "    return df, quality_symbols\n",
    "\n",
    "\n",
    "def find_symbol_segments(symbol_df, target_symbol):\n",
    "    \"\"\"Return (start, end) date pairs for each continuous period of target_symbol.\"\"\"\n",
    "    # Filter for matching symbol only\n",
    "    mask = (symbol_df['flow_symbol'] == target_symbol)\n",
    "    dates = symbol_df['flow_symbol'].index[mask]\n",
    "\n",
    "    if dates.empty:\n",
    "        return []\n",
    "\n",
    "    # Compute gaps in days between successive dates\n",
    "    gaps = dates.to_series().diff().gt(pd.Timedelta(days=1)).fillna(True)\n",
    "\n",
    "    # Group by contiguous regions (cumsum creates a new group after each gap)\n",
    "    group_ids = gaps.cumsum()\n",
    "\n",
    "    # Group by group ID and extract start and end of each contiguous block\n",
    "    segments = [(group.min(), group.max()) for _, group in dates.to_series().groupby(group_ids)]\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "073de4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_FDCs(df, stn, og_df, output_folder):\n",
    "\n",
    "    kde = KDEEstimator(baseline_log_grid, log_dx)\n",
    "    print(f'    Processing FDCs for {stn}')\n",
    "    por_metrics = {}\n",
    "    pmf_dfs = {}\n",
    "    years = []\n",
    "    for date in ['20250514', '20250627']:\n",
    "        pmf_columns = []\n",
    "        years = df.index.year.unique()\n",
    "        da = da_dict[stn]\n",
    "\n",
    "        pmf_fpath = f'data/results/lstm_pmfs/POR_{stn}_pmfs_{len(years)}_years_{date}.csv'\n",
    "        # if os.path.exists(pmf_fpath):\n",
    "        #     print(f'    PMFs for {stn} already exist, skipping.')\n",
    "        #     continue\n",
    "\n",
    "        # compute observed POR PMFs\n",
    "        obs_cols = [c for c in df.columns if c.startswith('streamflow_obs') and c.endswith(date)]\n",
    "        assert len(obs_cols) == 1, f'Expected exactly one observed column for {stn} on {date}, found {len(obs_cols)}'\n",
    "        por_obs_vals = df[obs_cols[0]].dropna().values\n",
    "        # print(f'{stn} POR obs: {min(por_obs_vals):.2f} - {max(por_obs_vals):.2f}')\n",
    "        por_obs_pmf, por_obs_pdf = kde.compute(por_obs_vals, da=da)\n",
    "        pmf_columns.append(pd.Series(por_obs_pmf, index=baseline_log_grid, name=f'POR_obs_{date}'))\n",
    "\n",
    "        # compute simulated POR PMFs\n",
    "        sim_cols = [c for c in df.columns if c.startswith('streamflow_sim') and c.endswith(date)]\n",
    "        # temporal mean of the simulated ensemble\n",
    "        sim_vals = df[sim_cols].mean(axis=1).dropna().values\n",
    "        sim_pmf, sim_pdf = kde.compute(sim_vals, da=da) # temporal mean ensemble PMF\n",
    "        pmf_columns.append(pd.Series(sim_pmf, index=baseline_log_grid, name=f'POR_sim_timeEnsemble_{date}'))\n",
    "\n",
    "        frequency_sim_pmfs = compute_ensemble_pmfs(df, sim_cols, kde, da)\n",
    "        mean_pmf = frequency_sim_pmfs.mean(axis=1).rename('sim_freqEnsemble_mean')\n",
    "        mean_pmf /= mean_pmf.sum()  # renormalize the PMF\n",
    "        pmf_columns.append(pd.Series(mean_pmf, index=baseline_log_grid, name=f'POR_sim_freqEnsemble_{date}'))\n",
    "\n",
    "        og_vals = og_df[f'{stn}_uar'].dropna().values\n",
    "        # print(f'{stn}_uar: {min(og_vals):.2f} - {max(og_vals):.2f}')\n",
    "        og_pmf, og_pdf = kde.compute(og_vals, da=da)\n",
    "        pmf_columns.append(pd.Series(og_pmf, index=baseline_log_grid, name=f'{stn}_uar'))\n",
    "        por_pmfs = pd.concat(pmf_columns, axis=1)    # do the same for the original timeseries\n",
    "        pmfs = pd.concat([por_pmfs, frequency_sim_pmfs], axis=1)  # initialize PMFs DataFrame with observed PMFs\n",
    "        pmfs.index.name = 'log_uar'  # set the index name for clarity\n",
    "        # save the PMFs to a CSV file\n",
    "        pmfs.to_csv(pmf_fpath)\n",
    "        pmf_dfs[date] = pmfs\n",
    "\n",
    "        # evaluate metrics\n",
    "        for col in [f'POR_sim_timeEnsemble_{date}', f'POR_sim_freqEnsemble_{date}']:\n",
    "            pmf = pmfs[col].values\n",
    "            por_metrics[col] = evaluate_fdc_metrics_from_pmf(pmf, por_obs_pmf, baseline_log_grid)\n",
    "\n",
    "        for col in frequency_sim_pmfs.columns:\n",
    "            pmf = frequency_sim_pmfs[col].values\n",
    "            por_metrics[col] = evaluate_fdc_metrics_from_pmf(pmf, por_obs_pmf, baseline_log_grid)\n",
    "\n",
    "    mdf = pd.DataFrame(por_metrics).T\n",
    "    # save to csv\n",
    "    metric_fpath = f'data/results/lstm_metrics/{stn}_{len(years)}_years_metrics.csv'\n",
    "    mdf.to_csv(metric_fpath)\n",
    "    return mdf, pmf_dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "27a29d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_quality_flag_periods(stn, df, hydat_df, quality_symbols, runoff_plot, obs_col):\n",
    "    symbol_dict = quality_symbols.to_dict()\n",
    "    symbol_colors = {\n",
    "        'B': 'dodgerblue',  # Baseflow\n",
    "        'D': 'firebrick',  # Dry weather flow\n",
    "        'E': 'orange'  # Estimated\n",
    "    }\n",
    "    df['flow_symbol'] = hydat_df['flow_symbol'].reindex(df.index, method=None)\n",
    "    uar_cols = obs_col + [c for c in df.columns if c.startswith('streamflow_sim')]\n",
    "\n",
    "    for symbol in ['B', 'D', 'E']:\n",
    "        description = symbol_dict.get(symbol, {})\n",
    "        color = symbol_colors.get(symbol, 'gray')\n",
    "        n_symbols = df['flow_symbol'].eq(symbol).sum()\n",
    "        if n_symbols == 0:\n",
    "            continue\n",
    "\n",
    "        segments = find_symbol_segments(df[['flow_symbol']].copy(), symbol)\n",
    "\n",
    "        for start, end in segments:\n",
    "            runoff_plot.varea(\n",
    "                x=pd.date_range(start, end),\n",
    "                y1=0.98 * df[uar_cols].min().min(),  # get the min of the dataframe for the lower bound\n",
    "                y2=1.02 * df[uar_cols].max().max(),  # a bit above max for visibility\n",
    "                fill_color=color, fill_alpha=0.3,\n",
    "                legend_label=f\"{description} ({symbol})\"\n",
    "            )\n",
    "    \n",
    "    df['flow'] = hydat_df['flow'].reindex(df.index, method=None)\n",
    "    df['hydat_uar'] = 1000 * df['flow'] / da_dict[stn]  # convert to unit area runoff (L/s/km2)\n",
    "    runoff_plot.line(df.index, df['hydat_uar'],\n",
    "                     color='dodgerblue', legend_label='HYDAT UAR', \n",
    "                     line_width=2, line_dash='dotted')\n",
    "    return runoff_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "132c6a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_runoff_timeseries(stn, lstm_df, date):\n",
    "\n",
    "    obs_col = [c for c in lstm_df.columns if '_obs_' in c and c.endswith(date)]\n",
    "    assert len(obs_col) == 1, f'Expected exactly one observed column for {stn}, found {len(obs_col)} {obs_col}'\n",
    "    sim_cols = [c for c in lstm_df.columns if c.startswith('streamflow_sim') and c.endswith(date)]\n",
    "    assert len(sim_cols) == 10, f'Expected ten simulation columns for {stn}, found {len(sim_cols)}'\n",
    "    # plot th time series of the observed values\n",
    "    runoff_plot = figure(title=f'{stn} Observed Unit Area Runoff', x_axis_type='datetime',\n",
    "                         width=800, height=350, y_axis_type='log')\n",
    "    \n",
    "    hydat_df, quality_symbols = query_hydat_database(stn)\n",
    "\n",
    "    # reindex to daily frequency and keep nans\n",
    "    df = lstm_df.copy().reindex(pd.date_range(start=lstm_df.index.min(), end=lstm_df.index.max(), freq='D'))\n",
    "\n",
    "    if not hydat_df.empty:\n",
    "        runoff_plot = plot_quality_flag_periods(stn, df, hydat_df, quality_symbols, runoff_plot, obs_col)\n",
    "    \n",
    "    runoff_plot.line(df.index, df[obs_col[0]], color='dodgerblue',\n",
    "                     legend_label='Observed UAR', line_width=2.)\n",
    "    for col in sim_cols:\n",
    "        runoff_plot.line(df.index, df[col], color='grey', alpha=0.5,\n",
    "                         legend_label=f'LSTM ensemble')\n",
    "    # compute the temporal mean of the simulated ensemble\n",
    "    mean_sim = df[sim_cols].mean(axis=1)\n",
    "    runoff_plot.line(df.index, mean_sim, color='black', legend_label='Ensemble Mean', line_width=2, line_dash='dashed')\n",
    "    runoff_plot.xaxis.axis_label = 'Date'\n",
    "    runoff_plot.yaxis.axis_label = 'Unit Area Runoff (L/s/km2)'\n",
    "    runoff_plot.legend.location = 'top_left'\n",
    "    runoff_plot.legend.click_policy = 'hide'\n",
    "    # runoff_plot.add_layout(runoff_plot.legend[0], 'right')\n",
    "    runoff_plot.legend.background_fill_alpha = 0.65\n",
    "    return runoff_plot\n",
    "\n",
    "\n",
    "def format_metrics_table(metric_df, stn, date):\n",
    "    metric_df.index.name = 'series'\n",
    "    metric_df = metric_df.reset_index()\n",
    "    metric_df['date'] = metric_df.apply(lambda row: row['series'].split('_')[-1], axis=1)\n",
    "\n",
    "    # lines = ['solid', 'dashed']\n",
    "    metric_cols = [c for c in metric_df.columns if c.startswith('FDC_')]\n",
    "    filtered_metrics = metric_df[metric_df['date'] == date].copy()\n",
    "    \n",
    "    time_df = filtered_metrics[filtered_metrics['series'].str.contains('timeEnsemble')].copy()\n",
    "    freq_df = filtered_metrics[filtered_metrics['series'].str.contains('freqEnsemble')].copy()\n",
    "    sim_df = filtered_metrics[filtered_metrics['series'].str.startswith('streamflow_sim_')].copy()\n",
    "    sim_vals = pd.DataFrame(np.percentile(sim_df[metric_cols].values, [5, 50, 95], axis=0), \n",
    "                            index=['5%', '50%', '95%'], columns=metric_cols)   \n",
    "\n",
    "    metric_table_df = pd.concat([time_df[metric_cols], freq_df[metric_cols], sim_vals], axis=0)\n",
    "    metric_table_df[date] = ['timeEnsemble', 'freqEnsemble', '5%', '50%', '95%']\n",
    "    metric_table_df.set_index(date, inplace=True)\n",
    "    metric_table_df.columns = [c.split('_')[-1] for c in metric_table_df.columns]\n",
    "    metric_table_df.columns = ['RE' if c == 'RelativeError' else c for c in metric_table_df.columns]\n",
    "    # format the metrics table for display in a Div\n",
    "    # metric_table_df = metric_table_df.round(3)\n",
    "    metric_table_html = metric_table_df.style.format(precision=3).set_properties(**{'padding': '3px'}).to_html(classes='table table-striped', border=0,\n",
    "                                                justify='center', index=True)\n",
    "    div = Div(text=metric_table_html, width=1000, height=200)\n",
    "    return div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0dd4d33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 723 monitored basins with LSTM ensemble results.\n",
      "    Found 36 complete years for 05DC012: [1985, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2020, 2021, 2022]\n",
      "    Processing FDCs for 05DC012\n",
      "    Saved plot for 05DC012 to /home/danbot/code/distribution_estimation/docs/notebooks/data/results/lstm_plots/05DC012_fdc.html\n",
      "    Found 18 complete years for 12323710: [2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023]\n",
      "    Processing FDCs for 12323710\n",
      "Station 12323710 not found in HYDAT database.\n",
      "    Saved plot for 12323710 to /home/danbot/code/distribution_estimation/docs/notebooks/data/results/lstm_plots/12323710_fdc.html\n",
      "    Found 22 complete years for 08LA008: [1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1972, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984]\n",
      "    Processing FDCs for 08LA008\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     22\u001b[39m og_df = og_df[og_df.index.isin(lstm_ensemble_df.index)]\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m mdf, pmf_dfs = \u001b[43mprocess_FDCs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlstm_ensemble_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mog_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m dates = \u001b[38;5;28mlist\u001b[39m(pmf_dfs.keys())\n\u001b[32m     26\u001b[39m pdf_plots, fdc_plots, metric_tables = [], [], []\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mprocess_FDCs\u001b[39m\u001b[34m(df, stn, og_df, output_folder)\u001b[39m\n\u001b[32m     30\u001b[39m sim_pmf, sim_pdf = kde.compute(sim_vals, da=da) \u001b[38;5;66;03m# temporal mean ensemble PMF\u001b[39;00m\n\u001b[32m     31\u001b[39m pmf_columns.append(pd.Series(sim_pmf, index=baseline_log_grid, name=\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mPOR_sim_timeEnsemble_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m frequency_sim_pmfs = \u001b[43mcompute_ensemble_pmfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msim_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkde\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m mean_pmf = frequency_sim_pmfs.mean(axis=\u001b[32m1\u001b[39m).rename(\u001b[33m'\u001b[39m\u001b[33msim_freqEnsemble_mean\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     35\u001b[39m mean_pmf /= mean_pmf.sum()  \u001b[38;5;66;03m# renormalize the PMF\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 290\u001b[39m, in \u001b[36mcompute_ensemble_pmfs\u001b[39m\u001b[34m(df, sim_cols, kde, da)\u001b[39m\n\u001b[32m    288\u001b[39m     sim_vals = df[sim_col].dropna().values\n\u001b[32m    289\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sim_vals) > \u001b[32m0\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mNo valid values found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msim_col\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m     sim_pmf, _ = \u001b[43mkde\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msim_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mda\u001b[49m\u001b[43m=\u001b[49m\u001b[43mda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    291\u001b[39m     sim_ensemble_pmfs.append(pd.Series(sim_pmf, index=baseline_log_grid, name=sim_col))\n\u001b[32m    292\u001b[39m \u001b[38;5;66;03m# concatenate all PMFs and compute the mean\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 100\u001b[39m, in \u001b[36mKDEEstimator.compute\u001b[39m\u001b[34m(self, uar_data, da)\u001b[39m\n\u001b[32m     97\u001b[39m uar_data = jnp.asarray(uar_data)\n\u001b[32m     98\u001b[39m da = \u001b[38;5;28mfloat\u001b[39m(da)\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m bw_values = \u001b[43madaptive_bandwidths\u001b[49m\u001b[43m(\u001b[49m\u001b[43muar_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m log_data = jnp.log(uar_data)[:, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    102\u001b[39m pdf = kde_kernel(log_data, bw_values, \u001b[38;5;28mself\u001b[39m.log_grid)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36madaptive_bandwidths\u001b[39m\u001b[34m(uar, da)\u001b[39m\n\u001b[32m     60\u001b[39m left_mirror = unique_UAR[\u001b[32m0\u001b[39m] - (log_midpoints[\u001b[32m0\u001b[39m] - unique_UAR[\u001b[32m0\u001b[39m])\n\u001b[32m     61\u001b[39m right_mirror = unique_UAR[-\u001b[32m1\u001b[39m] + (unique_UAR[-\u001b[32m1\u001b[39m] - log_midpoints[-\u001b[32m1\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m log_midpoints = \u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mleft_mirror\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_midpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mright_mirror\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m log_diffs = jnp.diff(log_midpoints)  / \u001b[32m2\u001b[39m / \u001b[32m1.15\u001b[39m\n\u001b[32m     65\u001b[39m bw_vals = jnp.where(log_diffs > err_widths_UAR, log_diffs, err_widths_UAR)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/data_analysis/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py:4625\u001b[39m, in \u001b[36mconcatenate\u001b[39m\u001b[34m(arrays, axis, dtype)\u001b[39m\n\u001b[32m   4623\u001b[39m k = \u001b[32m16\u001b[39m\n\u001b[32m   4624\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(arrays_out) > \u001b[32m1\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m4625\u001b[39m   arrays_out = [\u001b[43mlax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays_out\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m+\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4626\u001b[39m                 \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(arrays_out), k)]\n\u001b[32m   4627\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_out[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/data_analysis/lib/python3.12/site-packages/jax/_src/lax/lax.py:2006\u001b[39m, in \u001b[36mconcatenate\u001b[39m\u001b[34m(operands, dimension)\u001b[39m\n\u001b[32m   2004\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m op\n\u001b[32m   2005\u001b[39m operands = core.standard_insert_pvary(*operands)\n\u001b[32m-> \u001b[39m\u001b[32m2006\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcatenate_p\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43moperands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdimension\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdimension\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/data_analysis/lib/python3.12/site-packages/jax/_src/core.py:536\u001b[39m, in \u001b[36mPrimitive.bind\u001b[39m\u001b[34m(self, *args, **params)\u001b[39m\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **params):\n\u001b[32m    535\u001b[39m   args = args \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.skip_canonicalization \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(canonicalize_value, args)\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_true_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/data_analysis/lib/python3.12/site-packages/jax/_src/core.py:552\u001b[39m, in \u001b[36mPrimitive._true_bind\u001b[39m\u001b[34m(self, *args, **params)\u001b[39m\n\u001b[32m    550\u001b[39m trace_ctx.set_trace(eval_trace)\n\u001b[32m    551\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m552\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    554\u001b[39m   trace_ctx.set_trace(prev_trace)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/data_analysis/lib/python3.12/site-packages/jax/_src/core.py:562\u001b[39m, in \u001b[36mPrimitive.bind_with_trace\u001b[39m\u001b[34m(self, trace, args, params)\u001b[39m\n\u001b[32m    559\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m set_current_trace(trace):\n\u001b[32m    560\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_lojax(*args, **params)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/data_analysis/lib/python3.12/site-packages/jax/_src/core.py:1066\u001b[39m, in \u001b[36mEvalTrace.process_primitive\u001b[39m\u001b[34m(self, primitive, args, params)\u001b[39m\n\u001b[32m   1064\u001b[39m args = \u001b[38;5;28mmap\u001b[39m(full_lower, args)\n\u001b[32m   1065\u001b[39m check_eval_args(args)\n\u001b[32m-> \u001b[39m\u001b[32m1066\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/data_analysis/lib/python3.12/site-packages/jax/_src/dispatch.py:91\u001b[39m, in \u001b[36mapply_primitive\u001b[39m\u001b[34m(prim, *args, **params)\u001b[39m\n\u001b[32m     89\u001b[39m prev = lib.jax_jit.swap_thread_local_state_disable_jit(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m   outs = \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     93\u001b[39m   lib.jax_jit.swap_thread_local_state_disable_jit(prev)\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/data_analysis/lib/python3.12/site-packages/jax/_src/pjit.py:292\u001b[39m, in \u001b[36m_cpp_pjit.<locals>.cache_miss\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.no_tracing.value:\n\u001b[32m    288\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mre-tracing function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjit_info.fun_sourceinfo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    289\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33m`jit`, but \u001b[39m\u001b[33m'\u001b[39m\u001b[33mno_tracing\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is set\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    291\u001b[39m (outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked, box_data,\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m  executable, pgle_profiler) = \u001b[43m_python_pjit_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    294\u001b[39m maybe_fastpath_data = _get_fastpath_data(\n\u001b[32m    295\u001b[39m     executable, out_tree, args_flat, out_flat, attrs_tracked, box_data,\n\u001b[32m    296\u001b[39m     jaxpr.effects, jaxpr.consts, jit_info.abstracted_axes, pgle_profiler)\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outs, maybe_fastpath_data, _need_to_rebuild_with_fdo(pgle_profiler)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/data_analysis/lib/python3.12/site-packages/jax/_src/pjit.py:153\u001b[39m, in \u001b[36m_python_pjit_helper\u001b[39m\u001b[34m(fun, jit_info, *args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m   args_flat = \u001b[38;5;28mmap\u001b[39m(core.full_lower, args_flat)\n\u001b[32m    152\u001b[39m   core.check_eval_args(args_flat)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m   out_flat, compiled, profiler = \u001b[43m_pjit_call_impl_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    155\u001b[39m   out_flat = pjit_p.bind(*args_flat, **p.params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/data_analysis/lib/python3.12/site-packages/jax/_src/pjit.py:1855\u001b[39m, in \u001b[36m_pjit_call_impl_python\u001b[39m\u001b[34m(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, donated_invars, ctx_mesh, name, keep_unused, inline, compiler_options_kvs, *args)\u001b[39m\n\u001b[32m   1843\u001b[39m compiler_options_kvs = compiler_options_kvs + \u001b[38;5;28mtuple\u001b[39m(pgle_compile_options.items())\n\u001b[32m   1844\u001b[39m \u001b[38;5;66;03m# Passing mutable PGLE profile here since it should be extracted by JAXPR to\u001b[39;00m\n\u001b[32m   1845\u001b[39m \u001b[38;5;66;03m# initialize the fdo_profile compile option.\u001b[39;00m\n\u001b[32m   1846\u001b[39m compiled = \u001b[43m_resolve_and_lower\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1847\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_shardings\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1848\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_layouts\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_layouts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1849\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_layouts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_layouts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1850\u001b[39m \u001b[43m    \u001b[49m\u001b[43mctx_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mctx_mesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1851\u001b[39m \u001b[43m    \u001b[49m\u001b[43minline\u001b[49m\u001b[43m=\u001b[49m\u001b[43minline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlowering_platforms\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1852\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlowering_parameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmlir\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLoweringParameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1853\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1854\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1855\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1857\u001b[39m \u001b[38;5;66;03m# This check is expensive so only do it if enable_checks is on.\u001b[39;00m\n\u001b[32m   1858\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compiled._auto_spmd_lowering \u001b[38;5;129;01mand\u001b[39;00m config.enable_checks.value:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/data_analysis/lib/python3.12/site-packages/jax/_src/interpreters/pxla.py:2410\u001b[39m, in \u001b[36mMeshComputation.compile\u001b[39m\u001b[34m(self, compiler_options)\u001b[39m\n\u001b[32m   2408\u001b[39m compiler_options_kvs = \u001b[38;5;28mself\u001b[39m._compiler_options_kvs + t_compiler_options\n\u001b[32m   2409\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._executable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m compiler_options_kvs:\n\u001b[32m-> \u001b[39m\u001b[32m2410\u001b[39m   executable = \u001b[43mUnloadedMeshExecutable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_hlo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2411\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_hlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompile_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2412\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2413\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m compiler_options_kvs:\n\u001b[32m   2414\u001b[39m     \u001b[38;5;28mself\u001b[39m._executable = executable\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/data_analysis/lib/python3.12/site-packages/jax/_src/interpreters/pxla.py:2952\u001b[39m, in \u001b[36mUnloadedMeshExecutable.from_hlo\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   2949\u001b[39m       \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   2951\u001b[39m util.test_event(\u001b[33m\"\u001b[39m\u001b[33mpxla_cached_compilation\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2952\u001b[39m xla_executable = \u001b[43m_cached_compilation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2953\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspmd_lowering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtuple_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_spmd_lowering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_prop_to_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_prop_to_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpmap_nreps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2958\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m auto_spmd_lowering:\n\u001b[32m   2959\u001b[39m   \u001b[38;5;28;01massert\u001b[39;00m mesh \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/data_analysis/lib/python3.12/site-packages/jax/_src/interpreters/pxla.py:2743\u001b[39m, in \u001b[36m_cached_compilation\u001b[39m\u001b[34m(computation, name, mesh, spmd_lowering, tuple_args, auto_spmd_lowering, allow_prop_to_inputs, allow_prop_to_outputs, host_callbacks, backend, da, pmap_nreps, compiler_options_kvs, pgle_profiler)\u001b[39m\n\u001b[32m   2735\u001b[39m compile_options = create_compile_options(\n\u001b[32m   2736\u001b[39m     computation, mesh, spmd_lowering, tuple_args, auto_spmd_lowering,\n\u001b[32m   2737\u001b[39m     allow_prop_to_inputs, allow_prop_to_outputs, backend,\n\u001b[32m   2738\u001b[39m     dev, pmap_nreps, compiler_options)\n\u001b[32m   2740\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m dispatch.log_elapsed_time(\n\u001b[32m   2741\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFinished XLA compilation of \u001b[39m\u001b[38;5;132;01m{fun_name}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{elapsed_time:.9f}\u001b[39;00m\u001b[33m sec\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2742\u001b[39m     fun_name=name, event=dispatch.BACKEND_COMPILE_EVENT):\n\u001b[32m-> \u001b[39m\u001b[32m2743\u001b[39m   xla_executable = \u001b[43mcompiler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile_or_get_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2744\u001b[39m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2745\u001b[39m \u001b[43m      \u001b[49m\u001b[43mda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2746\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m xla_executable\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/data_analysis/lib/python3.12/site-packages/jax/_src/compiler.py:500\u001b[39m, in \u001b[36mcompile_or_get_cached\u001b[39m\u001b[34m(backend, computation, devices, compile_options, host_callbacks, executable_devices, pgle_profiler)\u001b[39m\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    499\u001b[39m   log_persistent_cache_miss(module_name, cache_key)\n\u001b[32m--> \u001b[39m\u001b[32m500\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_and_write_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m      \u001b[49m\u001b[43mexecutable_devices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m      \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/data_analysis/lib/python3.12/site-packages/jax/_src/compiler.py:768\u001b[39m, in \u001b[36m_compile_and_write_cache\u001b[39m\u001b[34m(backend, computation, executable_devices, compile_options, host_callbacks, module_name, cache_key)\u001b[39m\n\u001b[32m    758\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_compile_and_write_cache\u001b[39m(\n\u001b[32m    759\u001b[39m     backend: xc.Client,\n\u001b[32m    760\u001b[39m     computation: ir.Module,\n\u001b[32m   (...)\u001b[39m\u001b[32m    765\u001b[39m     cache_key: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    766\u001b[39m ) -> xc.LoadedExecutable:\n\u001b[32m    767\u001b[39m   start_time = time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m768\u001b[39m   executable = \u001b[43mbackend_compile_and_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    769\u001b[39m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable_devices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost_callbacks\u001b[49m\n\u001b[32m    770\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    771\u001b[39m   compile_time = time.monotonic() - start_time\n\u001b[32m    772\u001b[39m   _cache_write(\n\u001b[32m    773\u001b[39m       cache_key, compile_time, module_name, backend, executable, host_callbacks\n\u001b[32m    774\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/data_analysis/lib/python3.12/site-packages/jax/_src/profiler.py:354\u001b[39m, in \u001b[36mannotate_function.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    351\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    353\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, **decorator_kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/data_analysis/lib/python3.12/site-packages/jax/_src/compiler.py:385\u001b[39m, in \u001b[36mbackend_compile_and_load\u001b[39m\u001b[34m(backend, module, executable_devices, options, host_callbacks)\u001b[39m\n\u001b[32m    376\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m backend.compile_and_load(\n\u001b[32m    377\u001b[39m           built_c,\n\u001b[32m    378\u001b[39m           executable_devices=executable_devices,\n\u001b[32m    379\u001b[39m           compile_options=options,\n\u001b[32m    380\u001b[39m           host_callbacks=host_callbacks,\n\u001b[32m    381\u001b[39m       )\n\u001b[32m    382\u001b[39m     \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[32m    383\u001b[39m     \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[32m    384\u001b[39m     \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile_and_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutable_devices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutable_devices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m xc.XlaRuntimeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    391\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m error_handler \u001b[38;5;129;01min\u001b[39;00m _XLA_RUNTIME_ERROR_HANDLERS:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# retrieve LSTM ensemble predictions\n",
    "# filter for the common stations\n",
    "common_stations = list(set(station_ids) & set(lstm_result_stns))\n",
    "print(f'There are {len(common_stations)} monitored basins with LSTM ensemble results.')\n",
    "attr_df = attr_df[attr_df['official_id'].isin(common_stations)]\n",
    "\n",
    "output_folder = BASE_DIR / 'data' / 'results' /  'lstm_plots'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# plots = []\n",
    "process_fdcs = True\n",
    "if process_fdcs:\n",
    "    for stn in common_stations:\n",
    "        og_df = get_original_timeseries(stn, ds)\n",
    "        # print(og_df[og_df.index >= '1982-06-01'].head())\n",
    "        lstm_ensemble_df = filter_by_complete_years(stn, lstm_result_base_folder)\n",
    "\n",
    "        if lstm_ensemble_df.empty:\n",
    "            print(f'No complete years found for {stn}. Skipping.')\n",
    "            continue\n",
    "        og_df = og_df[og_df.index.isin(lstm_ensemble_df.index)]\n",
    "        mdf, pmf_dfs = process_FDCs(lstm_ensemble_df, stn, og_df, output_folder)\n",
    "\n",
    "        dates = list(pmf_dfs.keys())\n",
    "        pdf_plots, fdc_plots, metric_tables = [], [], []\n",
    "        ts_plot = plot_runoff_timeseries(stn, lstm_ensemble_df, dates[-1])\n",
    "        for date in dates:\n",
    "            metric_table = format_metrics_table(mdf, stn, date)\n",
    "            metric_tables.append(metric_table)\n",
    "            pdf_plot = plot_observed_and_simulated_pdf(stn, pmf_dfs, og_df, date, pdf_plots=pdf_plots)\n",
    "            pdf_plots.append(pdf_plot)\n",
    "            fdc_plot = plot_observed_and_simulated_fdc(stn, lstm_ensemble_df, og_df, date, fdc_plots=fdc_plots)\n",
    "            fdc_plots.append(fdc_plot)\n",
    "\n",
    "        layout = column(\n",
    "            row([ts_plot, fdc_plots[1]]), \n",
    "            row(pdf_plots[0], metric_tables[0]), \n",
    "            row(pdf_plots[1], metric_tables[1]), \n",
    "            )\n",
    "        # save the plot to an HTML file\n",
    "        # show(layout)\n",
    "        output_fname = output_folder / f'{stn}_fdc.html'\n",
    "        output_file(output_fname, title=f'{stn} FDCs')\n",
    "        save(layout)\n",
    "        print(f'    Saved plot for {stn} to {output_fname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc98f2d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1950, 1951]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6c70d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_metric_to_dict(metric_table, series):\n",
    "    return {\n",
    "        'RMSE': metric_table.loc[series, 'RMSE'],\n",
    "        'NSE': metric_table.loc[series, 'NSE'],\n",
    "        'KGE': metric_table.loc[series, 'KGE'],\n",
    "        'RE': metric_table.loc[series, 'RE'],\n",
    "        'KLD': metric_table.loc[series, 'KLD'],\n",
    "        'EMD': metric_table.loc[series, 'EMD'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4675d9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No metrics file found for 12102190, skipping.\n"
     ]
    }
   ],
   "source": [
    "# create a plot of CDFs by metric across all stations\n",
    "rmses, nses, kges, rel_errors, dkls, emds = [], [], [], [], [], []\n",
    "freq_ensemble_vals, time_ensemble_vals, low_bound_vals, high_bound_vals, median_vals = {}, {}, {}, {}, {}\n",
    "dates = ['20250514', '20250627']  # 20250514 use the latest date for the metrics\n",
    "metrics_files = sorted(os.listdir(BASE_DIR / 'data' / 'lstm_metrics'))\n",
    "metric_dict = defaultdict(dict)\n",
    "for stn in common_stations:\n",
    "    # files = [f for f in metrcs_files if f'_{stn}_' in f]\n",
    "    metric_files = [f for f in metrics_files if f.startswith(stn) and f.endswith('years_metrics.csv')]\n",
    "    n_years = int(metric_files[0].split('_')[1]) if metric_files else 0\n",
    "    if len(metric_files) == 0:\n",
    "        print(f'No metrics file found for {stn}, skipping.')\n",
    "        continue\n",
    "    assert len(metric_files) == 1, f'Expected exactly one metrics file for {stn}, found {len(metric_files)}'\n",
    "    mdf = pd.read_csv(BASE_DIR / 'data' / 'lstm_metrics' / metric_files[0])\n",
    "    metric_cols = [c for c in mdf.columns if c.startswith('FDC_')]\n",
    "    mdf.rename({'Unnamed: 0': 'series'}, axis=1, inplace=True)\n",
    "    mdf['date'] = mdf.apply(lambda row: row['series'].split('_')[-1], axis=1)\n",
    "    # date = files[0].split('_')[-1].split('.')[0]  # extract date from the file name\n",
    "\n",
    "    for date in dates:\n",
    "        filtered_metrics = mdf[mdf['date'] == date].copy()\n",
    "        time_df = filtered_metrics[filtered_metrics['series'].str.contains('timeEnsemble')].copy()\n",
    "        freq_df = filtered_metrics[filtered_metrics['series'].str.contains('freqEnsemble')].copy()\n",
    "        sim_df = filtered_metrics[filtered_metrics['series'].str.startswith('streamflow_sim_')].copy()\n",
    "        sim_vals = pd.DataFrame(np.percentile(sim_df[metric_cols].values, [5, 50, 95], axis=0), \n",
    "                                    index=['5%', '50%', '95%'], columns=metric_cols)     \n",
    "        \n",
    "        metric_table_df = pd.concat([time_df[metric_cols], freq_df[metric_cols], sim_vals], axis=0)\n",
    "        metric_table_df[date] = ['timeEnsemble', 'freqEnsemble', '5%', '50%', '95%']\n",
    "        metric_table_df.set_index(date, inplace=True)\n",
    "        metric_table_df.columns = [c.split('_')[-1] for c in metric_table_df.columns]\n",
    "        metric_table_df.columns = ['RE' if c == 'RelativeError' else c for c in metric_table_df.columns]\n",
    "        \n",
    "        for s in metric_table_df.index:\n",
    "            val_dict = map_metric_to_dict(metric_table_df, s)\n",
    "            #  append the value to the same metric\n",
    "            \n",
    "            for m, value in val_dict.items():\n",
    "                if m not in metric_dict[date]:\n",
    "                    metric_dict[date][m] = {}\n",
    "                if s not in metric_dict[date][m]:\n",
    "                    metric_dict[date][m][s] = [float(value)]\n",
    "                else:\n",
    "                    metric_dict[date][m][s].append(float(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c9c3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_empirical_cdf(data):\n",
    "    \"\"\"Compute the empirical CDF of the data.\"\"\"\n",
    "    sorted_data = np.sort(data)\n",
    "    n = len(sorted_data)\n",
    "    cdf = np.arange(1, n + 1) / n\n",
    "    return sorted_data, cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3b67f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdict = {'timeEnsemble': 'green', 'freqEnsemble': 'red', '5%': 'blue', '50%': 'black', '95%': 'navy'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7552ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeEnsemble solid green\n",
      "freqEnsemble solid red\n",
      "5% dotted blue\n",
      "50% dashdot black\n",
      "95% dashed navy\n",
      "timeEnsemble solid green\n",
      "freqEnsemble solid red\n",
      "5% dotted blue\n",
      "50% dashdot black\n",
      "95% dashed navy\n",
      "timeEnsemble solid green\n",
      "freqEnsemble solid red\n",
      "5% dotted blue\n",
      "50% dashdot black\n",
      "95% dashed navy\n",
      "timeEnsemble solid green\n",
      "freqEnsemble solid red\n",
      "5% dotted blue\n",
      "50% dashdot black\n",
      "95% dashed navy\n",
      "timeEnsemble solid green\n",
      "freqEnsemble solid red\n",
      "5% dotted blue\n",
      "50% dashdot black\n",
      "95% dashed navy\n",
      "timeEnsemble solid green\n",
      "freqEnsemble solid red\n",
      "5% dotted blue\n",
      "50% dashdot black\n",
      "95% dashed navy\n",
      "timeEnsemble solid green\n",
      "freqEnsemble solid red\n",
      "5% dotted blue\n",
      "50% dashdot black\n",
      "95% dashed navy\n",
      "timeEnsemble solid green\n",
      "freqEnsemble solid red\n",
      "5% dotted blue\n",
      "50% dashdot black\n",
      "95% dashed navy\n",
      "timeEnsemble solid green\n",
      "freqEnsemble solid red\n",
      "5% dotted blue\n",
      "50% dashdot black\n",
      "95% dashed navy\n",
      "timeEnsemble solid green\n",
      "freqEnsemble solid red\n",
      "5% dotted blue\n",
      "50% dashdot black\n",
      "95% dashed navy\n",
      "timeEnsemble solid green\n",
      "freqEnsemble solid red\n",
      "5% dotted blue\n",
      "50% dashdot black\n",
      "95% dashed navy\n",
      "timeEnsemble solid green\n",
      "freqEnsemble solid red\n",
      "5% dotted blue\n",
      "50% dashdot black\n",
      "95% dashed navy\n"
     ]
    }
   ],
   "source": [
    "plots = []\n",
    "for m in ['RMSE', 'NSE', 'KGE', 'RE', 'KLD', 'EMD']:\n",
    "    mplt = []\n",
    "    for date in dates:\n",
    "        data = metric_dict[date][m]\n",
    "        x_label = m\n",
    "        if m in ['NSE', 'KGE']:\n",
    "            x_label = f'1 - {m}'\n",
    "        if len(mplt) > 0:\n",
    "            p = figure(title=f'{m}: {date}', x_axis_label=x_label, y_axis_label='Density', \n",
    "                        width=800, height=400, x_axis_type='log', \n",
    "                        x_range=mplt[0].x_range, y_range=mplt[0].y_range)\n",
    "        else:\n",
    "            p = figure(title=f'{m}: {date}', x_axis_label=x_label, y_axis_label='Density', \n",
    "                        width=800, height=400, x_axis_type='log')\n",
    "        for s, vals in data.items():\n",
    "            line = 'solid'\n",
    "            legend_label = s\n",
    "            if s.startswith('95%'):\n",
    "                line = 'dashed'\n",
    "                legend_label = f'Sim Ensemble (95%)'\n",
    "            elif s.startswith('5%'):\n",
    "                line = 'dotted'\n",
    "                legend_label = f'Sim Ensemble (5%)'\n",
    "            elif s.startswith('50%'):\n",
    "                line='dashdot'\n",
    "                legend_label = f'Sim Ensemble (50%)'\n",
    "            if m in ['NSE', 'KGE']:\n",
    "                vals = [1-v for v in vals]\n",
    "            x, y = compute_empirical_cdf(np.array(vals))\n",
    "            print(s, line, cdict[s])\n",
    "            p.line(x=x, y=y, legend_label=legend_label, color=cdict[s], line_width=2.5, line_dash=line)\n",
    "        p.legend.location='top_left'\n",
    "        p.legend.click_policy = 'hide'\n",
    "        plots.append(p)\n",
    "        mplt.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae94fc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = gridplot(plots, ncols=2, width=700, height=400)\n",
    "# show(layout)\n",
    "# same the plot html\n",
    "output_fname = BASE_DIR / 'data' / 'LSTM_ensemble_metrics.html'\n",
    "output_file(output_fname, title='LSTM Ensemble Metrics')\n",
    "save(layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85f2a17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4afd6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
