{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71f97393",
   "metadata": {},
   "source": [
    "### Create diagnostic dashboard summaries for each catchment in the sample\n",
    "\n",
    "Create standalone HTML files with embedded plots and tables for each catchment in the large sample.  These will be rendered as searchable results diagnostic pages in the web-based jupyter book documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "558cd32f-b562-4502-ab63-826aede8f321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"d87337b8-e546-4aa6-9a2f-d96dba76cc5a\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"d87337b8-e546-4aa6-9a2f-d96dba76cc5a\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.8.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.8.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.8.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.8.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.8.0.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"d87337b8-e546-4aa6-9a2f-d96dba76cc5a\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import math\n",
    "import json\n",
    "import sqlite3\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# import geopandas as gpd\n",
    "# from shapely.geometry import Point\n",
    "import xyzservices.providers as xyz\n",
    "\n",
    "from bokeh.plotting import figure, output_file, save, show\n",
    "from bokeh.layouts import gridplot, row, column\n",
    "\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import Div\n",
    "\n",
    "from utils.kde_estimator import KDEEstimator\n",
    "from utils.evaluation_metrics import EvaluationMetrics\n",
    "# from utils.fdc_estimator_context import FDCEstimationContext\n",
    "# from utils.fdc_data import StationData\n",
    "\n",
    "import utils.data_processing_functions as dpf\n",
    "\n",
    "import xyzservices.providers as xyz\n",
    "tiles = xyz['USGS']['USTopo']\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9525e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Watershed_ID</th>\n",
       "      <th>Source</th>\n",
       "      <th>Name</th>\n",
       "      <th>Official_ID</th>\n",
       "      <th>Centroid_Lat_deg_N</th>\n",
       "      <th>Centroid_Lon_deg_E</th>\n",
       "      <th>Drainage_Area_km2</th>\n",
       "      <th>Drainage_Area_GSIM_km2</th>\n",
       "      <th>Flag_GSIM_boundaries</th>\n",
       "      <th>Flag_Artificial_Boundaries</th>\n",
       "      <th>...</th>\n",
       "      <th>Land_Use_Wetland_frac</th>\n",
       "      <th>Land_Use_Water_frac</th>\n",
       "      <th>Land_Use_Urban_frac</th>\n",
       "      <th>Land_Use_Shrubs_frac</th>\n",
       "      <th>Land_Use_Crops_frac</th>\n",
       "      <th>Land_Use_Snow_Ice_frac</th>\n",
       "      <th>Flag_Land_Use_Extraction</th>\n",
       "      <th>Permeability_logk_m2</th>\n",
       "      <th>Porosity_frac</th>\n",
       "      <th>Flag_Subsoil_Extraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>847</td>\n",
       "      <td>HYDAT</td>\n",
       "      <td>CROWSNEST RIVER AT FRANK</td>\n",
       "      <td>05AA008</td>\n",
       "      <td>49.59732</td>\n",
       "      <td>-114.4106</td>\n",
       "      <td>402.6522</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>0.0785</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>1</td>\n",
       "      <td>-15.543306</td>\n",
       "      <td>0.170479</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>850</td>\n",
       "      <td>HYDAT</td>\n",
       "      <td>CASTLE RIVER NEAR BEAVER MINES</td>\n",
       "      <td>05AA022</td>\n",
       "      <td>49.48866</td>\n",
       "      <td>-114.1444</td>\n",
       "      <td>820.6510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.1156</td>\n",
       "      <td>0.0246</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>-15.929747</td>\n",
       "      <td>0.150196</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Watershed_ID Source                            Name Official_ID  \\\n",
       "846           847  HYDAT        CROWSNEST RIVER AT FRANK     05AA008   \n",
       "849           850  HYDAT  CASTLE RIVER NEAR BEAVER MINES     05AA022   \n",
       "\n",
       "     Centroid_Lat_deg_N  Centroid_Lon_deg_E  Drainage_Area_km2  \\\n",
       "846            49.59732           -114.4106           402.6522   \n",
       "849            49.48866           -114.1444           820.6510   \n",
       "\n",
       "     Drainage_Area_GSIM_km2  Flag_GSIM_boundaries  Flag_Artificial_Boundaries  \\\n",
       "846                     NaN                     0                           0   \n",
       "849                     NaN                     0                           0   \n",
       "\n",
       "     ...  Land_Use_Wetland_frac  Land_Use_Water_frac  Land_Use_Urban_frac  \\\n",
       "846  ...                 0.0103               0.0065               0.0328   \n",
       "849  ...                 0.0058               0.0023               0.0105   \n",
       "\n",
       "     Land_Use_Shrubs_frac  Land_Use_Crops_frac  Land_Use_Snow_Ice_frac  \\\n",
       "846                0.0785               0.0015                  0.0002   \n",
       "849                0.1156               0.0246                  0.0000   \n",
       "\n",
       "     Flag_Land_Use_Extraction  Permeability_logk_m2  Porosity_frac  \\\n",
       "846                         1            -15.543306       0.170479   \n",
       "849                         1            -15.929747       0.150196   \n",
       "\n",
       "     Flag_Subsoil_Extraction  \n",
       "846                        1  \n",
       "849                        1  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_DIR = Path(os.getcwd())\n",
    "\n",
    "from utils.table_notes import notes_html\n",
    "attr_fpath = 'data/BCUB_watershed_attributes_updated_20250227.csv'\n",
    "attr_df = pd.read_csv(attr_fpath, dtype={'Official_ID': str})\n",
    "station_ids = sorted(attr_df['official_id'].unique().tolist())\n",
    "\n",
    "# streamflow folder from (updated) HYSETS\n",
    "HYSETS_DIR = Path('/home/danbot/code/common_data/HYSETS')\n",
    "hs_df = pd.read_csv('data/HYSETS_watershed_properties.txt', sep=';')\n",
    "hs_df = hs_df[hs_df['Official_ID'].isin(station_ids)]\n",
    "hs_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1356ccc6-3d6d-4e08-bdbc-53a798187d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 723 monitored basins concurrent with LSTM ensemble results.\n"
     ]
    }
   ],
   "source": [
    "# retrieve LSTM ensemble predictions\n",
    "lstm_result_base_folder = Path('/home/danbot/code/neuralhydrology/data/')\n",
    "results_revisions = ['20250514', '20250627']\n",
    "lstm_result_files = os.listdir(lstm_result_base_folder / f'ensemble_results_{results_revisions[0]}')\n",
    "lstm_result_stns = [e.split('_')[0] for e in lstm_result_files]\n",
    "\n",
    "# filter for the common stations between BCUB region and LSTM-compatible (i.e. 1980-)\n",
    "daymet_concurrent_stations = list(set(station_ids) & set(lstm_result_stns))\n",
    "# assert '012414900' in daymet_concurrent_stations\n",
    "print(f'There are {len(daymet_concurrent_stations)} monitored basins concurrent with LSTM ensemble results.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f016b1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see notebook 1 for details on how these were identified\n",
    "exclude_stations = ['08FA009', '08GA037', '08NC003', '12052500', '12090480', '12107950', '12108450', '12119300', \n",
    "                    '12119450', '12200684', '12200762', '12203000', '12409500', '15056070', '15081510',\n",
    "                    '12323760', '12143700', '12143900', '12398000', '12058800', '12137800', '12100000']\n",
    "daymet_concurrent_stations = [s for s in daymet_concurrent_stations if s not in exclude_stations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9c026ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "watershed_id_dict = {row['Watershed_ID']: row['Official_ID'] for _, row in hs_df.iterrows()}\n",
    "# and the inverse\n",
    "official_id_dict = {row['Official_ID']: row['Watershed_ID'] for _, row in hs_df.iterrows()}\n",
    "# also for drainage areas\n",
    "da_dict = {row['Official_ID']: row['Drainage_Area_km2'] for _, row in hs_df.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e37ba53",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmf_path = Path(os.getcwd()) / 'data' / 'results' / 'baseline_distributions' / f'pmf_obs.csv'\n",
    "pmf_obs_df = pd.read_csv(pmf_path)\n",
    "log_edges = np.concatenate([pmf_obs_df['left_log_edges'].values[:1], pmf_obs_df['right_log_edges'].values])\n",
    "log_w = np.diff(log_edges)\n",
    "eval_obj = EvaluationMetrics(log_x=pmf_obs_df['log_x'].values, log_w=log_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b7a4e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_filter_hysets_data(station_ids, hs_df):\n",
    "    hs_df = hs_df[hs_df['Official_ID'].isin(station_ids)]\n",
    "\n",
    "    # load the updated HYSETS data\n",
    "    updated_filename = 'HYSETS_2023_update_QC_stations.nc'\n",
    "    ds = xr.open_dataset(HYSETS_DIR / updated_filename)\n",
    "\n",
    "    # Get valid IDs as a NumPy array\n",
    "    selected_ids = hs_df['Watershed_ID'].values\n",
    "\n",
    "    # Get boolean index where watershedID in selected_set\n",
    "    # safely access watershedID as a variable first\n",
    "    ws_ids = ds['watershedID'].data  # or .values if you prefer\n",
    "    mask = np.isin(ws_ids, selected_ids)\n",
    "\n",
    "    # Apply mask to data\n",
    "    ds = ds.sel(watershed=mask)\n",
    "    # Step 1: Promote 'watershedID' to a coordinate on the 'watershed' dimension\n",
    "    ds = ds.assign_coords(watershedID=(\"watershed\", ds[\"watershedID\"].data))\n",
    "\n",
    "    # Step 2: Set 'watershedID' as the index for the 'watershed' dimension\n",
    "    return ds.set_index(watershed=\"watershedID\")\n",
    "\n",
    "\n",
    "ds = load_and_filter_hysets_data(station_ids, hs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cfb687e-91f2-4065-8900-bd43b1d9d057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_knn_label_col(df):\n",
    "    # Split the string column\n",
    "    # Determine format based on length\n",
    "    split_labels = df['Label'].str.split('_')\n",
    "    df['n_parts'] = split_labels.str.len()\n",
    "\n",
    "    assert len(set(df['n_parts'])) == 1, \"Not all labels have the same number of parts\"\n",
    "\n",
    "    # Define expected column structures\n",
    "    # format_a_cols = [\"Official_ID\", \"k\", \"NN\", 'concurrent', 'tree_type', 'dist', 'weighting', 'ensemble_method']\n",
    "    format_cols = [\"Official_ID\", \"k\", \"NN\", 'tree_type', 'dist', 'ensemble_weight', 'ensemble_method']\n",
    "\n",
    "    # Subset by format\n",
    "    df_a = df[df['n_parts'] == len(format_cols)].copy()\n",
    "\n",
    "    # Split and join with suffix to avoid conflicts\n",
    "    df_a_split = df_a['Label'].str.split('_', expand=True)\n",
    "    df_a_split.columns = format_cols\n",
    "    merged = pd.concat([df_a.reset_index(drop=True), df_a_split.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # Drop duplicates (if any) and update\n",
    "    merged.drop(columns=['NN', 'dist', 'n_parts', 'minYears', 'minOverlapPct'], errors='ignore', inplace=True)\n",
    "    merged = merged.loc[:, ~merged.columns.duplicated()]\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea3af379-ae22-464e-a816-8d959d69a28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 716 monitored basins concurrent with LSTM ensemble results.\n"
     ]
    }
   ],
   "source": [
    "# retrieve LSTM ensemble predictions\n",
    "LSTM_ensemble_result_folder = '/home/danbot/code/neuralhydrology/data/ensemble_results_20250514'# uses NSE mean as loss function\n",
    "# LSTM_ensemble_result_folder = '/home/danbot/code/neuralhydrology/data/ensemble_results_20250627'# uses NSE 95% as loss function\n",
    "lstm_result_files = os.listdir(LSTM_ensemble_result_folder)\n",
    "lstm_result_stns = [e.split('_')[0] for e in lstm_result_files]\n",
    "\n",
    "# filter for the common stations between BCUB region and LSTM-compatible (i.e. 1980-)\n",
    "daymet_concurrent_stations = list(set(station_ids) & set(lstm_result_stns))\n",
    "daymet_concurrent_stations = [s for s in daymet_concurrent_stations if s not in exclude_stations]\n",
    "# assert '012414900' in daymet_concurrent_stations\n",
    "print(f'There are {len(daymet_concurrent_stations)} monitored basins concurrent with LSTM ensemble results.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcfd8a6",
   "metadata": {},
   "source": [
    "### Compute the NSE on the daily timeseries for evaluation over the sample\n",
    "\n",
    "Compute the distribution of NSE values for the LSTM ensembles to report as a benchmark in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10383870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_NSE_for_lstm(obs, sim):\n",
    "    mean_observed = np.mean(obs)\n",
    "    numerator = np.sum((obs - sim) ** 2)\n",
    "    denominator = np.sum((obs - mean_observed) ** 2)\n",
    "    nse = 1 - (numerator / denominator)\n",
    "    return nse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3123a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "NSE_vals = []\n",
    "for f in lstm_result_files:\n",
    "    stn = f.split('_')[0]\n",
    "    if stn in exclude_stations:\n",
    "        continue\n",
    "    ldf = pd.read_csv(Path(LSTM_ensemble_result_folder) / f)\n",
    "    # compute the ensemble mean time sieres\n",
    "    ldf['qsim_mean'] = ldf[[c for c in ldf.columns if '_sim_' in c]].mean(axis=1)\n",
    "    ldf.dropna(subset=['streamflow_obs', 'qsim_mean'], inplace=True)\n",
    "    # compute the NSE on the daily timeseries\n",
    "    nse = compute_NSE_for_lstm(ldf['streamflow_obs'].values, ldf['qsim_mean'].values)\n",
    "    NSE_vals.append(nse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "817e08ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"af5f7eaa-f19c-4f22-89ac-b08788f166ef\" data-root-id=\"p1008\" style=\"display: contents;\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function embed_document(root) {\n  const docs_json = {\"86f86f32-6ebd-4cf6-bc60-04d96360793c\":{\"version\":\"3.8.0\",\"title\":\"Bokeh Application\",\"config\":{\"type\":\"object\",\"name\":\"DocumentConfig\",\"id\":\"p1054\",\"attributes\":{\"notifications\":{\"type\":\"object\",\"name\":\"Notifications\",\"id\":\"p1055\"}}},\"roots\":[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1008\",\"attributes\":{\"height\":400,\"x_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1009\"},\"y_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1010\"},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1018\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1019\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1011\",\"attributes\":{\"text\":\"Distribution of NSE values for LSTM ensemble\"}},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1049\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1043\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1044\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1045\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"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\"},\"shape\":[718],\"dtype\":\"float64\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"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\"},\"shape\":[718],\"dtype\":\"float64\",\"order\":\"little\"}]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1050\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1051\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1046\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"dodgerblue\",\"line_width\":2}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1047\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"dodgerblue\",\"line_alpha\":0.1,\"line_width\":2}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1048\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"dodgerblue\",\"line_alpha\":0.2,\"line_width\":2}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1017\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1030\"},{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1031\",\"attributes\":{\"renderers\":\"auto\"}},{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1032\",\"attributes\":{\"dimensions\":\"both\",\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1033\",\"attributes\":{\"syncable\":false,\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5,\"level\":\"overlay\",\"visible\":false,\"left\":{\"type\":\"number\",\"value\":\"nan\"},\"right\":{\"type\":\"number\",\"value\":\"nan\"},\"top\":{\"type\":\"number\",\"value\":\"nan\"},\"bottom\":{\"type\":\"number\",\"value\":\"nan\"},\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"handles\":{\"type\":\"object\",\"name\":\"BoxInteractionHandles\",\"id\":\"p1039\",\"attributes\":{\"all\":{\"type\":\"object\",\"name\":\"AreaVisuals\",\"id\":\"p1038\",\"attributes\":{\"fill_color\":\"white\",\"hover_fill_color\":\"lightgray\"}}}}}}}},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1040\"},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1041\"},{\"type\":\"object\",\"name\":\"HelpTool\",\"id\":\"p1042\"}]}},\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1025\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1026\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1027\"},\"axis_label\":\"Frequency\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1028\"}}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1020\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1021\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1022\"},\"axis_label\":\"NSE\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1023\"}}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1024\",\"attributes\":{\"axis\":{\"id\":\"p1020\"}}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1029\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1025\"}}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1052\",\"attributes\":{\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1053\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"LSTM Ensemble\"},\"renderers\":[{\"id\":\"p1049\"}]}}]}}]}}]}};\n  const render_items = [{\"docid\":\"86f86f32-6ebd-4cf6-bc60-04d96360793c\",\"roots\":{\"p1008\":\"af5f7eaa-f19c-4f22-89ac-b08788f166ef\"},\"root_ids\":[\"p1008\"]}];\n  void root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n  }\n  if (root.Bokeh !== undefined) {\n    embed_document(root);\n  } else {\n    let attempts = 0;\n    const timer = setInterval(function(root) {\n      if (root.Bokeh !== undefined) {\n        clearInterval(timer);\n        embed_document(root);\n      } else {\n        attempts++;\n        if (attempts > 100) {\n          clearInterval(timer);\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n        }\n      }\n    }, 10, root)\n  }\n})(window);",
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "p1008"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the distribution of NSE values with bokeh\n",
    "p = figure(title='Distribution of NSE values for LSTM ensemble', \n",
    "           x_axis_label='NSE', y_axis_label='Frequency', \n",
    "           width=600, height=400)\n",
    "# compute the empirical cdf\n",
    "x = np.sort(NSE_vals)\n",
    "assert np.all(np.isfinite(x))\n",
    "y = np.arange(1, len(x) + 1) / len(x)\n",
    "p.line(x, y, line_width=2, color='dodgerblue', legend_label='LSTM Ensemble')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1251a51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean NSE: 0.56, Median NSE: 0.83, 95% CI: (-0.77, 0.96), % Failures: 35/718 4.87%\n"
     ]
    }
   ],
   "source": [
    "mean_nse, median_nse, ci_nse = np.mean(NSE_vals), np.median(NSE_vals), (np.percentile(NSE_vals, 2.5), np.percentile(NSE_vals, 97.5))\n",
    "n_failures = np.sum(np.array(NSE_vals) <= 0)\n",
    "pct_failures = n_failures / len(NSE_vals) * 100\n",
    "print(f'Mean NSE: {mean_nse:.2f}, Median NSE: {median_nse:.2f}, 95% CI: ({ci_nse[0]:.2f}, {ci_nse[1]:.2f}), % Failures: {n_failures}/{len(NSE_vals)} {pct_failures:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b80c4b12-832e-4481-886b-558db49922c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 715 completed stations in lstm_20250514 results folder.\n",
      "   Loading parametric results\n",
      "   Loaded 2860 parametric results from data/results/parametric_all_results.csv\n",
      "   Loading lstm results\n",
      "   Loaded 1430 lstm results from data/results/lstm_all_results_20250514.csv\n",
      "   Loading knn results\n",
      "   Loaded 57200 knn results from data/results/knn_all_results.csv\n"
     ]
    }
   ],
   "source": [
    "results_dfs = {}\n",
    "lstm_rev_date = LSTM_ensemble_result_folder.split('_')[-1]\n",
    "sub_folder = f'lstm_{lstm_rev_date}' \n",
    "# results_folder = '/media/danbot/Samsung_T5/fdc_estimation_results/'\n",
    "results_folder = 'data/results/fdc_estimation_results'\n",
    "completed_stns = [c.split('_')[0] for c in os.listdir(os.path.join(results_folder, sub_folder))]\n",
    "print(f'Found {len(set(completed_stns))} completed stations in {sub_folder} results folder.')\n",
    "\n",
    "for method in ['parametric', 'lstm', 'knn']:\n",
    "    print(f'   Loading {method} results')\n",
    "    method_results_fpath = os.path.join('data', 'results', f'{method}_all_results.csv')\n",
    "    if method == 'lstm':\n",
    "        rev_date = LSTM_ensemble_result_folder.split('_')[-1]\n",
    "        method_results_fpath = os.path.join('data', 'results', f'{method}_all_results_{rev_date}.csv')\n",
    "    if os.path.exists(method_results_fpath):\n",
    "        results_dfs[method] = pd.read_csv(method_results_fpath, dtype={'Official_ID': str})\n",
    "        print(f'   Loaded {len(results_dfs[method])} {method} results from {method_results_fpath}')\n",
    "    else:\n",
    "        print(f'   {method} results not found in {method_results_fpath}, loading from individual station files...')\n",
    "        res_folder = os.path.join(results_folder, method)\n",
    "        if method == 'lstm':\n",
    "            res_folder = os.path.join(results_folder, f'{method}_{rev_date}')\n",
    "        args = [(stn, res_folder, method) for stn in completed_stns]\n",
    "        with Pool() as pool:\n",
    "            results_list = pool.map(dpf.load_results, args)\n",
    "\n",
    "        foo = pd.concat(results_list, ignore_index=True)\n",
    "        bad_dkl = foo[foo['KLD'].isna() | (foo['KLD'] < 0)].copy()\n",
    "        if not bad_dkl.empty:\n",
    "            print(f'Warning: {len(bad_dkl)} {method} rows with NaN or negative DKL values.')\n",
    "            bad_stns = bad_dkl['Official_ID'].values\n",
    "            raise Exception(f'Results have {len(bad_stns)} NaN or negative DKL values: {bad_stns}')\n",
    "        method_results = pd.concat(results_list, ignore_index=True)\n",
    "        results_dfs[method] = method_results\n",
    "        print(f'   Loaded {int(len(results_dfs[method])/len(set(completed_stns)))} station results for {method} results')\n",
    "        method_results.to_csv(method_results_fpath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98bc7051-29f9-42cf-94b5-d6aa1c936f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4290\n",
      "['MLE', 'PredictedLog', 'PredictedMOM', 'RandomDraw', 'frequency', 'time']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Official_ID</th>\n",
       "      <th>Label</th>\n",
       "      <th>KLD</th>\n",
       "      <th>EMD</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MB</th>\n",
       "      <th>RB</th>\n",
       "      <th>MARE</th>\n",
       "      <th>NSE</th>\n",
       "      <th>KGE</th>\n",
       "      <th>VE</th>\n",
       "      <th>VB_PMF</th>\n",
       "      <th>VB_FDC</th>\n",
       "      <th>MEAN_FRAC_DIFF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08EE008</td>\n",
       "      <td>MLE</td>\n",
       "      <td>0.129513</td>\n",
       "      <td>8.8949</td>\n",
       "      <td>0.225032</td>\n",
       "      <td>-0.185829</td>\n",
       "      <td>-0.014232</td>\n",
       "      <td>0.175017</td>\n",
       "      <td>0.968031</td>\n",
       "      <td>0.982444</td>\n",
       "      <td>0.828507</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>08EE008</td>\n",
       "      <td>PredictedLog</td>\n",
       "      <td>0.158698</td>\n",
       "      <td>13.1931</td>\n",
       "      <td>0.381366</td>\n",
       "      <td>-5.580974</td>\n",
       "      <td>-0.427429</td>\n",
       "      <td>0.415715</td>\n",
       "      <td>0.908183</td>\n",
       "      <td>0.833915</td>\n",
       "      <td>0.571025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>08EE008</td>\n",
       "      <td>PredictedMOM</td>\n",
       "      <td>0.392917</td>\n",
       "      <td>11.9852</td>\n",
       "      <td>0.728415</td>\n",
       "      <td>-6.385106</td>\n",
       "      <td>-0.489015</td>\n",
       "      <td>1.020078</td>\n",
       "      <td>0.665034</td>\n",
       "      <td>0.589825</td>\n",
       "      <td>0.510985</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>08EE008</td>\n",
       "      <td>RandomDraw</td>\n",
       "      <td>0.165444</td>\n",
       "      <td>9.5843</td>\n",
       "      <td>0.367609</td>\n",
       "      <td>4.142383</td>\n",
       "      <td>0.317252</td>\n",
       "      <td>0.255449</td>\n",
       "      <td>0.914687</td>\n",
       "      <td>0.819094</td>\n",
       "      <td>0.675923</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>09AA013</td>\n",
       "      <td>MLE</td>\n",
       "      <td>0.222709</td>\n",
       "      <td>7.4458</td>\n",
       "      <td>0.231586</td>\n",
       "      <td>0.447600</td>\n",
       "      <td>0.028895</td>\n",
       "      <td>0.182911</td>\n",
       "      <td>0.941227</td>\n",
       "      <td>0.967423</td>\n",
       "      <td>0.858849</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Official_ID         Label       KLD      EMD      RMSE        MB        RB  \\\n",
       "0     08EE008           MLE  0.129513   8.8949  0.225032 -0.185829 -0.014232   \n",
       "1     08EE008  PredictedLog  0.158698  13.1931  0.381366 -5.580974 -0.427429   \n",
       "2     08EE008  PredictedMOM  0.392917  11.9852  0.728415 -6.385106 -0.489015   \n",
       "3     08EE008    RandomDraw  0.165444   9.5843  0.367609  4.142383  0.317252   \n",
       "4     09AA013           MLE  0.222709   7.4458  0.231586  0.447600  0.028895   \n",
       "\n",
       "       MARE       NSE       KGE        VE  VB_PMF  VB_FDC  MEAN_FRAC_DIFF  \n",
       "0  0.175017  0.968031  0.982444  0.828507     NaN     NaN             NaN  \n",
       "1  0.415715  0.908183  0.833915  0.571025     NaN     NaN             NaN  \n",
       "2  1.020078  0.665034  0.589825  0.510985     NaN     NaN             NaN  \n",
       "3  0.255449  0.914687  0.819094  0.675923     NaN     NaN             NaN  \n",
       "4  0.182911  0.941227  0.967423  0.858849     NaN     NaN             NaN  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdc_df = pd.concat([results_dfs['parametric'], results_dfs['lstm']], axis=0)\n",
    "# fdc_df = results_dfs['parametric'].copy()\n",
    "np.unique(fdc_df['Label'].values)\n",
    "results_dfs['parametric'].keys()\n",
    "print(len(fdc_df))\n",
    "model_labels = sorted(list(set(fdc_df['Label'])))\n",
    "print(model_labels)\n",
    "fdc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76045271-54b5-4116-bfbc-2925afac1630",
   "metadata": {},
   "outputs": [],
   "source": [
    "parametric_targets = list(set(results_dfs['parametric']['Label'].values))\n",
    "results_dfs['knn'] = split_knn_label_col(results_dfs['knn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e23844b2-9307-4383-93e0-6d90f843733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result_and_ids(label, metric):\n",
    "    data = fdc_df[fdc_df['Label'] == label].copy()\n",
    "    data = data.dropna(subset=[metric])\n",
    "    values = data[metric].values\n",
    "    if metric in ['NSE', 'KGE']:\n",
    "        # for NSE and KGE, we want to plot the upper bound as the maximum value\n",
    "        values = 1 - values\n",
    "    return values, data['Official_ID']\n",
    "\n",
    "\n",
    "def get_knn_group_results(tree_type='attribute', ensemble_type='freqEnsemble', weighting='ID2', k=7, which_set='knn'):\n",
    "    data = results_dfs[which_set].copy()\n",
    "    data = data[data['tree_type'] == 'attribute']\n",
    "    data = data[data['ensemble_method'] == ensemble_type]\n",
    "    data = data[data['ensemble_weight'] == weighting]\n",
    "    data = data[data['k'] == str(k)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2ca5e81-b17a-479c-953c-cf862e8a03b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_result_vals = {}\n",
    "all_metrics = ['KLD', 'EMD', 'RMSE', 'RB', 'MB', 'NSE', 'KGE', 'VE', 'VB_PMF', 'VB_FDC', 'MEAN_FRAC_DIFF']\n",
    "tree_type = 'attribute'\n",
    "# Define metrics that are naturally \"higher is better\" and need inversion to match \"lower is better\"\n",
    "invert_metrics = {'NSE', 'KGE', 'VE'}\n",
    "\n",
    "def invert_if_needed(values, dm):\n",
    "    \"\"\"Ensure metric values follow good = 0, bad = large positive\"\"\"\n",
    "    values = np.asarray(values)\n",
    "    if dm in invert_metrics:\n",
    "        if np.nanmin(values) < 0.:  # assumes values ~ [-inf, 1] if not yet inverted\n",
    "            return 1 - values\n",
    "    return values\n",
    "\n",
    "# Loop through each metric\n",
    "for dm in all_metrics:    \n",
    "    # Parametric LN MoM\n",
    "    for label, name in [('PredictedMOM', 'LN MoM'), ('PredictedLog', 'LN Direct'), ('MLE', 'MLE')]:\n",
    "        data, ids = get_result_and_ids(label, dm)\n",
    "        data = invert_if_needed(data, dm)\n",
    "        main_result_vals[f'{name} {dm}'] = pd.DataFrame({'ids': ids, 'values': data})\n",
    "\n",
    "    # kNN group results\n",
    "    for k in [2, 4, 8]:\n",
    "        knn_df = get_knn_group_results(k=k)\n",
    "        data = invert_if_needed(knn_df[dm].values, dm)\n",
    "        ids = knn_df['Official_ID'].values\n",
    "        main_result_vals[f'{k} kNN {dm}'] = pd.DataFrame({'ids': ids, 'values': data})\n",
    "\n",
    "    # LSTM models\n",
    "    for label, suffix in [('time', 'LSTM time'), ('frequency', 'LSTM dist.')]:\n",
    "        subset = fdc_df[fdc_df['Label'] == label]\n",
    "        data = invert_if_needed(subset[dm].values, dm)\n",
    "        ids = subset['Official_ID'].values\n",
    "        main_result_vals[f'{suffix} {dm}'] = pd.DataFrame({'ids': ids, 'values': data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba75d83-45a6-4e09-a43c-1caec48a8225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LN MoM KLD</th>\n",
       "      <th>LN Direct KLD</th>\n",
       "      <th>MLE KLD</th>\n",
       "      <th>2 kNN KLD</th>\n",
       "      <th>4 kNN KLD</th>\n",
       "      <th>8 kNN KLD</th>\n",
       "      <th>LSTM time KLD</th>\n",
       "      <th>LSTM dist. KLD</th>\n",
       "      <th>LN MoM EMD</th>\n",
       "      <th>LN Direct EMD</th>\n",
       "      <th>...</th>\n",
       "      <th>LSTM time VB_FDC</th>\n",
       "      <th>LSTM dist. VB_FDC</th>\n",
       "      <th>LN MoM MEAN_FRAC_DIFF</th>\n",
       "      <th>LN Direct MEAN_FRAC_DIFF</th>\n",
       "      <th>MLE MEAN_FRAC_DIFF</th>\n",
       "      <th>2 kNN MEAN_FRAC_DIFF</th>\n",
       "      <th>4 kNN MEAN_FRAC_DIFF</th>\n",
       "      <th>8 kNN MEAN_FRAC_DIFF</th>\n",
       "      <th>LSTM time MEAN_FRAC_DIFF</th>\n",
       "      <th>LSTM dist. MEAN_FRAC_DIFF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>715.000000</td>\n",
       "      <td>715.000000</td>\n",
       "      <td>715.000000</td>\n",
       "      <td>715.000000</td>\n",
       "      <td>715.000000</td>\n",
       "      <td>715.000000</td>\n",
       "      <td>715.000000</td>\n",
       "      <td>715.000000</td>\n",
       "      <td>715.000000</td>\n",
       "      <td>715.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.451058</td>\n",
       "      <td>0.354213</td>\n",
       "      <td>0.276437</td>\n",
       "      <td>0.523235</td>\n",
       "      <td>0.430187</td>\n",
       "      <td>0.390798</td>\n",
       "      <td>0.593528</td>\n",
       "      <td>0.363069</td>\n",
       "      <td>12.904535</td>\n",
       "      <td>15.193866</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.541223</td>\n",
       "      <td>0.387011</td>\n",
       "      <td>0.329113</td>\n",
       "      <td>0.765469</td>\n",
       "      <td>0.637722</td>\n",
       "      <td>0.566807</td>\n",
       "      <td>0.733763</td>\n",
       "      <td>0.498149</td>\n",
       "      <td>10.742931</td>\n",
       "      <td>13.338878</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.033893</td>\n",
       "      <td>0.032935</td>\n",
       "      <td>0.020722</td>\n",
       "      <td>0.024780</td>\n",
       "      <td>0.024890</td>\n",
       "      <td>0.023272</td>\n",
       "      <td>0.034882</td>\n",
       "      <td>0.022229</td>\n",
       "      <td>4.815600</td>\n",
       "      <td>4.464800</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.175558</td>\n",
       "      <td>0.163618</td>\n",
       "      <td>0.115455</td>\n",
       "      <td>0.110361</td>\n",
       "      <td>0.104760</td>\n",
       "      <td>0.111420</td>\n",
       "      <td>0.146002</td>\n",
       "      <td>0.084951</td>\n",
       "      <td>7.481900</td>\n",
       "      <td>7.689500</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.283326</td>\n",
       "      <td>0.240475</td>\n",
       "      <td>0.191308</td>\n",
       "      <td>0.237662</td>\n",
       "      <td>0.202776</td>\n",
       "      <td>0.196894</td>\n",
       "      <td>0.318790</td>\n",
       "      <td>0.172647</td>\n",
       "      <td>9.620200</td>\n",
       "      <td>10.632300</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.497868</td>\n",
       "      <td>0.361168</td>\n",
       "      <td>0.290657</td>\n",
       "      <td>0.563115</td>\n",
       "      <td>0.480097</td>\n",
       "      <td>0.425681</td>\n",
       "      <td>0.683473</td>\n",
       "      <td>0.425735</td>\n",
       "      <td>13.700050</td>\n",
       "      <td>16.356550</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.522313</td>\n",
       "      <td>3.108440</td>\n",
       "      <td>2.408180</td>\n",
       "      <td>5.601580</td>\n",
       "      <td>4.775778</td>\n",
       "      <td>4.060141</td>\n",
       "      <td>4.806284</td>\n",
       "      <td>3.450067</td>\n",
       "      <td>101.706600</td>\n",
       "      <td>111.485700</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       LN MoM KLD  LN Direct KLD     MLE KLD   2 kNN KLD   4 kNN KLD  \\\n",
       "count  715.000000     715.000000  715.000000  715.000000  715.000000   \n",
       "mean     0.451058       0.354213    0.276437    0.523235    0.430187   \n",
       "std      0.541223       0.387011    0.329113    0.765469    0.637722   \n",
       "min      0.033893       0.032935    0.020722    0.024780    0.024890   \n",
       "25%      0.175558       0.163618    0.115455    0.110361    0.104760   \n",
       "50%      0.283326       0.240475    0.191308    0.237662    0.202776   \n",
       "75%      0.497868       0.361168    0.290657    0.563115    0.480097   \n",
       "max      5.522313       3.108440    2.408180    5.601580    4.775778   \n",
       "\n",
       "        8 kNN KLD  LSTM time KLD  LSTM dist. KLD  LN MoM EMD  LN Direct EMD  \\\n",
       "count  715.000000     715.000000      715.000000  715.000000     715.000000   \n",
       "mean     0.390798       0.593528        0.363069   12.904535      15.193866   \n",
       "std      0.566807       0.733763        0.498149   10.742931      13.338878   \n",
       "min      0.023272       0.034882        0.022229    4.815600       4.464800   \n",
       "25%      0.111420       0.146002        0.084951    7.481900       7.689500   \n",
       "50%      0.196894       0.318790        0.172647    9.620200      10.632300   \n",
       "75%      0.425681       0.683473        0.425735   13.700050      16.356550   \n",
       "max      4.060141       4.806284        3.450067  101.706600     111.485700   \n",
       "\n",
       "       ...  LSTM time VB_FDC  LSTM dist. VB_FDC  LN MoM MEAN_FRAC_DIFF  \\\n",
       "count  ...               0.0                0.0                    0.0   \n",
       "mean   ...               NaN                NaN                    NaN   \n",
       "std    ...               NaN                NaN                    NaN   \n",
       "min    ...               NaN                NaN                    NaN   \n",
       "25%    ...               NaN                NaN                    NaN   \n",
       "50%    ...               NaN                NaN                    NaN   \n",
       "75%    ...               NaN                NaN                    NaN   \n",
       "max    ...               NaN                NaN                    NaN   \n",
       "\n",
       "       LN Direct MEAN_FRAC_DIFF  MLE MEAN_FRAC_DIFF  2 kNN MEAN_FRAC_DIFF  \\\n",
       "count                       0.0                 0.0                   0.0   \n",
       "mean                        NaN                 NaN                   NaN   \n",
       "std                         NaN                 NaN                   NaN   \n",
       "min                         NaN                 NaN                   NaN   \n",
       "25%                         NaN                 NaN                   NaN   \n",
       "50%                         NaN                 NaN                   NaN   \n",
       "75%                         NaN                 NaN                   NaN   \n",
       "max                         NaN                 NaN                   NaN   \n",
       "\n",
       "       4 kNN MEAN_FRAC_DIFF  8 kNN MEAN_FRAC_DIFF  LSTM time MEAN_FRAC_DIFF  \\\n",
       "count                   0.0                   0.0                       0.0   \n",
       "mean                    NaN                   NaN                       NaN   \n",
       "std                     NaN                   NaN                       NaN   \n",
       "min                     NaN                   NaN                       NaN   \n",
       "25%                     NaN                   NaN                       NaN   \n",
       "50%                     NaN                   NaN                       NaN   \n",
       "75%                     NaN                   NaN                       NaN   \n",
       "max                     NaN                   NaN                       NaN   \n",
       "\n",
       "       LSTM dist. MEAN_FRAC_DIFF  \n",
       "count                        0.0  \n",
       "mean                         NaN  \n",
       "std                          NaN  \n",
       "min                          NaN  \n",
       "25%                          NaN  \n",
       "50%                          NaN  \n",
       "75%                          NaN  \n",
       "max                          NaN  \n",
       "\n",
       "[8 rows x 88 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dataframe with all the model results indexed by station\n",
    "all_results = []\n",
    "for m in main_result_vals.keys():\n",
    "    df = main_result_vals[m].copy()\n",
    "    df.rename(columns={'values': m}, inplace=True)\n",
    "    df.set_index('ids', inplace=True)\n",
    "    all_results.append(df)\n",
    "all_results_df = pd.concat(all_results, axis=1)\n",
    "all_results_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c2cea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the complete years previously processed\n",
    "\n",
    "with open('data/complete_years.json', 'r') as f:\n",
    "    complete_year_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21e971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load baseline distributions to get global support grid\n",
    "pmf_path = Path(os.getcwd()) / 'data' / 'results' / 'baseline_distributions' / f'pmf_obs.csv'\n",
    "pmf_obs_df = pd.read_csv(pmf_path)\n",
    "baseline_log_grid = pmf_obs_df['log_x'].values\n",
    "baseline_log_edges = np.concatenate([pmf_obs_df['left_log_edges'].values[:1], pmf_obs_df['right_log_edges'].values])\n",
    "baseline_log_w = np.diff(baseline_log_edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3b3b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_timeseries_discharge(stn, ds):\n",
    "    watershed_id = official_id_dict[stn]\n",
    "    df = ds['discharge'].sel(watershed=str(watershed_id)).to_dataframe(name='discharge').reset_index()\n",
    "    df = df.set_index('time')[['discharge']]\n",
    "    df.dropna(inplace=True)\n",
    "    # clip minimum flow to 1e-4\n",
    "    df['discharge'] = np.clip(df['discharge'], 1e-4, None)\n",
    "    df.rename(columns={'discharge': stn}, inplace=True)\n",
    "    df[f'{stn}_uar'] = 1000 * df[stn] / da_dict[stn]\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_ensemble_results(stn, folder):\n",
    "    \"\"\"Plot the ensemble results for a given station.\"\"\"\n",
    "    p = figure(title=f'Ensemble results for {stn}', x_axis_type='datetime', \n",
    "               y_axis_type='log', width=800, height=400)\n",
    "\n",
    "    for date, clr in zip(['20250514', '20250627'], ['black', 'red']):\n",
    "        fpath = folder / f'ensemble_results_{date}' / f'{stn}_ensemble.csv'\n",
    "        df = pd.read_csv(fpath)\n",
    "        df.rename(columns={'Unnamed: 0': 'time'}, inplace=True)\n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "        df.set_index('time', inplace=True)\n",
    "        df = np.exp(df)\n",
    "        if 'streamflow_obs' in df.columns:\n",
    "            p.line(df.index, df['streamflow_obs'], color=clr, legend_label=f'{date} Obs', line_width=2)\n",
    "        \n",
    "        sim_cols = [c for c in df.columns if c.startswith('streamflow_sim')]\n",
    "        mean_sim = df[sim_cols].mean(axis=1)\n",
    "        # compute the 5% quantiles on the simulation columns\n",
    "        lb = df[sim_cols].quantile(0.05, axis=1)\n",
    "        ub = df[sim_cols].quantile(0.95, axis=1)\n",
    "\n",
    "        p.varea(df.index, lb, ub, color=clr, alpha=0.2, legend_label=f'{date} 90% CI')\n",
    "        p.line(df.index, mean_sim, color=clr, legend_label=f'{date} Mean', line_dash='dashed', line_width=2)\n",
    "\n",
    "    p.legend.location = 'top_left'\n",
    "    p.xaxis.axis_label = 'Time'\n",
    "    p.yaxis.axis_label = 'Streamflow (L/s/km2)'\n",
    "    p.legend.click_policy= 'hide'\n",
    "    return p\n",
    "\n",
    "\n",
    "def filter_by_complete_years(stn, folder):\n",
    "    all_dfs = []\n",
    "    for date, clr in zip(['20250514', '20250627'], ['black', 'red']):\n",
    "        fpath = folder / f'ensemble_results_{date}' / f'{stn}_ensemble.csv'\n",
    "        if not os.path.exists(fpath):\n",
    "            return pd.DataFrame()\n",
    "        df = pd.read_csv(fpath)\n",
    "        df.rename(columns={'Unnamed: 0': 'time'}, inplace=True)\n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "        df.set_index('time', inplace=True)\n",
    "        df.columns = [f'{c}_{date}' for c in df.columns]\n",
    "        df = np.exp(df)\n",
    "        all_dfs.append(df)\n",
    "    result = pd.concat(all_dfs, axis=1, join='inner')\n",
    "    result = result.dropna(how='any', axis=0)\n",
    "    complete_years = complete_year_dict.get(stn, None).get('complete_years', [])\n",
    "    # print(f'    Found {len(complete_years)} complete years for {stn}: {complete_years}')\n",
    "    return result[result.index.year.isin(complete_years)]\n",
    "\n",
    "\n",
    "def get_original_timeseries(stn, ds):\n",
    "    \"\"\"Retrieve the original timeseries for a given station.\"\"\"\n",
    "    watershed_id = official_id_dict[stn]\n",
    "    df = ds['discharge'].sel(watershed=str(watershed_id)).to_dataframe(name='discharge').reset_index()\n",
    "    df = df.set_index('time')[['discharge']]\n",
    "    df.dropna(inplace=True)\n",
    "    # clip minimum flow to 1e-4\n",
    "    df['zero_flow_flag'] = df['discharge'] < 1e-4\n",
    "    df['discharge'] = np.clip(df['discharge'], 1e-4, None)\n",
    "    df.rename(columns={'discharge': stn}, inplace=True)\n",
    "    df[f'{stn}_uar'] = 1000 * df[stn] / da_dict[stn]\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_afdcs_from_sorted_flows(df, years, da, date):\n",
    "    \"\"\"Compute AFDCs from sorted daily flows, rather than PDFs.\"\"\"\n",
    "    \n",
    "    obs_cols = [c for c in df.columns if c.startswith('streamflow_obs') and c.endswith(date)]\n",
    "    assert len(obs_cols) == 1, f'Expected one observed column, found {len(obs_cols)}'\n",
    "    sim_cols = [c for c in df.columns if c.startswith('streamflow_sim') and c.endswith(date)]\n",
    "    assert len(sim_cols) == 10, f'Expected 10 simulated columns, found {len(sim_cols)}'\n",
    "\n",
    "    afdc_obs, afdc_sim = [], []\n",
    "\n",
    "    for year in years:\n",
    "        year_df = df[df.index.year == year]\n",
    "\n",
    "        # Observed\n",
    "        obs_values = year_df[obs_cols[0]].dropna().values\n",
    "        if len(obs_values) > 0:\n",
    "            sorted_obs = np.sort(obs_values)[::-1]  # descending\n",
    "            afdc_obs.append(pd.Series(sorted_obs, name=f\"{year}_obs\"))\n",
    "\n",
    "        # Simulated ensemble mean\n",
    "        sim_ensemble = year_df[sim_cols].dropna(how='all')  # drop rows with all NaNs\n",
    "        if not sim_ensemble.empty:\n",
    "            sim_mean = sim_ensemble.mean(axis=1).dropna().values\n",
    "            sorted_sim = np.sort(sim_mean)[::-1]\n",
    "            afdc_sim.append(pd.Series(sorted_sim, name=f\"{year}_sim\"))\n",
    "\n",
    "    # Align lengths: trim to shortest year\n",
    "    min_len = min(len(s) for s in afdc_obs + afdc_sim)\n",
    "    afdc_obs_trimmed = [s.iloc[:min_len].reset_index(drop=True) for s in afdc_obs]\n",
    "    afdc_sim_trimmed = [s.iloc[:min_len].reset_index(drop=True) for s in afdc_sim]\n",
    "\n",
    "    # Combine into DataFrames\n",
    "    obs_df = pd.concat(afdc_obs_trimmed, axis=1)\n",
    "    sim_df = pd.concat(afdc_sim_trimmed, axis=1)\n",
    "\n",
    "    # Compute percentile summary\n",
    "    afdc_summary = pd.DataFrame(index=np.arange(1, min_len + 1))\n",
    "    afdc_summary[f'AFDC50_obs_{date}'] = obs_df.median(axis=1)\n",
    "    afdc_summary[f'AFDC10_obs_{date}'] = obs_df.quantile(0.10, axis=1)\n",
    "    afdc_summary[f'AFDC90_obs_{date}'] = obs_df.quantile(0.90, axis=1)\n",
    "\n",
    "    afdc_summary[f'AFDC50_sim_{date}'] = sim_df.median(axis=1)\n",
    "    afdc_summary[f'AFDC10_sim_{date}'] = sim_df.quantile(0.10, axis=1)\n",
    "    afdc_summary[f'AFDC90_sim_{date}'] = sim_df.quantile(0.90, axis=1)\n",
    "\n",
    "    afdc_summary.index.name = 'Rank'\n",
    "    return afdc_summary\n",
    "\n",
    "\n",
    "def compute_ensemble_pmfs(df, sim_cols, kde, da):\n",
    "    \"\"\"Compute the frequency mean PMF for the simulated ensemble.\"\"\"\n",
    "    sim_ensemble_pmfs = []\n",
    "    for sim_col in sim_cols:\n",
    "        sim_vals = df[sim_col].dropna().values\n",
    "        assert len(sim_vals) > 0, f'No valid values found for {sim_col}'\n",
    "        sim_pmf, _ = kde.compute(sim_vals, da=da)\n",
    "        sim_ensemble_pmfs.append(pd.Series(sim_pmf, index=baseline_log_grid, name=sim_col))\n",
    "    # concatenate all PMFs and compute the mean\n",
    "    return pd.concat(sim_ensemble_pmfs, axis=1)\n",
    "\n",
    "\n",
    "def compute_series_range(pmf, grid, threshold=1e-4):\n",
    "    # Find indices where pmf is above the threshold\n",
    "    valid_idx = np.where(pmf >= threshold)[0]\n",
    "    if len(valid_idx) > 0:\n",
    "        left_bound = grid[valid_idx[0]]\n",
    "        right_bound = grid[valid_idx[-1]]\n",
    "    else:\n",
    "        # Fallback if all values are below threshold\n",
    "        left_bound = grid[0]\n",
    "        right_bound = grid[-1]\n",
    "    return left_bound, right_bound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2173e371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_observed_and_simulated_pdf(stn, pmf_dfs, og_df, date, pdf_plots=[]):\n",
    "    \"\"\"Plot the observed and simulated PDFs for a given station.\"\"\"\n",
    "\n",
    "    baseline_lin_grid = np.exp(baseline_log_grid)\n",
    "    lbs, rbs = [], []\n",
    "    if date == '20250514':\n",
    "        title = f'{stn}: LSTM PDFs Mean NSE Objective'\n",
    "    elif date == '20250627':\n",
    "        title = f'{stn}: LSTM PDFs 95% Quantile Objective'\n",
    "    if len(pdf_plots) > 0:\n",
    "        p = figure(title=title, x_axis_type='log', toolbar_location='above',\n",
    "            width=1000, height=350, x_range=pdf_plots[0].x_range,\n",
    "            y_range=pdf_plots[0].y_range)\n",
    "    else:\n",
    "        p = figure(title=title, x_axis_type='log',toolbar_location='above',\n",
    "            width=1000, height=350)\n",
    "\n",
    "    # plot the observed values as quad glyphs\n",
    "    observed_vals = og_df[f'{stn}_uar'].dropna().values\n",
    "    observed_log_vals = np.log(observed_vals)\n",
    "    min_q, max_q = observed_log_vals.min(), observed_log_vals.max()\n",
    "    obs_log_w = np.linspace(min_q - 0.1, max_q + 0.1, num=128)\n",
    "    hist, edges = np.histogram(observed_log_vals, bins=obs_log_w, density=True)\n",
    "    edges = np.exp(edges)  # convert edges back to linear space\n",
    "    # convert to probbility mass function (PMF)\n",
    "    p.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:],\n",
    "            fill_color='dodgerblue', alpha=0.5, legend_label='Observed')\n",
    "    \n",
    "    df = pmf_dfs[date].copy()\n",
    "    for i in range(10):\n",
    "        sim_col = f'streamflow_sim_{i}_{date}'\n",
    "        if sim_col in df.columns:\n",
    "            vals = df[sim_col].values / log_w\n",
    "            lb, rb = compute_series_range(vals, baseline_lin_grid, threshold=1e-4)\n",
    "            lbs.append(lb)\n",
    "            rbs.append(rb)\n",
    "            p.line(baseline_lin_grid, vals, color='grey', alpha=0.5, \n",
    "                    legend_label=f'LSTM Ensemble')\n",
    "\n",
    "    # convert pmfs to pdfs\n",
    "    obs_vals = df[f'POR_obs_{date}'].values / log_w\n",
    "    lb, rb = compute_series_range(obs_vals, baseline_lin_grid, threshold=1e-4)\n",
    "    lbs.append(lb)\n",
    "    rbs.append(rb)\n",
    "    p.line(baseline_lin_grid, obs_vals, \n",
    "           color='black', line_width=2.5, legend_label=f'POR Observed', line_dash='dotted')\n",
    "    sim_time_vals = df[f'POR_sim_timeEnsemble_{date}'].values / log_w\n",
    "    lb, rb = compute_series_range(sim_time_vals, baseline_lin_grid, threshold=1e-4)\n",
    "    lbs.append(lb)\n",
    "    rbs.append(rb)\n",
    "    p.line(baseline_lin_grid, sim_time_vals, \n",
    "           line_width=2.5, color='green', legend_label=f'timeEnsemble', line_dash='dashed')\n",
    "    sim_dist_vals = df[f'POR_sim_distEnsemble_{date}'].values / log_w\n",
    "    lb, rb = compute_series_range(sim_dist_vals, baseline_lin_grid, threshold=1e-4)\n",
    "    lbs.append(lb)\n",
    "    rbs.append(rb)\n",
    "    p.line(baseline_lin_grid, sim_dist_vals, \n",
    "           line_width=2.5, color='green', legend_label=f'distEnsemble')\n",
    "    \n",
    "    clrs = ['orange', 'purple']\n",
    "    lss = ['solid', 'dashed', 'dotted']\n",
    "    cols = list(pmf_dfs.keys())\n",
    "    \n",
    "    for i, lb in enumerate(['LN', 'KNN']):\n",
    "        model_cols = [c for c in cols if c.startswith(lb)]\n",
    "        for j, mc in enumerate(model_cols):\n",
    "            mc_pdf = pmf_dfs[mc] / log_w\n",
    "            lb, rb = compute_series_range(mc_pdf, baseline_lin_grid, threshold=1e-4)\n",
    "            lbs.append(lb)\n",
    "            rbs.append(rb)\n",
    "            p.line(baseline_lin_grid, mc_pdf, line_color=clrs[i], line_dash=lss[j]\n",
    "                    , legend_label=f'{mc}', line_width=2.5)\n",
    "    \n",
    "    p.x_range.start = np.min(lbs)\n",
    "    p.x_range.end = np.max(rbs)\n",
    "\n",
    "    p.xaxis.axis_label = 'Log Unit Area Runoff (L/s/km2)'\n",
    "    p.xaxis.axis_label = 'Unit Area Runoff (L/s/km2)'\n",
    "    p.yaxis.axis_label = 'Probability Density'\n",
    "    p.legend.location = 'top_left'\n",
    "    p.legend.click_policy = 'hide'\n",
    "    p.add_layout(p.legend[0], 'left')\n",
    "    return p, baseline_log_grid, baseline_lin_grid, log_w\n",
    "\n",
    "\n",
    "def plot_observed_and_simulated_fdc(stn, pmf_dfs, baseline_lin_grid, lstm_df, og_df, date, fdc_plots=[]):\n",
    "    \"\"\"Plot the observed and simulated FDCs for a given station.\"\"\"\n",
    "\n",
    "    # baseline_lin_grid = np.exp(baseline_log_grid)\n",
    "    if date == '20250514':\n",
    "        title = f'{stn}: LSTM FDCs Mean NSE Objective'\n",
    "    elif date == '20250627':\n",
    "        title = f'{stn}: LSTM FDCs 95% Quantile Objective'\n",
    "\n",
    "    if len(fdc_plots) > 0:\n",
    "        fdc_plot = figure(title=title, y_axis_type='log',\n",
    "            width=600, height=350, x_range=fdc_plots[0].x_range,\n",
    "            y_range=fdc_plots[0].y_range, toolbar_location='above')\n",
    "    else:\n",
    "        fdc_plot = figure(title=title, toolbar_location='above', \n",
    "                          width=600, height=350, y_axis_type='log')\n",
    "        \n",
    "    # plot the observed duration curve\n",
    "    pcts = np.linspace(0.01, 0.99, 99)\n",
    "    observed_vals = og_df[f'{stn}_uar'].dropna().values\n",
    "    obs_fdc = np.percentile(observed_vals, pcts * 100)[::-1]\n",
    "    fdc_plot.line(pcts, obs_fdc, color='dodgerblue', line_width=2.5, legend_label=f'Observed')\n",
    "\n",
    "    sim_cols = [c for c in lstm_df.columns if c.startswith('streamflow_sim') and c.endswith(date)]\n",
    "    assert len(sim_cols) == 10, f'Expected 10 simulated columns, found {len(sim_cols)}'\n",
    "\n",
    "    sim_fdcs = pd.DataFrame(index=pcts)\n",
    "    for i, sim_col in enumerate(sim_cols):\n",
    "        sim_vals = lstm_df[sim_col].dropna().values\n",
    "        sim_fdc = np.percentile(sim_vals, pcts * 100)[::-1]\n",
    "        sim_fdcs[f'LSTM Simulation {i+1}'] = sim_fdc\n",
    "        fdc_plot.line(pcts, sim_fdc, color='grey', alpha=0.5, \n",
    "                      legend_label=f'LSTM Simulation')\n",
    "\n",
    "    # compute the temporal ensemble mean FDC\n",
    "    temporal_mean_fdc = lstm_df[sim_cols].mean(axis=1).dropna().values\n",
    "    time_ensemble_fdc = np.percentile(temporal_mean_fdc, pcts * 100)[::-1]\n",
    "    fdc_plot.line(pcts, time_ensemble_fdc, color='green', \n",
    "                  line_width=2.5, line_dash='dashed',\n",
    "                  legend_label=f'LSTM Time')\n",
    "    \n",
    "    # compute the frequency ensemble mean FDC\n",
    "    freq_ensemble_fdc = sim_fdcs.mean(axis=1).values\n",
    "    fdc_plot.line(pcts, freq_ensemble_fdc, color='green', line_width=2.5, \n",
    "                  legend_label=f'LSTM Dist.')\n",
    "    \n",
    "    clrs = ['orange', 'purple']\n",
    "    lss = ['solid', 'dashed', 'dotted']\n",
    "    cols = list(pmf_dfs.keys())\n",
    "    fdc_metrics = {}\n",
    "    fdc_metrics['distEnsemble'] = float((freq_ensemble_fdc - obs_fdc).sum())\n",
    "    fdc_metrics['timeEnsemble'] = float((time_ensemble_fdc - obs_fdc).sum())\n",
    "    for i, lb in enumerate(['LN', 'KNN']):\n",
    "        model_cols = [c for c in cols if c.startswith(lb)]\n",
    "        for j, mc in enumerate(model_cols):\n",
    "            mc_pmf = pmf_dfs[mc]\n",
    "            # compute the cdf\n",
    "            mc_cdf = np.cumsum(mc_pmf) \n",
    "            # interpolate between the percentiles to get the 1, 99 percentile values\n",
    "            model_vals = np.interp(pcts, mc_cdf, baseline_lin_grid)\n",
    "            fdc_metrics[f'{mc}'] = float((model_vals - obs_fdc).sum())\n",
    "            fdc_plot.line(pcts, model_vals[::-1], line_color=clrs[i], line_dash=lss[j],\n",
    "                    legend_label=f'{mc}', line_width=2.5)\n",
    "    \n",
    "    fdc_plot.xaxis.axis_label = 'Exceedance Probability (%)'\n",
    "    fdc_plot.legend.background_fill_alpha = 0.5\n",
    "    fdc_plot.add_layout(fdc_plot.legend[0], 'left')\n",
    "    fdc_plot.yaxis.axis_label = 'Unit Area Runoff (L/s/km2)'\n",
    "    fdc_plot.legend.location = 'top_right'\n",
    "    fdc_plot.legend.click_policy = 'hide'\n",
    "    \n",
    "    return fdc_plot, fdc_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39962a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_station_in_hydat(stn, conn):\n",
    "    # Query to check if the station exists\n",
    "    check_query = \"\"\"\n",
    "    SELECT STATION_NUMBER, STATION_NAME\n",
    "    FROM STATIONS\n",
    "    WHERE STATION_NUMBER = ?\n",
    "    \"\"\"\n",
    "    # Run the query\n",
    "    station_check = pd.read_sql_query(check_query, conn, params=(stn,))\n",
    "\n",
    "    # Test if any result was returned\n",
    "    if station_check.empty:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "    \n",
    "def query_data_symbols(stn):\n",
    "    hydat_path = Path('/home/danbot/code/common_data/HYDAT') / 'Hydat_20250715.sqlite3'\n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(hydat_path)\n",
    "    # Query all data symbols\n",
    "    query = \"SELECT SYMBOL_ID, SYMBOL_EN FROM DATA_SYMBOLS ORDER BY SYMBOL_ID\"\n",
    "    try:\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        return df.set_index('SYMBOL_ID')['SYMBOL_EN']\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def reshape_hydat_wide(df):\n",
    "    # First, ensure all FLOW_SYMBOL columns exist and are named correctly\n",
    "    id_vars = [\"STATION_NUMBER\", \"YEAR\", \"MONTH\", \"NO_DAYS\"]\n",
    "    \n",
    "    # Melt flows\n",
    "    flow_df = df.melt(id_vars=id_vars, \n",
    "                      value_vars=[f\"FLOW{i}\" for i in range(1, 32)],\n",
    "                      var_name=\"day\", value_name=\"flow\")\n",
    "\n",
    "    # Melt symbols\n",
    "    sym_df = df.melt(id_vars=id_vars, \n",
    "                     value_vars=[f\"FLOW_SYMBOL{i}\" for i in range(1, 32)],\n",
    "                     var_name=\"day\", value_name=\"flow_symbol\")\n",
    "\n",
    "    # Extract day number\n",
    "    flow_df[\"day\"] = flow_df[\"day\"].str.extract(r\"(\\d+)$\").astype(int)\n",
    "    sym_df[\"day\"] = sym_df[\"day\"].str.extract(r\"(\\d+)$\").astype(int)\n",
    "\n",
    "    # Merge on ID columns + day\n",
    "    merged = pd.merge(flow_df, sym_df, on=id_vars + [\"day\"])\n",
    "\n",
    "    # Construct date\n",
    "    merged[\"date\"] = pd.to_datetime(dict(year=merged[\"YEAR\"], \n",
    "                                         month=merged[\"MONTH\"], \n",
    "                                         day=merged[\"day\"]), errors='coerce')\n",
    "\n",
    "    # Filter out invalid days (e.g., day > NO_DAYS)\n",
    "    merged = merged[merged[\"day\"] <= merged[\"NO_DAYS\"]]\n",
    "    formatted_df = merged[[\"STATION_NUMBER\", \"date\", \"flow\", \"flow_symbol\"]].dropna(subset=[\"flow\"])\n",
    "    formatted_df.set_index('date', inplace=True)\n",
    "    return formatted_df\n",
    "\n",
    "\n",
    "def query_hydat_database(stn):\n",
    "    \"\"\"Query the HYDAT database for a given station and date range.\"\"\"\n",
    "    hydat_path = Path('/home/danbot/code/common_data/HYDAT') / 'Hydat_20250715.sqlite3'\n",
    "    # Connect to the database\n",
    "    conn = sqlite3.connect(hydat_path)\n",
    "\n",
    "    station_in_hydat = check_if_station_in_hydat(stn, conn)\n",
    "    if station_in_hydat is False:\n",
    "        print(f'Station {stn} not found in HYDAT database.')\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    base_columns = [\"STATION_NUMBER\", \"YEAR\", \"MONTH\"]\n",
    "    flow_columns = [f\"FLOW{i}, FLOW_SYMBOL{i}\" for i in range(1, 32)]\n",
    "    end_columns = [\"NO_DAYS\"]\n",
    "\n",
    "    all_columns = base_columns + flow_columns + end_columns\n",
    "    column_str = \",\\n    \".join(all_columns)\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        {column_str}\n",
    "    FROM DLY_FLOWS\n",
    "    WHERE STATION_NUMBER = ?\n",
    "    ORDER BY YEAR, MONTH;\n",
    "    \"\"\"\n",
    "    df = pd.read_sql_query(query, conn, params=(stn,))\n",
    "\n",
    "    if df.empty:\n",
    "        print(f'No data found for {stn} in HYDAT.')\n",
    "        return pd.DataFrame()\n",
    "    df = reshape_hydat_wide(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def find_symbol_segments(symbol_df, target_symbol):\n",
    "    \"\"\"Return (start, end) date pairs for each continuous period of target_symbol.\"\"\"\n",
    "    # Filter for matching symbol only\n",
    "    mask = (symbol_df['flow_symbol'] == target_symbol)\n",
    "    dates = symbol_df['flow_symbol'].index[mask]\n",
    "\n",
    "    if dates.empty:\n",
    "        return []\n",
    "\n",
    "    # Compute gaps in days between successive dates\n",
    "    gaps = dates.to_series().diff().gt(pd.Timedelta(days=1)).fillna(True)\n",
    "\n",
    "    # Group by contiguous regions (cumsum creates a new group after each gap)\n",
    "    group_ids = gaps.cumsum()\n",
    "\n",
    "    # Group by group ID and extract start and end of each contiguous block\n",
    "    segments = [(group.min(), group.max()) for _, group in dates.to_series().groupby(group_ids)]\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a29d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_quality_flag_periods(stn, df, hydat_df, quality_symbols, runoff_plot, obs_col):\n",
    "    symbol_dict = quality_symbols.to_dict()\n",
    "    symbol_colors = {\n",
    "        'B': 'dodgerblue',  # Baseflow\n",
    "        'D': 'firebrick',  # Dry weather flow\n",
    "        'E': 'orange',  # Estimated\n",
    "    }\n",
    "    df['flow_symbol'] = hydat_df['flow_symbol'].reindex(df.index, method=None)\n",
    "    uar_cols = obs_col + [c for c in df.columns if c.startswith('streamflow_sim')]\n",
    "\n",
    "    for symbol in ['B', 'D', 'E']:\n",
    "        description = symbol_dict.get(symbol, {})\n",
    "        color = symbol_colors.get(symbol, 'gray')\n",
    "        n_symbols = df['flow_symbol'].eq(symbol).sum()\n",
    "        if n_symbols == 0:\n",
    "            continue\n",
    "\n",
    "        segments = find_symbol_segments(df[['flow_symbol']].copy(), symbol)\n",
    "\n",
    "        for start, end in segments:\n",
    "            runoff_plot.varea(\n",
    "                x=pd.date_range(start, end),\n",
    "                y1=0.98 * df[uar_cols].min().min(),  # get the min of the dataframe for the lower bound\n",
    "                y2=1.02 * df[uar_cols].max().max(),  # a bit above max for visibility\n",
    "                fill_color=color, fill_alpha=0.3,\n",
    "                legend_label=f\"{description} ({symbol})\"\n",
    "            )\n",
    "    \n",
    "    df['flow'] = hydat_df['flow'].reindex(df.index, method=None)\n",
    "    df['hydat_uar'] = 1000 * df['flow'] / da_dict[stn]  # convert to unit area runoff (L/s/km2)\n",
    "    runoff_plot.line(df.index, df['hydat_uar'],\n",
    "                     color='dodgerblue', legend_label='HYDAT UAR', \n",
    "                     line_width=2, line_dash='dotted')\n",
    "    return runoff_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50012f63-aad3-40a2-82a5-ab913bb07b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_zero_flow_periods(stn, df, runoff_plot, obs_col):\n",
    "\n",
    "    color = 'salmon'\n",
    "    n_symbols = (df['zero_flow_flag'] == True).sum()\n",
    "    if n_symbols == 0:\n",
    "        return runoff_plot\n",
    "\n",
    "    df.rename({'zero_flow_flag': 'flow_symbol'}, inplace=True, axis=1)\n",
    "\n",
    "    segments = find_symbol_segments(df[['flow_symbol']].copy(), True)\n",
    "    for start, end in segments:\n",
    "        runoff_plot.varea(\n",
    "            x=pd.date_range(start, end),\n",
    "            y1=0.98 * df[f'{stn}_uar'].min(),  # get the min of the dataframe for the lower bound\n",
    "            y2=1.02 * df[f'{stn}_uar'].max(),  # a bit above max for visibility\n",
    "            fill_color=color, fill_alpha=0.3,\n",
    "            legend_label=f\"Q=0 replaced\"\n",
    "        )\n",
    "    runoff_plot.line(df.index, df[f'{stn}_uar'],\n",
    "                     color='purple', legend_label='UAR', \n",
    "                     line_width=2, line_dash='dotted')\n",
    "    return runoff_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073de4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_FDCs(df, stn, og_df, result_folder):\n",
    "    pmf_dfs, por_metrics, other_metrics = {}, {}, {}\n",
    "    # load the parametric result\n",
    "    parametric_fpath = result_folder / 'parametric' / f'{stn}_fdc_results.json'\n",
    "    # if not os.path.exists(parametric_fpath):\n",
    "    #     raise xception(f'Parametric results for {stn} not found at {parametric_fpath}')\n",
    "    with open(parametric_fpath, 'r') as f:\n",
    "        parametric_results = json.load(f)\n",
    "    other_metrics[f'LN_MLE'] = parametric_results['MLE']['eval']\n",
    "    other_metrics[f'LN_PredictedLog'] = parametric_results['PredictedLog']['eval']\n",
    "    other_metrics[f'LN_PredictedMOM'] = parametric_results['PredictedMOM']['eval']\n",
    "    pmf_dfs['LN_MLE'] = parametric_results['MLE']['pmf']\n",
    "    pmf_dfs['LN_PredictedLog'] = parametric_results['PredictedLog']['pmf']\n",
    "    pmf_dfs['LN_PredictedMOM'] = parametric_results['PredictedMOM']['pmf']\n",
    "\n",
    "    knn_fpath = result_folder / 'knn' / f'{stn}_fdc_results.json'\n",
    "    with open(knn_fpath, 'r') as f:\n",
    "        knn_results = json.load(f)\n",
    "    knn_cols = list(knn_results.keys())\n",
    "    nn2 = [c for c in knn_cols if '2_NN_attribute_dist_ID2_freqEnsemble' in c]\n",
    "    nn4 = [c for c in knn_cols if '4_NN_attribute_dist_ID2_freqEnsemble' in c]\n",
    "    nn8 = [c for c in knn_cols if '8_NN_attribute_dist_ID2_freqEnsemble' in c]\n",
    "\n",
    "    nn2_pmf = knn_results[nn2[0]]['pmf']\n",
    "    nn4_pmf = knn_results[nn4[0]]['pmf']\n",
    "    nn8_pmf = knn_results[nn8[0]]['pmf']\n",
    "    pmf_dfs['KNN_2_NN'] = nn2_pmf\n",
    "    pmf_dfs['KNN_4_NN'] = nn4_pmf\n",
    "    pmf_dfs['KNN_8_NN'] = nn8_pmf\n",
    "    # for m in ['nse', 'kld', 'rmse', 'relative_error', 'emd', 'kge']:\n",
    "    other_metrics[f'KNN_2_NN'] = knn_results[nn2[0]]['eval']\n",
    "    other_metrics[f'KNN_4_NN'] = knn_results[nn4[0]]['eval']\n",
    "    other_metrics[f'KNN_8_NN'] = knn_results[nn8[0]]['eval']\n",
    "\n",
    "    kde = KDEEstimator(baseline_log_edges)\n",
    "    eval_object = EvaluationMetrics(log_x=baseline_log_grid, log_w=log_w)\n",
    "    print(f'    Processing FDCs for {stn}')\n",
    "    \n",
    "    years = []\n",
    "    for date in ['20250514', '20250627']:\n",
    "        pmf_columns = []\n",
    "        years = df.index.year.unique()\n",
    "        da = da_dict[stn]\n",
    "\n",
    "        pmf_fpath = f'data/results/lstm_pmfs/POR_{stn}_pmfs_{len(years)}_years_{date}.csv'\n",
    "        # if os.path.exists(pmf_fpath):\n",
    "        #     print(f'    PMFs for {stn} already exist, skipping.')\n",
    "        #     continue\n",
    "\n",
    "        # compute observed POR PMFs \n",
    "        obs_cols = [c for c in df.columns if c.startswith('streamflow_obs') and c.endswith(date)]\n",
    "        assert len(obs_cols) == 1, f'Expected exactly one observed column for {stn} on {date}, found {len(obs_cols)}'\n",
    "        por_obs_vals = df[obs_cols[0]].dropna().values\n",
    "        # print(f'{stn} POR obs: {min(por_obs_vals):.2f} - {max(por_obs_vals):.2f}')\n",
    "        por_obs_pmf, por_obs_pdf = kde.compute(por_obs_vals, da=da)\n",
    "        assert len(por_obs_pmf) == len(baseline_log_grid), f'PMF length mismatch for {stn} on {date} ({len(por_obs_pmf)} vs {len(baseline_log_grid)})'\n",
    "        pmf_columns.append(pd.Series(por_obs_pmf, index=baseline_log_grid, name=f'POR_obs_{date}'))\n",
    "\n",
    "        # compute simulated POR PMFs\n",
    "        sim_cols = [c for c in df.columns if c.startswith('streamflow_sim') and c.endswith(date)]\n",
    "        # temporal mean of the simulated ensemble\n",
    "        sim_vals = df[sim_cols].mean(axis=1).dropna().values\n",
    "        sim_pmf, sim_pdf = kde.compute(sim_vals, da=da) # temporal mean ensemble PMF\n",
    "        assert len(sim_pmf) == len(baseline_log_grid), f'PMF length mismatch for {stn} on {date}'\n",
    "        pmf_columns.append(pd.Series(sim_pmf, index=baseline_log_grid, name=f'POR_sim_timeEnsemble_{date}'))\n",
    "\n",
    "        frequency_sim_pmfs = compute_ensemble_pmfs(df, sim_cols, kde, da)\n",
    "        mean_pmf = frequency_sim_pmfs.mean(axis=1).rename('sim_distEnsemble_mean')\n",
    "        mean_pmf /= mean_pmf.sum()  # renormalize the PMF\n",
    "        pmf_columns.append(pd.Series(mean_pmf, index=baseline_log_grid, name=f'POR_sim_distEnsemble_{date}'))\n",
    "\n",
    "        og_vals = og_df[f'{stn}_uar'].dropna().values\n",
    "        # print(f'{stn}_uar: {min(og_vals):.2f} - {max(og_vals):.2f}')\n",
    "        og_pmf, og_pdf = kde.compute(og_vals, da=da)\n",
    "        pmf_columns.append(pd.Series(og_pmf, index=baseline_log_grid, name=f'{stn}_uar'))\n",
    "        por_pmfs = pd.concat(pmf_columns, axis=1)    # do the same for the original timeseries\n",
    "        pmfs = pd.concat([por_pmfs, frequency_sim_pmfs], axis=1)  # initialize PMFs DataFrame with observed PMFs\n",
    "        pmfs.index.name = 'log_uar'  # set the index name for clarity\n",
    "        # save the PMFs to a CSV file\n",
    "        pmfs.to_csv(pmf_fpath)\n",
    "        pmf_dfs[date] = pmfs\n",
    "\n",
    "        # evaluate metrics\n",
    "        for col in [f'POR_sim_timeEnsemble_{date}', f'POR_sim_distEnsemble_{date}']:\n",
    "            pmf = pmfs[col].values\n",
    "            por_metrics[col] = eval_object._evaluate_fdc_metrics_from_pmf(pmf, por_obs_pmf)\n",
    "\n",
    "        for col in frequency_sim_pmfs.columns:\n",
    "            pmf = frequency_sim_pmfs[col].values\n",
    "            por_metrics[col] = eval_object._evaluate_fdc_metrics_from_pmf(pmf, por_obs_pmf)\n",
    "\n",
    "    mdf = pd.DataFrame(por_metrics).T\n",
    "    odf = pd.DataFrame(other_metrics).T\n",
    "    # save to csv\n",
    "    metric_fpath = f'data/results/lstm_metrics/{stn}_{len(years)}_years_metrics.csv'\n",
    "    mdf.to_csv(metric_fpath)\n",
    "    return mdf, odf, pmf_dfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe813c6-1700-4488-a4a9-5c7aa2089997",
   "metadata": {},
   "source": [
    "### Streamflow monitoring stations that we found to be regulated or influenced by regulation by looking at results plots\n",
    "\n",
    "A number of stations in the large sample were found to be regulated or influenced by regulation by looking at results plots.  These are listed below with notes.  These stations should be excluded from the sample for future analysis unless the particular application calls for time series of regulated streams.\n",
    "\n",
    "* 12398000 - Sullivan Lake upstream is a reservoir\n",
    "* 12058800 - Dam!\n",
    "\n",
    "### Other anomalous conditions\n",
    "\n",
    "* 12143700 -  *Small catchment adjacent to dam* (USGS.gov) No regulation or diversion upstream from station. **Flow is mostly seepage from Chester Morse Lake.** U.S. Geological Survey satellite telemeter at station.  This represnts a very specific scenario that depends upon hydraulic conditions in neighbouring catchments but also the operation policy if the neighbouring catchment contains a large reservoir. \n",
    "* 12143900 -  *Just downstream from 12143700* Heavily affected by the same condition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132c6a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_label_styles = {\n",
    "    \"timeEnsemble\":       (\"#2ca25f\", \"white\"),  # green\n",
    "    \"distEnsemble\":       (\"#2ca25f\", \"white\"),\n",
    "    \"LN MLE\":             (\"#fdae6b\", \"black\"),  # orange\n",
    "    \"LN_PredictedLog\":    (\"#fdae6b\", \"black\"),\n",
    "    \"LN_PredictedMOM\":    (\"#fdae6b\", \"black\"),\n",
    "    \"KNN_2_NN\":           (\"#756bb1\", \"white\"),  # purple\n",
    "    \"KNN_4_NN\":           (\"#756bb1\", \"white\"),  # purple\n",
    "    \"KNN_8_NN\":           (\"#756bb1\", \"white\"),\n",
    "}\n",
    "\n",
    "\n",
    "def key_mapper(mod,met):\n",
    "    \n",
    "    if met == 'pct_vol_bias':\n",
    "        met = 'RB'\n",
    "    elif met == 'mean_error':\n",
    "        met = 'MB'\n",
    "    elif met == 'pmf':\n",
    "        met = 'VB_PMF'\n",
    "    elif met == 'fdc':\n",
    "        met = 'VB_FDC'\n",
    "    elif met == 'diff':\n",
    "        met = 'mean_frac_diff'\n",
    "\n",
    "    met = met.upper()\n",
    "    if mod.startswith('time'):\n",
    "        key = f'LSTM time {met}'\n",
    "    elif mod.startswith('dist'):\n",
    "        key = f'LSTM dist. {met}'\n",
    "    elif 'MOM' in mod:\n",
    "        key = f'LN MoM {met}'\n",
    "    elif 'PredictedLog' in mod:\n",
    "        key = f'LN Direct {met}'\n",
    "    elif 'MLE' in mod:\n",
    "        key = f'MLE {met}'\n",
    "    elif mod.endswith('_NN'):\n",
    "        k = mod.split('_')[1]\n",
    "        key = f'{k} kNN {met}'\n",
    "    else:\n",
    "        raise Exception(f'{mod} {met} combo not found')\n",
    "    return key\n",
    "\n",
    "\n",
    "def plot_runoff_timeseries(stn, lstm_df, og_df, date):\n",
    "\n",
    "    obs_col = [c for c in lstm_df.columns if '_obs_' in c and c.endswith(date)]\n",
    "    assert len(obs_col) == 1, f'Expected exactly one observed column for {stn}, found {len(obs_col)} {obs_col}'\n",
    "    sim_cols = [c for c in lstm_df.columns if c.startswith('streamflow_sim') and c.endswith(date)]\n",
    "    assert len(sim_cols) == 10, f'Expected ten simulation columns for {stn}, found {len(sim_cols)}'\n",
    "    # plot th time series of the observed values\n",
    "    runoff_plot = figure(title=f'{stn} Observed Unit Area Runoff', x_axis_type='datetime',\n",
    "                         width=1000, height=300, y_axis_type='log', toolbar_location='above')\n",
    "    \n",
    "    hydat_df = query_hydat_database(stn)\n",
    "    quality_symbols = query_data_symbols(stn)\n",
    "\n",
    "    # reindex to daily frequency and keep nans\n",
    "    df = lstm_df.copy().reindex(pd.date_range(start=lstm_df.index.min(), end=lstm_df.index.max(), freq='D'))\n",
    "\n",
    "    if (not hydat_df.empty) & (not quality_symbols.empty):\n",
    "        runoff_plot = plot_quality_flag_periods(stn, df, hydat_df, quality_symbols, runoff_plot, obs_col)\n",
    "\n",
    "    # plot zero flow segments on the runoff_plot\n",
    "    runoff_plot = plot_zero_flow_periods(stn, og_df, runoff_plot, obs_col)\n",
    "    # plot the adjusted line\n",
    "    runoff_plot.line(df.index, df[obs_col[0]], color='dodgerblue',\n",
    "                     legend_label='Observed UAR', line_width=2.)\n",
    "    for col in sim_cols:\n",
    "        runoff_plot.line(df.index, df[col], color='grey', alpha=0.5,\n",
    "                         legend_label=f'LSTM ensemble')\n",
    "    # compute the temporal mean of the simulated ensemble\n",
    "    mean_sim = df[sim_cols].mean(axis=1)\n",
    "    runoff_plot.line(df.index, mean_sim, color='green', legend_label='Ensemble Mean', line_width=3, line_dash='dashed')\n",
    "    runoff_plot.xaxis.axis_label = 'Date'\n",
    "    runoff_plot.yaxis.axis_label = 'Unit Area Runoff (L/s/km2)'\n",
    "    runoff_plot.legend.location = 'top_left'\n",
    "    runoff_plot.legend.click_policy = 'hide'\n",
    "    runoff_plot.add_layout(runoff_plot.legend[0], 'right')\n",
    "    runoff_plot.legend.background_fill_alpha = 0.65\n",
    "    return runoff_plot\n",
    "    \n",
    "\n",
    "def format_metrics_table(metric_df, odf, stn, date, fdc_metrics):\n",
    "    # Prep LSTM metrics\n",
    "    metric_df.index.name = 'series'\n",
    "    metric_df = metric_df.reset_index()\n",
    "    \n",
    "    lstm_df = metric_df[metric_df['series'].str.endswith(date)].copy()\n",
    "    metric_cols = [c for c in lstm_df.columns if c not in ['series', 'date']]    \n",
    "    time_df = lstm_df[lstm_df['series'].str.contains('timeEnsemble')][metric_cols]\n",
    "    freq_df = lstm_df[lstm_df['series'].str.contains('distEnsemble')][metric_cols]\n",
    "    # sim_df = lstm_df[lstm_df['series'].str.startswith('streamflow_sim_')][metric_cols]\n",
    "    # sim_perc = pd.DataFrame(np.percentile(sim_df.values, [5, 95], axis=0), index=['5%', '95%'], columns=metric_cols)\n",
    "    \n",
    "    table = pd.concat([time_df, freq_df])#, sim_perc])\n",
    "    table.index = ['timeEnsemble', 'distEnsemble']#, '5%', '95%']\n",
    "    \n",
    "    # Prep ODF\n",
    "    mapper = {'mean_frac_diff': 'diff', 'vb_pmf': 'pmf', 'vb_fdc': 'fdc',\n",
    "             'pct_vol_bias': 'rb', 'mean_error': 'mb'}\n",
    "    odf = odf.rename(columns=mapper)\n",
    "    table = table.rename(columns=mapper)\n",
    "\n",
    "    max_cols = ['nse', 'kge', 've']\n",
    "    min_abs_cols = ['pmf', 'fdc', 'diff', 'vb_pmf', 'vb_fdc', 'error', 'pct_vol_bias', 'mean_error', \n",
    "                   'mean_abs_rel_error', 'rb', 'mb']\n",
    "\n",
    "    # Combine and compute percentiles\n",
    "    df = pd.concat([table, odf], axis=0)\n",
    "    # df.drop(labels=['median_error'], axis=1, inplace=True)\n",
    "        \n",
    "    colors = [\n",
    "        \"#2166ac\", \"#4393c3\", \"#92c5de\", \"#d1e5f0\", \"#f7f7f7\",\n",
    "        \"#fddbc7\", \"#f4a582\", \"#d6604d\", \"#b2182b\", \"#67001f\"\n",
    "    ]\n",
    "    # Build HTML table with row labels\n",
    "    html = '<table style=\"width:100%; border-collapse:collapse; margin-top:25px;\">'\n",
    "    html += '<thead><tr>'\n",
    "    table_cols = {'pmf': 'pmfBias', 'fdc': 'fdcBias', 'diff': 'fdcpErr', 'rb': 'RB', 'mb': 'MB'}\n",
    "    col_order = ['rb', 'mb', 'rmse', 'kld', 'emd', 'nse', 'kge']#, 'pmf', 'fdc']\n",
    "    # 'rb', 'mb', 'mean_abs_rel_error', 'rmse', 'nse', 'kge', 've', 'pb_50',\n",
    "    #    'kld', 'emd']\n",
    "    df = df[col_order]\n",
    "    for c in df.columns:\n",
    "        label = c\n",
    "        if c in table_cols:\n",
    "            label = table_cols[c]\n",
    "        html += f'<th>{label}</th>'\n",
    "\n",
    "    html += '<th style=\"text-align:left;\">Model</th></tr></thead><tbody>'      \n",
    "\n",
    "    for model, row in df.iterrows():\n",
    "        html += f'<tr>'\n",
    "        for metric in df.columns:\n",
    "            key = key_mapper(model, metric)\n",
    "            val = df.at[model, metric]\n",
    "            global_vals = all_results_df[key].dropna().values\n",
    "            minv, maxv = np.min(global_vals), np.max(global_vals)\n",
    "            \n",
    "            if metric in min_abs_cols:\n",
    "                rank = np.searchsorted(np.sort(np.abs(global_vals)), abs(val), side=\"right\")\n",
    "            else:\n",
    "                global_vals = np.sort(global_vals)\n",
    "                rank = np.searchsorted(global_vals, val, side=\"right\")\n",
    "            \n",
    "            pct = rank / len(global_vals)\n",
    "            if metric in max_cols:\n",
    "                pct = 1 - pct\n",
    "            idx = min(math.floor(pct * 10), 9)\n",
    "            bg = colors[idx]\n",
    "            font = \"white\" if idx in {0, 1, 8, 9} else \"black\"\n",
    "            \n",
    "            # html += f'<td style=\"background:{color};text-align:center;\"><span style=\"color:{font};\">{row[c]:.2f}</span></td>'\n",
    "            html += f'<td style=\"background:{bg}; text-align:center; padding:4px 6px;\"><span style=\"color:{font};\">{row[metric]:.2f}</span></td>'\n",
    "        \n",
    "        row_bg, row_font = model_label_styles.get(model, (\"#ffffff\", \"black\"))\n",
    "        html += f'<td style=\"background:{row_bg}; color:{row_font}; text-align:left; font-weight:bold;padding:4px 4px;\">{model}</td>'\n",
    "        # html += f'<td style=\"text-align:left; font-weight:bold; padding:4px 4px;\">{model}</td>'\n",
    "    html += '</tbody></table>'\n",
    "    # Add color legend\n",
    "    legend_html = '<table style=\"border-collapse:collapse; margin-bottom:4px;\"><caption>Rank percentile colour map (N=720):</caption><tr>'\n",
    "    for i, color in enumerate(colors):\n",
    "        pct_label = f\"{(i+1)*10}%\"\n",
    "        font = \"white\" if i in {0, 1, 8, 9} else \"black\"\n",
    "        legend_html += f'<td style=\"background:{color}; padding:4px 4px; text-align:center;\"><span style=\"color:{font}\">{pct_label}</span></td>'\n",
    "    legend_html += '</tr></table>'\n",
    "\n",
    "    return Div(text=html), Div(text=legend_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5df24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stn = '12143900'\n",
    "og_df = get_original_timeseries(stn, ds)\n",
    "lstm_ensemble_df = filter_by_complete_years(stn, lstm_result_base_folder)\n",
    "# boxley_df = retrieve_timeseries_discharge('12143700', ds)\n",
    "boxley_df = retrieve_timeseries_discharge('12143900', ds)\n",
    "# fig = figure(width=800, height=350, x_axis_type='datetime')\n",
    "# fig.line(boxley_df.index, boxley_df['12143700_uar'], line_width=2, color='blue', legend_label='Boxley Creek UAR')\n",
    "date = '20250514'\n",
    "ts = plot_runoff_timeseries('12143700', lstm_ensemble_df, og_df, date)\n",
    "ts.line(boxley_df.index, boxley_df['12143900_uar'], line_width=2, color='purple',\n",
    "        line_dash='dashed', legend_label='12143900 UAR')\n",
    "# change the legend labels\n",
    "# Rename a legend label\n",
    "for legend in ts.legend:\n",
    "    for item in legend.items:\n",
    "        if item.label['value'] == \"Observed UAR\":\n",
    "            print(item.label)\n",
    "            item.label.value = \"12143700 UAR\"\n",
    "            # item.label['value'] = \"12143700 UAR\"\n",
    "            # break # Exit inner loop once found\n",
    "        elif item.label['value'] == \"Observed UAR\":\n",
    "            item.label.value = \"12143900 UAR\"\n",
    "            # item.label['value'] = \"12143900 UAR\"\n",
    "            # break # Exit inner loop once found\n",
    "\n",
    "show(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb23fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_diagnostic_plot_layout(stn):\n",
    "    output_folder = BASE_DIR / 'data' / 'results' /  'lstm_plots'\n",
    "    result_folder = BASE_DIR / 'data' / 'results' /'fdc_estimation_results'\n",
    "\n",
    "    og_df = get_original_timeseries(stn, ds)\n",
    "    lstm_ensemble_df = filter_by_complete_years(stn, lstm_result_base_folder)\n",
    "\n",
    "    if lstm_ensemble_df.empty:\n",
    "        print(f'No complete years found for {stn}. Skipping.')\n",
    "        return Div(text=f'<h2>Insufficient records found for {stn}. Skipping.</h2>', width=800)\n",
    "    \n",
    "    og_df = og_df[og_df.index.isin(lstm_ensemble_df.index)]\n",
    "    mdf, odf, pmf_dfs = process_FDCs(lstm_ensemble_df, stn, og_df, result_folder)\n",
    "\n",
    "    dates = list(pmf_dfs.keys())\n",
    "    pdf_plots, fdc_plots, metric_tables, other_tables = [], [], [], []\n",
    "    date = '20250514'\n",
    "    \n",
    "    pdf_plot, baseline_log_grid, baseline_lin_grid, log_w = plot_observed_and_simulated_pdf(stn, pmf_dfs, og_df, date, pdf_plots=pdf_plots)\n",
    "    pdf_plots.append(pdf_plot)\n",
    "    \n",
    "    fdc_plot, fdc_metrics = plot_observed_and_simulated_fdc(stn, pmf_dfs, baseline_lin_grid, lstm_ensemble_df, og_df, date, fdc_plots=fdc_plots)\n",
    "    fdc_plots.append(fdc_plot)\n",
    "    \n",
    "    ts_plot = plot_runoff_timeseries(stn, lstm_ensemble_df, og_df, dates[-1])\n",
    "    \n",
    "    metric_table, legend_html = format_metrics_table(mdf, odf, stn, date, fdc_metrics)\n",
    "\n",
    "    table_div = column(\n",
    "        metric_table,\n",
    "        legend_html,\n",
    "    )\n",
    "    notes_div = Div(text=notes_html, width=1000)\n",
    "    layout = column(\n",
    "        row(fdc_plots[0], table_div),\n",
    "        row(ts_plot), \n",
    "        row(pdf_plots[0]),\n",
    "        row(notes_div),\n",
    "        )\n",
    "    return layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd4d33a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# retrieve LSTM ensemble predictions\n",
    "# filter for the common stations\n",
    "common_stations = list(set(station_ids) & set(lstm_result_stns))\n",
    "stations_to_process = ['08AA008', '12090500', '12102190', '12115700', '12115800',\n",
    "       '12157025', '12201960', '10CD003', '10CD004', '12447383',\n",
    "       '08HB075', '10ED009', '08HA026', '12202310', '10CD005', '12202420',\n",
    "       '12210900', '12193500', '12036650', '07BB003']\n",
    "\n",
    "# print(f'There are {len(common_stations)} monitored basins with LSTM ensemble results.')\n",
    "attr_df = attr_df[attr_df['official_id'].isin(stations_to_process)]\n",
    "\n",
    "output_folder = BASE_DIR / 'data' / 'results' /  'lstm_plots'\n",
    "result_folder = BASE_DIR / 'data' / 'results' /'fdc_estimation_results'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# plots = []\n",
    "excluded = ['12137800'] \n",
    "dam_sites = ['12398000', '12058800', '12143700', '12323760'] \n",
    "process_plots = True\n",
    "if process_plots:\n",
    "    for stn in common_stations:\n",
    "        if stn in dam_sites:\n",
    "            continue\n",
    "        output_fname = output_folder / f'{stn}_fdc.html'\n",
    "        if stn in excluded:\n",
    "            continue\n",
    "        # if os.path.exists(output_fname):\n",
    "        #     print(output_fname)\n",
    "        #     continue\n",
    "        layout = process_diagnostic_plot_layout(stn)\n",
    "        # save the plot to an HTML file\n",
    "        output_fname = output_folder / f'{stn}_fdc.html'\n",
    "        output_file(output_fname, title=f'{stn} FDCs')\n",
    "        save(layout)\n",
    "        print(f'    Saved plot for {stn} to {output_fname}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0582a197-227b-4414-a5e9-9adcd3671bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef288d3-6772-4fce-82f7-309fc3eb1892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754d6ded-8f6b-4947-ad95-48c20f61e3f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
