{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a31275eb",
   "metadata": {},
   "source": [
    "# Large Sample Comparison Results\n",
    "\n",
    "In this section:\n",
    "\n",
    "1. We compile the full set of model results across all catchments and performance metrics, creating a consistent large-sample evaluation dataset.\n",
    "\n",
    "2. We analyse the distribution of model skill, comparing parametric, kNN, LSTM, and ensemble methods across multiple residual error measures and the KL divergence.\n",
    "\n",
    "3. We examine cross-model rank correlations to identify where models preserve distinct or overlapping information about the FDC.\n",
    "\n",
    "4. We interpret large-sample patterns to assess when model complexity matters, when simpler methods remain competitive, and where ensembles provide measurable gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558cd32f-b562-4502-ab63-826aede8f321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import xyzservices.providers as xyz\n",
    "from scipy.stats import linregress\n",
    "\n",
    "from bokeh.plotting import figure, show, output_file, save\n",
    "from bokeh.layouts import gridplot, row, column, layout\n",
    "# from bokeh.transform import factor_cmap, linear_cmap\n",
    "# from bokeh.models.formatters import CustomJSTickFormatter\n",
    "\n",
    "from bokeh.models import ColumnDataSource, LinearAxis, Range1d, HoverTool, Div\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.palettes import Sunset10, Vibrant7, Category20, Bokeh4, Bokeh6, Bokeh7, Bokeh8, Greys256, Blues256\n",
    "\n",
    "from bokeh.models import ColumnDataSource, LinearColorMapper, ColorBar, PrintfTickFormatter, FixedTicker\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.transform import transform\n",
    "from bokeh.palettes import RdBu\n",
    "\n",
    "from bokeh.plotting import figure\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.ticker import NullFormatter, AutoMinorLocator, LogFormatterMathtext #, SymmetricalLogLocator\n",
    "\n",
    "# from shapely.geometry import Point\n",
    "\n",
    "# from utils.kde_estimator import KDEEstimator\n",
    "# from utils.fdc_estimator_context import FDCEstimationContext\n",
    "# from utils.fdc_data import StationData\n",
    "# from utils.evaluation_metrics import EvaluationMetrics\n",
    "\n",
    "import utils.data_processing_functions as dpf\n",
    "\n",
    "import xyzservices.providers as xyz\n",
    "tiles = xyz['USGS']['USTopo']\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87c598e",
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_fpath = 'data/catchment_attributes_with_runoff_stats.csv'\n",
    "attr_df = pd.read_csv(attr_fpath, dtype={'Official_ID': str})\n",
    "station_ids = sorted(attr_df['official_id'].unique().tolist())\n",
    "\n",
    "# streamflow folder from (updated) HYSETS\n",
    "HYSETS_DIR = Path('/home/danbot/code/common_data/HYSETS')\n",
    "hs_df = pd.read_csv('data/HYSETS_watershed_properties.txt', sep=';')\n",
    "hs_df = hs_df[hs_df['Official_ID'].isin(station_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870b0bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "watershed_id_dict = {row['Watershed_ID']: row['Official_ID'] for _, row in hs_df.iterrows()}\n",
    "# and the inverse\n",
    "official_id_dict = {row['Official_ID']: row['Watershed_ID'] for _, row in hs_df.iterrows()}\n",
    "# also for drainage areas\n",
    "da_dict = {row['Official_ID']: row['Drainage_Area_km2'] for _, row in hs_df.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1356ccc6-3d6d-4e08-bdbc-53a798187d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve LSTM ensemble predictions\n",
    "LSTM_ensemble_result_folder = '/home/danbot/code/neuralhydrology/data/ensemble_results_20250514'# uses NSE mean as loss function\n",
    "# LSTM_ensemble_result_folder = '/home/danbot/code/neuralhydrology/data/ensemble_results_20250627'# uses NSE 95% as loss function\n",
    "lstm_result_files = os.listdir(LSTM_ensemble_result_folder)\n",
    "lstm_result_stns = [e.split('_')[0] for e in lstm_result_files]\n",
    "\n",
    "# filter for the common stations between BCUB region and LSTM-compatible (i.e. 1980-)\n",
    "daymet_concurrent_stations = list(set(station_ids) & set(lstm_result_stns))\n",
    "# assert '012414900' in daymet_concurrent_stations\n",
    "print(f'There are {len(daymet_concurrent_stations)} monitored basins concurrent with LSTM ensemble results.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c96fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "regularization_type = 'discrete'  # options: 'kde' or 'discrete'\n",
    "# the bitrate used for the PMF representation\n",
    "bitrate = 5  # number of bins = 2**bitrate\n",
    "\n",
    "#  load the formatted parametric/lstm results from the previous notebook\n",
    "formatted_result_fpath = f'data/results/additional_results/formatted_results_by_performance_measure_{bitrate:02d}_bits.csv'\n",
    "knn_result_path = f'data/results/additional_results/knn_all_results_formatted_{bitrate:02d}_bits.csv'\n",
    "\n",
    "if regularization_type == 'kde':\n",
    "    formatted_result_fpath = f'data/results/additional_results/formatted_results_by_performance_measure_kde.csv'\n",
    "    knn_result_path = f'data/results/additional_results/knn_all_results_formatted_kde.csv'\n",
    "\n",
    "fdc_df = pd.read_csv(formatted_result_fpath, dtype={'Official_ID': str})\n",
    "\n",
    "# load the formatted knn results\n",
    "knn_df = pd.read_csv(knn_result_path, dtype={'Official_ID': str})\n",
    "len(knn_df['Official_ID'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedee171",
   "metadata": {},
   "source": [
    "Load the global mean PMF and resample to the higher resolution evaluation grid (12 bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fd6b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the mean global PMF\n",
    "mean_pmf_df_bits = pd.read_csv(f'data/results/sample_distribution_mixture/mean_distribution_{bitrate}bits.csv')\n",
    "# mean_pmf_df_bits.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "# # upsample to the 12 bits over the same range\n",
    "# a, b = mean_pmf_df_bits['log_x_uar'].min(), mean_pmf_df_bits['right_log_edges'].max()\n",
    "# log_edges_10bit = np.linspace(a, b, 2**10 + 1)\n",
    "# log_x_10bit = 0.5 * (log_edges_10bit[:-1] + log_edges_10bit[1:])\n",
    "# # interpolate the mean_pmf_df to the 12-bit edges\n",
    "# pmf_resampled = np.interp(\n",
    "#     x=log_x_10bit,\n",
    "#     xp=mean_pmf_df_bits['log_x'],\n",
    "#     fp=mean_pmf_df_bits['pmf']\n",
    "# )\n",
    "# mean_pmf_df = pd.DataFrame({\n",
    "#     'left_log_edges': log_edges_10bit[:-1],\n",
    "#     'right_log_edges': log_edges_10bit[1:],\n",
    "#     'log_x': log_x_10bit,\n",
    "#     'pmf': pmf_resampled,\n",
    "# })\n",
    "# mean_pmf_df['pmf'] /= mean_pmf_df['pmf'].sum() # normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7a4e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_filter_hysets_data(station_ids, hs_df):\n",
    "    hs_df = hs_df[hs_df['Official_ID'].isin(station_ids)]\n",
    "\n",
    "    # load the updated HYSETS data\n",
    "    updated_filename = 'HYSETS_2023_update_QC_stations.nc'\n",
    "    ds = xr.open_dataset(HYSETS_DIR / updated_filename)\n",
    "\n",
    "    # Get valid IDs as a NumPy array\n",
    "    selected_ids = hs_df['Watershed_ID'].values\n",
    "\n",
    "    # Get boolean index where watershedID in selected_set\n",
    "    # safely access watershedID as a variable first\n",
    "    ws_ids = ds['watershedID'].data  # or .values if you prefer\n",
    "    mask = np.isin(ws_ids, selected_ids)\n",
    "\n",
    "    # Apply mask to data\n",
    "    ds = ds.sel(watershed=mask)\n",
    "    # Step 1: Promote 'watershedID' to a coordinate on the 'watershed' dimension\n",
    "    ds = ds.assign_coords(watershedID=(\"watershed\", ds[\"watershedID\"].data))\n",
    "\n",
    "    # Step 2: Set 'watershedID' as the index for the 'watershed' dimension\n",
    "    return ds.set_index(watershed=\"watershedID\")\n",
    "\n",
    "ds = load_and_filter_hysets_data(station_ids, hs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073de4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_empirical_cdf(data):\n",
    "    \"\"\"Compute the empirical CDF of the data.\n",
    "    Address missing, NaN, and non-finite values by removing them.\n",
    "    \"\"\"\n",
    "    # data = data[~np.isnan(data) & np.isfinite(data)]  \n",
    "    # remove to see the CDFs without filtering\n",
    "    assert np.all(np.isfinite(data))\n",
    "    sorted_data = np.sort(data)  \n",
    "    # print('sorted_data: ', sorted_data[:5])\n",
    "    n = len(sorted_data)\n",
    "    cdf = np.arange(1, n + 1) / (n + 1)\n",
    "    # print(f'    min cdf = {np.min(cdf):.2f}')\n",
    "    return sorted_data, cdf\n",
    "\n",
    "\n",
    "def retrieve_timeseries_discharge(stn, ds):\n",
    "    watershed_id = official_id_dict[stn]\n",
    "    # drainage_area = self.ctx.da_dict[stn]\n",
    "    # data = self.ctx.data\n",
    "    df = ds['discharge'].sel(watershed=str(watershed_id)).to_dataframe(name='discharge').reset_index()\n",
    "    df = df.set_index('time')[['discharge']]\n",
    "    df['zero_flow_flag'] = df['discharge'] == 0\n",
    "    df.dropna(inplace=True)\n",
    "    # clip minimum flow to 1e-4\n",
    "    df['discharge'] = np.clip(df['discharge'], 1e-4, None)\n",
    "    df.rename(columns={'discharge': stn}, inplace=True)\n",
    "    df[f'{stn}_uar'] = 1000 * df[stn] / da_dict[stn]\n",
    "    return df\n",
    "\n",
    "\n",
    "def compare_results_and_input(stn, sim_df, ds):\n",
    "    \"\"\"Compare the input streamflow timeseries with the observed streamflow timeseries.\n",
    "    Check that the dates in the output match the common dates between Daymet and the input data.\n",
    "    \"\"\"\n",
    "    input_df = retrieve_timeseries_discharge(stn, ds)\n",
    "    # clip the 'discharge' column to 1e-4, convert to unit area runoff (L/s/km2), and take the log\n",
    "    input_df = input_df[input_df.index >= '1980-01-01']\n",
    "\n",
    "    df = pd.concat([input_df, sim_df], axis=1, join='inner')\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    df['streamflow_obs'] = np.exp(df['streamflow_obs'])\n",
    "    sim_cols = [c for c in sim_df.columns if c.startswith('streamflow_sim')]\n",
    "    df[sim_cols] = np.exp(df[sim_cols])\n",
    "    # assert that the 'log_obs' and the 'streamflow_obs' columns are approximately equal\n",
    "\n",
    "    # set tolerance in the order of 1 L/s/km2\n",
    "    if not np.allclose(df[f'{stn}_uar'], df['streamflow_obs'], atol=1): \n",
    "        max_diff = np.abs(df[f'{stn}_uar'] - df['streamflow_obs']).max()\n",
    "        print(f'Warning: {stn} has a max difference of {max_diff:.2f} between the input and output streamflow timeseries.')\n",
    "        # find the dates around the max difference index\n",
    "        # diff_index = np.abs(df['uar'] - df['streamflow_obs']).idxmax()\n",
    "        # print(diff_index)\n",
    "        # view 5 before and 5 after the max diff index\n",
    "        # print(df.loc[diff_index - pd.Timedelta(days=5):diff_index + pd.Timedelta(days=5), ['uar', 'streamflow_obs']].head(10))\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def process_lstm_station(args):\n",
    "    stn, folder, ds = args\n",
    "    try:\n",
    "        fpath = os.path.join(folder, f'{stn}_ensemble.csv')\n",
    "        df = pd.read_csv(fpath)\n",
    "        df.rename(columns={'Unnamed: 0': 'time'}, inplace=True)\n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "        df.set_index('time', inplace=True)\n",
    "        result = compare_results_and_input(stn, df, ds)\n",
    "        return stn, result\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {stn}: {e}\")\n",
    "        return stn, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd4d33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve LSTM ensemble predictions\n",
    "# filter for the common stations\n",
    "common_stations = list(set(station_ids) & set(lstm_result_stns) & set(knn_df['Official_ID'].unique()))\n",
    "print(f'There are {len(common_stations)} monitored basins with LSTM ensemble results.')\n",
    "# attr_df = attr_df[attr_df['official_id'].isin(common_stations)]\n",
    "\n",
    "# args_list = [(stn, lstm_result_folder, ds) for stn in common_stations]\n",
    "\n",
    "# with Pool() as pool:\n",
    "#     lstm_results = dict(pool.map(process_lstm_station, args_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a48368-9dda-4442-a09c-c506db94ba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the predicted parameter results (Notebook 3)\n",
    "target_cols = [\n",
    "    'uar_mean_predicted', 'uar_std_predicted', 'uar_median_predicted', 'uar_mad_predicted',\n",
    "    'log_uar_mean_predicted', 'log_uar_std_predicted', 'log_uar_median_predicted', 'log_uar_mad_predicted',\n",
    "]\n",
    "\n",
    "parameter_prediction_results_folder = 'data/results/parameter_prediction_results'\n",
    "predicted_params_fpath   = os.path.join(parameter_prediction_results_folder, 'OOS_parameter_predictions.csv')\n",
    "stats = pd.read_csv(predicted_params_fpath, index_col=['official_id'], dtype={'official_id': str})\n",
    "stats.head()\n",
    "stats.columns = ['_'.join(c.split('_')) for c in stats.columns]\n",
    "print(f' Loaded {len(stats)} stations with predicted parameters from {predicted_params_fpath}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd4b47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = []\n",
    "m1s = ['log_uar_mean_actual', 'log_uar_std_actual']\n",
    "m2s = ['log_uar_median_actual', 'log_uar_mad_actual']\n",
    "for m1, s1 in [m1s, m2s]:\n",
    "    p = figure(title=f'Observed {m1} vs. {s1} over (N={len(stats)})', width=600, height=350)\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(stats[m1], stats[s1])\n",
    "    p.scatter(stats[m1], stats[s1], size=10, color='green', alpha=0.5, legend_label='Observed')\n",
    "    x = np.linspace(stats[m1].min(), stats[m1].max(), 100)\n",
    "    p.line(x, slope * x + intercept, color='red', legend_label=f'Y={slope:.2f}x + {intercept:.2f} (R²={r_value**2:.2f})', line_width=2)\n",
    "    p.xaxis.axis_label = f'Log {m1.split('_')[2]} unit area runoff (L/s/km²)'\n",
    "    p.yaxis.axis_label = f'Log {s1.split('_')[2]} of unit area runoff (L/s/km²)'\n",
    "    # p.legend.location = 'top_left'\n",
    "    p = dpf.format_fig_fonts(p,font_size=14)\n",
    "    plots.append(p)\n",
    "lt = column(plots)\n",
    "# show(lt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7ead98-de6f-4f40-918a-6bdc42350631",
   "metadata": {},
   "source": [
    "### Load Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6c7871",
   "metadata": {},
   "source": [
    "We started by asking about the \"simplest\" approximation of an FDC, the parametric approximation from predicted values.  \n",
    "\n",
    "1. First, let's review the predicted and \"observed\" mean, standard deviation, log-mean, and log-standard deviation predicted from catchment attributes.  This will give us a first clue of what to expect as far as which approach yields the better approximation of the FDC.  Since we're predicting these values using an objective function that minimizes the difference between predicted and observed values, we should first look at the distribution of the target values, since some metrics are sensitive to skewness and outliers.  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25415d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_plots, plots = [], []\n",
    "for target in target_cols:\n",
    "    \n",
    "    obs_label = '_'.join(target.split('_')[:-1]) + '_actual'\n",
    "    pred, obs = stats[target].values, stats[obs_label].values\n",
    "    # get the regression results from scipy.stats.linregress()\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(obs, pred)\n",
    "\n",
    "    f = figure()\n",
    "    x = np.linspace(obs.min(), obs.max(), 100)\n",
    "    y = slope * x + intercept\n",
    "    \n",
    "    f.scatter(obs, pred, size=2, color='black', alpha=0.5, legend_label=f'{target}')\n",
    "    f.line(x, y, line_color='firebrick', line_width=2, legend_label=f'{target} (R²={r_value**2:.2f})')\n",
    "    f.legend.location = 'top_left'\n",
    "    f.yaxis.axis_label = f'Predicted {target} (L/s/km²)'\n",
    "    f.xaxis.axis_label = f'Observed {target} (L/s/km²)'\n",
    "    \n",
    "    # Create histogram data\n",
    "    hist, edges = np.histogram(obs, bins=20)\n",
    "    hist = hist / hist.sum() * 100  # Normalize to percentage\n",
    "\n",
    "    # Prepare data for quad glyph: left and right edges of bins\\\n",
    "    hist_source = ColumnDataSource(data=dict(\n",
    "        left=edges[:-1],\n",
    "        right=edges[1:],\n",
    "        top=hist,\n",
    "        bottom=np.zeros_like(hist)\n",
    "    ))\n",
    "\n",
    "    # Create figure\n",
    "    hist_plot = figure(width=450, height=100, x_axis_label=target, y_axis_label='Percentage')\n",
    "\n",
    "    # Plot using quad\n",
    "    hist_plot.quad(\n",
    "        top='top', bottom='bottom', left='left', right='right',\n",
    "        source=hist_source,\n",
    "        fill_color='dodgerblue', fill_alpha=0.5, line_color='black'\n",
    "    )\n",
    "\n",
    "    # Optional styling\n",
    "    hist_plot.xaxis.axis_label = target\n",
    "    hist_plot.yaxis.axis_label = 'P(x)'\n",
    "    f.legend.background_fill_alpha = 0.3\n",
    "    f = dpf.format_fig_fonts(f, font_size=12)\n",
    "    hist_plots.append(hist_plot)\n",
    "    plots.append(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da437ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt = gridplot(plots, ncols=4, width=350, height=300)\n",
    "# show(lt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205f2099",
   "metadata": {},
   "source": [
    "From the plots above, the coefficient of determination around 0.8 for the mean and standard deviation show that these are reasonably well predicted from catchment attributes.  The log-mean is also fairly predictable, but the log-standard deviation is not.  This might lead us to expect that the FDC approximation based on the predicted log-mean and log-stdev might be worse than the approximation based on the predicted mean and stdev.  However, the method of moments must still be applied to the mean and stdev to get the log-normal parameters.  \n",
    "\n",
    "The question is how the structural error introduced by the method of moments interacts with the parametric model, and how the log-transform affects the distribution of the target variables and the objective function of the predictive model by extension.  While the log-transform doesn't change the rank of the values, it still changes the emphasis on the objective function of the predictive model, since the model's performance can vary significantly based on the distribution of the input data.  By de-emphasizing the tails by the log-transform, we are effectively changing the model's focus to the central tendency of the data.  (should we consider quantile regression or another form of robust regression)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d0c310",
   "metadata": {},
   "source": [
    "## kNN - based FDC approximation\n",
    "\n",
    "### Nuance of \"concurrent\" kNN: k or not k?\n",
    "\n",
    "When using kNN derived from sparse monitoring networks with inconsistent coverage, the \"k\" isn't exactly \"k\".  At the timestep level, addressing gaps in data yields different interpretations of k, where the simplest interpretation is that it represents the number of independent monitoring locations used to generate an ensemble simulation of an unmonitored location.  Werstuck and Coulibaly (2018) describe infilling data gaps with kNN, effectively a nested kNN, which appears most consistent with the \"dynamic k\" described below, but it is not the same, rather the infilled data are a temporal mean which suppresses the variability of the ensemble.  Below we describe several variants that address data gaps in kNN selection, and how they relate to the concept of k.:\n",
    "\n",
    "* **Maximum k**:  The maximum number of stations that can be used to generate an ensemble simulation.  In the event of missing observations at one or more k, the effective number of stations is less k.  Overall, $k_\\text{actual} \\leq k$.  Given periods of concurrent gaps, the ensemble simulation could effectively be much lower than k, though it must be at least 1.\n",
    "* **Strict k**: The number of stations used to generate an ensemble simulation is strictly k.  This method is related to the set-cover problem, where the goal is to select a subset of stations that maximizes the intersection of their data availability over a specified time period.  The problem is NP-hard and requires a greedy or approximate subset selection strategy when exhaustively testing all combinations becomes computationally infeasible.  I set a lenient minimum concurrent period (5 complete periods of 12 consecutive complete months (minimum 10 days per month)) to avoid expensive computation.  This is more truly \"k-nearest\", but it achieves this by searching further away in the network and ignoring potentially more relevant information if it is not concurrent with all records -- the higher the k, the greater chance of misalignment of records.\n",
    "* **Effective k**:  The number of stations used to generate the ensemble simulation increases until average k observations per timestep.  In the event of missing observations at one or more k, more (less similar, more distant) stations must be incorporated to yield an average of k ensemble members per timestep.  Overall, $k_\\text{actual} \\geq k$.  This requires generating weights per timestep such that they sum to 1 where the set comprising k are not constant.\n",
    "* **Dynamic k**: The number of stations overall to generate an ensemble simulation varies, but we guarantee that the number of stations used to generate the simulation is **k at each timestep**.  Here k is really fixed but the stations may vary across timesteps.\n",
    "\n",
    "\n",
    "The goal is to better understand the trade-off between the selection criteria and the performance of the kNN method under as broad a range of conditions as possible.  On the one hand, we can be strict in the temporal sense that it is only valid to compare observations that occurred at the same time, but another interpratation is that a significant proportion of the interannual variability can be captured given sufficient data, and more of the interannual variability can be covered by not requiring concurrency.  Strictness in the spatial (neighbour) sense means we take exactly the same k neighbours, but this leads to loss of data because more distant neighbours must be sought to satisfy the \"k\" contributors constraint, whereas we can interpret it as \"take the best information where it's available, and resort to potentially less relevant contributors if necessary to fill gaps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8f4e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result_and_ids(label, metric):\n",
    "    data = fdc_df[fdc_df['Label'] == label].copy()\n",
    "    data = data.dropna(subset=[metric])\n",
    "    values = data[metric].values\n",
    "    return values, data['Official_ID']\n",
    "\n",
    "\n",
    "def get_knn_group_results(tree_type='attribute', ensemble_type='freqEnsemble', weighting='ID2', k=7, which_set='knn'):\n",
    "    data = knn_df.copy()\n",
    "    data = data[data['tree_type'] == tree_type]\n",
    "    data = data[data['ensemble_method'] == ensemble_type]\n",
    "    data = data[data['ensemble_weight'] == weighting]\n",
    "    data = data[data['k'] == k]\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_model_data(label, model_key, dm, regularization_type='discrete'):\n",
    "    \"\"\"Load and return data and IDs for a given model and metric.\"\"\"\n",
    "    if '-NN' in label:\n",
    "        k = int(label.split('-')[0])\n",
    "        df = get_knn_group_results(k=k)\n",
    "        values = df[dm].values\n",
    "        return values, df['Official_ID'].values \n",
    "\n",
    "    elif label.startswith('LSTM') and '4NN' not in label:\n",
    "        key = 'time' if 'Time' in label else 'frequency'\n",
    "        df = fdc_df[fdc_df['Label'] == key]\n",
    "        return df[dm].values, df['Official_ID'].values\n",
    "\n",
    "    elif '4NN' in label:\n",
    "        if label == 'LSTM-LN-4NN':\n",
    "            fname = 'knn-lstm-lognorm_4NN.csv'\n",
    "        elif label == 'LSTM-4NN':\n",
    "            fname = 'knn-lstm_4NN.csv'\n",
    "        elif label == 'LN-4NN':\n",
    "            fname = 'knn-lognorm_4NN.csv'\n",
    "        else:\n",
    "            raise ValueError(f'Unknown 4NN label: {label}')\n",
    "        result_fpath = os.path.join(f'data/results/ensemble_results/', f'{bitrate:02d}_bits', fname)\n",
    "        if regularization_type == 'kde':\n",
    "            result_fpath = os.path.join(f'data/results/ensemble_results/', f'kde', fname)\n",
    "        df = pd.read_csv(result_fpath)\n",
    "        key_map = {'PB': 'pct_vol_bias', 'MAPE': 'mean_abs_pct_error', \n",
    "                   'RMSE': 'rmse', 'KLD': 'kld', 'NSE': 'nse', 'VE': 've', \n",
    "                   'NAE': 'nae'}\n",
    "        if dm == 'NAE':\n",
    "            data = 100 * (1 - df['ve'].values)\n",
    "        elif dm == 'RMSE':\n",
    "            data = 100*(np.exp(df[key_map[dm]].values) - 1)\n",
    "        elif dm == 'NSE':\n",
    "            data = 1 - df[key_map[dm]].values\n",
    "        elif dm in ['VE']:\n",
    "            data = 100 * (1 - df[key_map[dm]].values)\n",
    "        else:\n",
    "            data = df[key_map[dm]].values\n",
    "        \n",
    "        assert not np.isnan(data).any(), f'NaNs in {label} {dm}'\n",
    "        return data, df['stn_id'].values\n",
    "    else:\n",
    "        return get_result_and_ids(model_key, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e939d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_specs = [\n",
    "    ('MLE', 'dashed', 'grey', 'MLE* LogNorm'),\n",
    "    ('PredictedMOM', 'dashed', 'black', 'MoM LogNorm'),\n",
    "    ('PredictedLog', 'solid', 'black', 'LogNorm'),\n",
    "    ('2-NN', 'solid', 'orange', '2-NN'),\n",
    "    ('8-NN', 'dashed', 'orange', '8-NN'),\n",
    "    ('LSTM Time', 'dotted', 'green', 'LSTM Time'),\n",
    "    ('LSTM Dist.', 'solid', 'green', 'LSTM Dist.'),\n",
    "    ('LN-4NN', 'dotted', 'magenta', 'LN-4NN'),\n",
    "    ('LSTM-4NN', 'solid', 'magenta', 'LSTM-4NN'),\n",
    "    ('LSTM-LN-4NN.', 'dashed', 'magenta', 'LSTM-LN-4NN'),\n",
    "]\n",
    "all_metrics = ['PB', 'NAE', 'RMSE', 'NSE', 'KLD', ]#, 'VB_FDC', 'MEAN_FRAC_DIFF']\n",
    "axis_labels = [\n",
    "    r'$$\\text{PB} \\text{ [\\%]}$$',\n",
    "    r'$$\\text{NAE} \\text{ [\\%]}$$',\n",
    "    r'$$\\text{RMSE } [\\%]$$',\n",
    "    r'$$1 - \\text{NSE} \\text{ [-]}$$',\n",
    "    r'$$\\text{KLD} \\text{ [bits/sample]}$$',\n",
    "]\n",
    "plotting_metrics = [m for m in all_metrics if m != 'PB']\n",
    "plotting_axis_labels = [e for e in axis_labels if 'PB' not in e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67624ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FormatStrFormatter, FixedLocator, NullFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "from bokeh.palettes import Sunset10\n",
    "\n",
    "def plot_parametric_bounds(f, dm, lb_label='MLE', rb_label='RandomDraw', label='LogNorm bounds', color='black', divide_re=1.):\n",
    "    ub = fdc_df[fdc_df['Label'] == lb_label][dm].values\n",
    "    lb = fdc_df[fdc_df['Label'] == rb_label][dm].values\n",
    "    x2, y = compute_empirical_cdf(ub)\n",
    "    x1, _ = compute_empirical_cdf(lb)\n",
    "    f.harea(x1=x1, x2=x2, y=y, fill_color=color, fill_alpha=0.3, legend_label=label)\n",
    "    # plot the mean pmf line\n",
    "    for label in ['Mean_PMF', 'Uniform']:\n",
    "        new_pmf = fdc_df[fdc_df['Label'] == label][dm].values\n",
    "        x_mean, y_mean = compute_empirical_cdf(new_pmf)\n",
    "        f.line(x_mean, y_mean, line_color='red', line_width=2, legend_label='Mean PMF')\n",
    "    return f, max(np.max(x1), np.max(x2))\n",
    "\n",
    "\n",
    "def plot_parametric_bounds_matplotlib(ax, dm, fdc_df, divide_re=1.0, #mean_pmf,\n",
    "                                      lb_label='MLE', rb_label='RandomDraw',\n",
    "                                      label='LogNorm bounds', color='black', alpha=0.25):\n",
    "    ub = fdc_df[fdc_df['Label'] == lb_label][dm].values\n",
    "    lb = fdc_df[fdc_df['Label'] == rb_label][dm].values\n",
    "\n",
    "    x2, y = compute_empirical_cdf(ub)\n",
    "    x1, _ = compute_empirical_cdf(lb)\n",
    "    ax.fill_betweenx(y, x1, x2, color=color, alpha=alpha, label=label)\n",
    "    for l, c in zip(['Mean_PMF', 'Uniform'], ['red', 'brown']):\n",
    "        if l == 'Uniform':\n",
    "            continue\n",
    "        new_pmf = fdc_df[fdc_df['Label'] == l][dm].values\n",
    "        mn, _ = compute_empirical_cdf(new_pmf)\n",
    "        ax.plot(mn, y, color=c, linewidth=2, label=l)\n",
    "\n",
    "    return max(np.max(x1), np.max(x2), np.max(mn))\n",
    "\n",
    "\n",
    "def add_vertical_spans(ax, ticks):\n",
    "    ticks = list(ticks)\n",
    "    for i in range(0, len(ticks) - 1, 2):\n",
    "        ax.axvspan(ticks[i], ticks[i+1], \n",
    "                   facecolor='none', edgecolor='grey', alpha=0.2)\n",
    "    \n",
    "        \n",
    "def create_knn_plots_matplotlib_grid(results_dfs, completed_stns, all_metrics, metric_label_dict, ncols=3, savepath=None):\n",
    "\n",
    "\n",
    "    knn_df = results_dfs.copy()\n",
    "    combos = [\n",
    "        ('ID2', 'timeEnsemble', 'spatial'),\n",
    "        ('ID2', 'freqEnsemble', 'spatial'),\n",
    "        ('ID2', 'freqEnsemble', 'attribute'),\n",
    "    ]\n",
    "    \n",
    "    clrs = Sunset10\n",
    "    ncols = len(combos)\n",
    "\n",
    "    total = len(all_metrics) * len(combos)\n",
    "    nrows = (total + ncols - 1) // ncols\n",
    "    fig, axs = plt.subplots(nrows, ncols, figsize=(ncols * 5, nrows * 4))\n",
    "    axs = axs.flatten()\n",
    "    plot_idx = 0\n",
    "\n",
    "    n = 1\n",
    "    for metric in all_metrics:\n",
    "        print(metric)\n",
    "        if metric == 'PB':\n",
    "            continue\n",
    "        for wm, ensemble, tree in combos:\n",
    "            df = knn_df.query(\n",
    "                \"ensemble_weight == @wm and ensemble_method == @ensemble and tree_type == @tree\"\n",
    "            )\n",
    "            if df.empty:\n",
    "                print(f\"No data for {tree} {ensemble} {metric}\")\n",
    "                continue\n",
    "\n",
    "            ax = axs[plot_idx]       \n",
    "            legend_loc = 'upper left'\n",
    "            plot_idx += 1\n",
    "            linthresh = 1.0\n",
    "            # bound_metric = metric if metric != 'RB' else 'MAE'\n",
    "            max_val = plot_parametric_bounds_matplotlib(ax, metric, fdc_df)\n",
    "            # tick_loc = 1\n",
    "            if metric == 'PB':\n",
    "                linthresh = 100\n",
    "                xmin, xmax = -5, 500\n",
    "                b_xmin, b_xmax = 100, 1000\n",
    "                ticks = [0, 25, 50, 75, 100, 200, 500]\n",
    "                add_vertical_spans(ax, np.arange(ticks[0], ticks[-1], 25))\n",
    "                str_format = '%.0f'\n",
    "            elif metric == 'NSE':\n",
    "                linthresh = 1.0\n",
    "                xmin, xmax = -0.05, 5.0\n",
    "                b_xmin, b_xmax = linthresh, 10\n",
    "                ticks = [0, 0.25, 0.5, 1.0, 2.0, 5.0]\n",
    "                add_vertical_spans(ax, np.arange(ticks[0], ticks[-1], 0.25))\n",
    "                str_format = '%.2f'\n",
    "            elif metric == 'KLD':\n",
    "                linthresh = 1.0\n",
    "                xmin, xmax = -0.05, 2\n",
    "                b_xmin, b_xmax = linthresh, 5\n",
    "                ticks = [0, 0.25, 0.5, 0.75, 1.0, 2.0]\n",
    "                add_vertical_spans(ax, np.arange(ticks[0], ticks[-1], 0.25))\n",
    "                str_format = '%.2f'\n",
    "            elif metric == 'NAE':\n",
    "                linthresh = 100\n",
    "                xmin, xmax = -5, 505\n",
    "                b_xmin, b_xmax = linthresh, 500\n",
    "                ticks = [0, 25, 50, 75, 100, 200, 500]\n",
    "                add_vertical_spans(ax, np.arange(ticks[0], ticks[-1], 20))\n",
    "                str_format = '%.0f'\n",
    "            else:                \n",
    "                linthresh = 100.0\n",
    "                xmin, xmax = -5, 500\n",
    "                b_xmin, b_xmax = 100, 500\n",
    "                ticks = [0, 25, 50, 75, 100, 200, 500]\n",
    "                add_vertical_spans(ax, np.arange(ticks[0], ticks[-1], 25))\n",
    "                str_format = '%.0f'\n",
    "\n",
    "            minor_positions = ticks\n",
    "            ax.set_xscale('symlog', linthresh=linthresh)\n",
    "            ax.set_xlim(xmin, xmax)\n",
    "            ax.axvspan(b_xmin, b_xmax, alpha=0.3, color='red', label='log x-scale')\n",
    "            ax.axvline(0, ymin=0, ymax=1, color='grey', linestyle='dashed', alpha=0.5)\n",
    "            ax.set_xticks(ticks)\n",
    "            ax.xaxis.set_major_formatter(FormatStrFormatter(str_format))\n",
    "            # minor ticks/gridlines: use LogLocator for positive and negative ranges\n",
    "            ax.xaxis.set_minor_locator(FixedLocator(minor_positions))\n",
    "            ax.xaxis.set_minor_formatter(NullFormatter())   # suppress labels\n",
    "            ax.tick_params(axis='x', which='minor', length=3)\n",
    "\n",
    "            e_label = 'timeEnsemble' if ensemble.startswith('time') else 'distEnsemble'\n",
    "            ax.set_title(f'{tree} dist., {e_label} avg.', fontsize=10)\n",
    "\n",
    "            # plot the Log normal model result distribution\n",
    "            pv = fdc_df[fdc_df['Label'] == 'PredictedLog'][metric].values\n",
    "            x_ln, y_ln = compute_empirical_cdf(pv)\n",
    "            ax.plot(x_ln, y_ln, color='black', linestyle='solid', linewidth=2, label='LogNorm')\n",
    "\n",
    "            for i in range(1, 11):\n",
    "                vals = df[df['k'] == i][metric].values\n",
    "                assert len(vals) == len(completed_stns), f'   len(vals)={len(vals)} vs. {len(completed_stns)} for {i}NN {tree} {ensemble} {metric}'\n",
    "                x, y = compute_empirical_cdf(vals)\n",
    "                ax.plot(x, y, label=f'{i}NN', color=clrs[i - 1], lw=2)            \n",
    "            \n",
    "            ax.set_ylabel(r'$P(X \\geq x)$', fontsize=12)\n",
    "            ax.set_xlabel(metric_label_dict[metric][1:-1], fontsize=12)\n",
    "            if n == 3:\n",
    "                ax.legend(loc='lower right', framealpha=0.6, fontsize=7)\n",
    "            else:\n",
    "                ax.legend(loc='lower right', framealpha=0.6).set_visible(False)\n",
    "\n",
    "    # Remove unused subplots\n",
    "    for j in range(plot_idx, len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if savepath:\n",
    "        fig.savefig(savepath, dpi=150)\n",
    "        print(f'Saved figure to {savepath}')\n",
    "    return fig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04252166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_log_knn_plots_matplotlib_grid(\n",
    "        results_dfs, completed_stns, all_metrics,\n",
    "        metric_label_dict, ncols=3, savepath=None):\n",
    "\n",
    "    knn_df = results_dfs.copy()\n",
    "\n",
    "    combos = [\n",
    "        ('ID2', 'timeEnsemble', 'spatial'),\n",
    "        ('ID2', 'freqEnsemble', 'spatial'),\n",
    "        ('ID2', 'freqEnsemble', 'attribute'),\n",
    "    ]\n",
    "\n",
    "    clrs = Sunset10\n",
    "    ncols = len(combos)\n",
    "\n",
    "    total = len(all_metrics) * len(combos)\n",
    "    nrows = (total + ncols - 1) // ncols\n",
    "    fig, axs = plt.subplots(nrows, ncols, figsize=(ncols * 5, nrows * 4))\n",
    "    axs = axs.flatten()\n",
    "    plot_idx = 0\n",
    "\n",
    "    for metric in all_metrics:\n",
    "        if metric == 'PB':\n",
    "            continue\n",
    "\n",
    "        for wm, ensemble, tree in combos:\n",
    "\n",
    "            df = knn_df.query(\n",
    "                \"ensemble_weight == @wm and ensemble_method == @ensemble and tree_type == @tree\"\n",
    "            )\n",
    "            if df.empty:\n",
    "                continue\n",
    "\n",
    "            ax = axs[plot_idx]\n",
    "            plot_idx += 1\n",
    "\n",
    "            max_val = plot_parametric_bounds_matplotlib(ax, metric, fdc_df)\n",
    "\n",
    "            # ---- SIMPLE LOG-X AXIS CONFIG ----\n",
    "            if metric in ['PB', 'NAE']:\n",
    "                xmin, xmax = 1, 1e3\n",
    "                ticks = [1, 10, 100, 1000]\n",
    "            elif metric in ['NSE', 'KLD']:\n",
    "                xmin, xmax = 1e-3, 10\n",
    "                ticks = [1e-3, 1e-2, 1e-1, 1, 10]\n",
    "                # make ticks just 10^\n",
    "            else:\n",
    "                xmin, xmax = 1, 1e3\n",
    "                ticks = [1, 10, 100, 1000]\n",
    "            fmt = LogFormatterMathtext()\n",
    "\n",
    "            # log x-axis\n",
    "            ax.set_xscale('log')\n",
    "            ax.set_xlim(xmin, xmax)\n",
    "            ax.set_ylim(-0.01, 1.01)\n",
    "            ax.set_xticks(ticks)\n",
    "            ax.xaxis.set_major_formatter(fmt)\n",
    "            # add vertical grid lines (light grey) at tick positions\n",
    "            # for t in ticks:\n",
    "            #     ax.axvline(t, color='black', alpha=0.7)\n",
    "\n",
    "            # find minor tick positions between major ticks\n",
    "            ax.grid(which='major', axis='x', color='black', alpha=0.7, linewidth=0.6)\n",
    "            ax.grid(which='minor', axis='x', color='grey', alpha=0.6, linewidth=0.4)\n",
    "            ax.yaxis.set_minor_locator(AutoMinorLocator())\n",
    "\n",
    "            # title\n",
    "            e_label = (\n",
    "                'timeEnsemble' if ensemble.startswith('time')\n",
    "                else 'distEnsemble'\n",
    "            )\n",
    "            ax.set_title(f'{tree} dist., {e_label} avg.', fontsize=10)\n",
    "\n",
    "            # ---- Log-normal parametric CDF ----\n",
    "            pv = fdc_df[fdc_df['Label'] == 'PredictedLog'][metric].values\n",
    "            x_ln, y_ln = compute_empirical_cdf(pv)\n",
    "            ax.plot(x_ln, y_ln, color='black', linestyle='solid', linewidth=2,\n",
    "                    label='LogNorm')\n",
    "\n",
    "            # ---- kNN curves ----\n",
    "            for k in range(1, 11):\n",
    "                vals = df[df['k'] == k][metric].values\n",
    "                assert len(vals) == len(completed_stns), \\\n",
    "                    f'len(vals)={len(vals)} vs {len(completed_stns)} for {k}NN {tree} {ensemble} {metric}'\n",
    "\n",
    "                x, y = compute_empirical_cdf(vals)\n",
    "                ax.plot(x, y, label=f'{k}NN', color=clrs[k - 1], lw=2)\n",
    "\n",
    "            # labels\n",
    "            ax.set_ylabel(\"P(X ≥ x)\", fontsize=12)\n",
    "            ax.set_xlabel(metric_label_dict[metric][1:-1], fontsize=12)\n",
    "\n",
    "            # legend only on last panel of the row\n",
    "            if (plot_idx % ncols) == 0:\n",
    "                ax.legend(loc='upper left', framealpha=0.6, fontsize=7)\n",
    "            else:\n",
    "                ax.legend().set_visible(False)\n",
    "\n",
    "    # remove leftover axes\n",
    "    for j in range(plot_idx, len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if savepath:\n",
    "        fig.savefig(savepath, dpi=150)\n",
    "        print(f'Saved figure to {savepath}')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df53267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_axis_labels = [\n",
    "    r'$$\\text{NAE} \\text{ [\\%]}$$',\n",
    "    r'$$\\text{RMSE } [\\%]$$',\n",
    "    r'$$1 - \\text{NSE} \\text{ [-]}$$',\n",
    "    r'$$\\text{KLD} \\text{ [bits/sample]}$$',\n",
    "]\n",
    "metric_label_dict = dict(zip(plotting_metrics, plotting_axis_labels))\n",
    "knn_fig_path = f'images/knn_plot_subset_{bitrate:02d}_bits.png'\n",
    "if regularization_type == 'kde':\n",
    "    knn_fig_path = f'images/knn_plot_subset_kde.png'\n",
    "plots = create_log_knn_plots_matplotlib_grid(knn_df, common_stations, plotting_metrics, metric_label_dict, ncols=3, savepath=knn_fig_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7055a975",
   "metadata": {},
   "source": [
    "## Plot CDFs of error metrics for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929e6e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models.formatters import CustomJSTickFormatter\n",
    "# create labels for symmetric log axes\n",
    "symlog_formatter = CustomJSTickFormatter(code=\"\"\"\n",
    "    const lin_thresh = 1.0;\n",
    "    const abs_tick = Math.abs(tick);\n",
    "    let original;\n",
    "\n",
    "    if (abs_tick <= lin_thresh) {\n",
    "        original = tick;\n",
    "    } else {\n",
    "        original = Math.sign(tick) * Math.exp(abs_tick - Math.log(lin_thresh));\n",
    "    }\n",
    "\n",
    "    const val = original * 100;\n",
    "    const abs_val = Math.abs(val);\n",
    "    const rounded = Math.round(abs_val);\n",
    "\n",
    "    if (rounded === 0) {\n",
    "        return \"0%\";\n",
    "    }\n",
    "\n",
    "    return (val < 0 ? \"-\" : \"\") + rounded.toString() + \"%\";\n",
    "\"\"\")\n",
    "\n",
    "plots = []\n",
    "\n",
    "main_result_vals = {}\n",
    "for dm, x_label in zip(all_metrics, axis_labels):\n",
    "    for model_key, line_style, color, label in model_specs:\n",
    "        data, ids = load_model_data(label, model_key, dm, regularization_type=regularization_type)\n",
    "        main_result_vals[f\"{label} {dm}\"] = pd.DataFrame({'ids': ids, 'values': data})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3123382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the 4NN results\n",
    "print(all_metrics)\n",
    "for dm in all_metrics:\n",
    "    for label in ['4-NN']:\n",
    "        data, ids = load_model_data(label, label, dm, regularization_type=regularization_type)\n",
    "        main_result_vals[f\"{label} {dm}\"] = pd.DataFrame({'ids': ids, 'values': data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf95582c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the baseline fit based on FFT kernel density estimation\n",
    "# fft_kde_df = pd.read_csv(f'data/results/additional_results/kde_fft_comparison_10_bits.csv', dtype={'station_id': str}, index_col='station_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208ce6b1-1546-4e4b-b056-1baabfcbc09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_performance_plots_matplotlib(all_metrics, axis_labels, model_specs, fdc_df,\n",
    "                                               main_result_vals, idx, ncols=3,):\n",
    "    nrows = int(np.ceil(len(all_metrics) / ncols))\n",
    "    fig, axs = plt.subplots(nrows, ncols, figsize=(ncols * 5.5, nrows * 4.5))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i, (dm, x_label) in enumerate(zip(all_metrics, axis_labels)):\n",
    "\n",
    "        # Axis setup\n",
    "        ax = axs[i]\n",
    "        max_val = plot_parametric_bounds_matplotlib(ax, dm, fdc_df)\n",
    "        \n",
    "        if dm == 'PB':\n",
    "            linthresh = 100\n",
    "            xmin, xmax = -5, 500\n",
    "            b_xmin, b_xmax = 100, 1000\n",
    "            ticks = [0, 25, 50, 75, 100, 200, 500]\n",
    "            add_vertical_spans(ax, np.arange(ticks[0], ticks[-1], 25))\n",
    "            str_format = '%.0f'\n",
    "        elif dm == 'NSE':\n",
    "            linthresh = 1.0\n",
    "            xmin, xmax = -0.05, 5.0\n",
    "            b_xmin, b_xmax = linthresh, 10\n",
    "            ticks = [0, 0.25, 0.5, 0.75, 1.0, 2.0, 5.0]\n",
    "            add_vertical_spans(ax, np.arange(ticks[0], ticks[-1], 0.25))\n",
    "            str_format = '%.2f'\n",
    "        elif dm == 'KLD':\n",
    "            linthresh = 1.0\n",
    "            xmin, xmax = 0.0, 5\n",
    "            b_xmin, b_xmax = linthresh, 5\n",
    "            ticks = [0, 0.25, 0.5, 0.75, 1.0, 2.0, 5]\n",
    "            add_vertical_spans(ax, np.arange(ticks[0], ticks[-1], 0.25))\n",
    "            str_format = '%.2f'\n",
    "        elif dm == 'NAE':\n",
    "            linthresh = 100\n",
    "            xmin, xmax = -5, 505\n",
    "            b_xmin, b_xmax = linthresh, 500\n",
    "            ticks = [0, 25, 50, 75, 100, 200, 500]\n",
    "            add_vertical_spans(ax, np.arange(ticks[0], ticks[-1], 25))\n",
    "            str_format = '%.0f'\n",
    "        else:                \n",
    "            linthresh = 100.0\n",
    "            xmin, xmax = -1, 500\n",
    "            b_xmin, b_xmax = 100, 500\n",
    "            ticks = [0, 25, 50, 75, 100, 200, 500]\n",
    "            add_vertical_spans(ax, np.arange(ticks[0], ticks[-1], 25))\n",
    "            str_format = '%.0f'\n",
    "\n",
    "        minor_positions = ticks\n",
    "        ax.set_xscale('symlog', linthresh=linthresh)\n",
    "        ax.set_xlim(xmin, xmax)\n",
    "        ax.axvspan(b_xmin, b_xmax, alpha=0.3, color='red', label='log x-scale')\n",
    "        ax.axvline(0, ymin=0, ymax=1, color='grey', linestyle='dashed', alpha=0.5)\n",
    "        ax.set_xticks(ticks)\n",
    "        ax.xaxis.set_major_formatter(FormatStrFormatter(str_format))\n",
    "        # minor ticks/gridlines: use LogLocator for positive and negative ranges\n",
    "        ax.xaxis.set_minor_locator(FixedLocator(minor_positions))\n",
    "        ax.xaxis.set_minor_formatter(NullFormatter())   # suppress labels\n",
    "        ax.tick_params(axis='x', which='minor', length=3)\n",
    "\n",
    "        # Metric name mapping for ensemble CSVs\n",
    "        # metric_map = {'RB': 'pct_vol_bias', 'MB': 'mean_error', 'NAE': 've',\n",
    "        #               'RMSE': 'rmse', 'KLD': 'kld', 'NSE': 'nse'}\n",
    "\n",
    "        for model_key, line_style, color, label in model_specs[:idx]:\n",
    "            if model_key == 'MLE':\n",
    "                continue\n",
    "            data, ids = load_model_data(label, model_key, dm, regularization_type=regularization_type)\n",
    "            x, y = compute_empirical_cdf(data)\n",
    "            ax.plot(x, y, label=label, color=color, linestyle=line_style, linewidth=2)\n",
    "            main_result_vals[f\"{label} {dm}\"] = pd.DataFrame({'ids': ids, 'values': data})\n",
    "            \n",
    "        ax.set_xlabel(x_label[1:-1], fontsize=12)\n",
    "        ax.set_ylabel(r'$P(X \\geq x)$' if i % ncols == 0 else '', fontsize=12)\n",
    "        ax.tick_params(labelsize=12)\n",
    "        # ax.grid(True, which='both', linestyle=':', alpha=0.5)\n",
    "        if i == len(axis_labels) - 1:\n",
    "            ax.legend(loc='lower right', framealpha=0.6, fontsize=11)\n",
    "        else:\n",
    "            ax.legend(loc='lower right', framealpha=0.6).set_visible(False)\n",
    "\n",
    "    for j in range(len(all_metrics), len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253d9fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_log_model_performance_plots_matplotlib(all_metrics, axis_labels, model_specs, fdc_df,\n",
    "                                               main_result_vals, idx, ncols=3):\n",
    "    nrows = int(np.ceil(len(all_metrics) / ncols))\n",
    "    fig, axs = plt.subplots(nrows, ncols, figsize=(ncols * 5.5, nrows * 4.5))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i, (dm, x_label) in enumerate(zip(all_metrics, axis_labels)):\n",
    "        ax = axs[i]\n",
    "\n",
    "        # Plot parametric bounds (unchanged)\n",
    "        max_val = plot_parametric_bounds_matplotlib(ax, dm, fdc_df)\n",
    "\n",
    "        # --- PURE LOG AXIS CONFIGURATION ---\n",
    "        if dm == 'PB':\n",
    "            xmin, xmax = 1e-2, 5e2, 1000\n",
    "            ticks = [1e-2, 1e-1, 1, 10, 100, 1000]\n",
    "            fmt = '%.0f'\n",
    "        elif dm == 'NSE':\n",
    "            xmin, xmax = 1e-3, 10\n",
    "            ticks = [1e-3, 1e-2, 1e-1, 1, 10]\n",
    "            fmt = '%.2f'\n",
    "        elif dm == 'KLD':\n",
    "            xmin, xmax = 1e-3, 10\n",
    "            ticks = [1e-3, 1e-2, 1e-1, 1, 10]\n",
    "            fmt = '%.2f'\n",
    "        elif dm == 'NAE':\n",
    "            xmin, xmax = 1, 1e3\n",
    "            ticks = [1, 10, 100, 1000]\n",
    "            fmt = '%.0f'\n",
    "        else:\n",
    "            xmin, xmax = 1, 1e3\n",
    "            ticks = [1, 10, 100, 1000]\n",
    "            fmt = '%.0f'\n",
    "\n",
    "        fmt = LogFormatterMathtext()\n",
    "\n",
    "        ## --- APPLY PURE LOG SCALE ---\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_xlim(xmin, xmax)\n",
    "        ax.set_xticks(ticks)\n",
    "        ax.xaxis.set_major_formatter(fmt)\n",
    "\n",
    "        ax.grid(which='major', axis='x', color='black', alpha=0.7, linewidth=0.6)\n",
    "        ax.grid(which='minor', axis='x', color='grey', alpha=0.6, linewidth=0.4)\n",
    "        ax.yaxis.set_minor_locator(AutoMinorLocator())\n",
    "        ax.set_ylim(-0.01, 1.01)\n",
    "\n",
    "        # Optionally: faint vertical spans at tick marks (if you still want them)\n",
    "        # add_vertical_spans(ax, ticks)\n",
    "\n",
    "        # Plot model lines\n",
    "        for model_key, line_style, color, label in model_specs[:idx]:\n",
    "            if model_key == 'MLE':\n",
    "                continue\n",
    "            data, ids = load_model_data(label, model_key, dm, regularization_type=regularization_type)\n",
    "            x, y = compute_empirical_cdf(data)\n",
    "            ax.plot(x, y, label=label, color=color, linestyle=line_style, linewidth=2)\n",
    "            main_result_vals[f\"{label} {dm}\"] = pd.DataFrame({'ids': ids, 'values': data})\n",
    "\n",
    "        ax.set_xlabel(x_label[1:-1], fontsize=12)\n",
    "        ax.set_ylabel(r'$P(X \\geq x)$' if i % ncols == 0 else '', fontsize=12)\n",
    "        ax.tick_params(labelsize=12)\n",
    "\n",
    "        if i == len(axis_labels) - 1:\n",
    "            ax.legend(loc='upper left', framealpha=0.6, fontsize=11)\n",
    "        else:\n",
    "            ax.legend(loc='upper left', framealpha=0.6).set_visible(False)\n",
    "\n",
    "    # Remove unused axes\n",
    "    for j in range(len(all_metrics), len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73289db3-0980-4fc6-b18a-f6f75a76d612",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_idxs = [3, 5, 7, 10]\n",
    "\n",
    "f1 = f'parametric_results_{bitrate:02d}_bits.png'\n",
    "f2 = f'kNN_results_{bitrate:02d}_bits.png'\n",
    "f3 = f'LSTM_result_plots_{bitrate:02d}_bits.png'\n",
    "f4 = f'model_ensemble_results_{bitrate:02d}_bits.png'\n",
    "if regularization_type == 'kde':\n",
    "    f1 = f'parametric_results_kde.png'\n",
    "    f2 = f'kNN_results_kde.png'\n",
    "    f3 = f'LSTM_result_plots_kde.png'\n",
    "    f4 = f'model_ensemble_results_kde.png'\n",
    "    \n",
    "labels = [f1, f2, f3, f4]\n",
    "\n",
    "for idx, fname in zip(set_idxs, labels):\n",
    "    fig = create_log_model_performance_plots_matplotlib(\n",
    "        plotting_metrics, plotting_axis_labels, model_specs, \n",
    "        fdc_df, main_result_vals, idx, ncols=4,\n",
    "    )\n",
    "    fp = 'images/' + fname\n",
    "    fig.savefig(fp, dpi=150)\n",
    "    print(f'saved to {fp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadf71a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdc_panel_boxplot(all_metrics, axis_labels, model_specs, order_models):\n",
    "    \"\"\"Main results boxplot panel.\"\"\"\n",
    "    \n",
    "    n_metrics = len(all_metrics)\n",
    "    fig, axs = plt.subplots(1, n_metrics, figsize=(4.8 * n_metrics, 4.2), squeeze=False)\n",
    "    axs = axs.ravel()\n",
    "    set_ylabels = True\n",
    "    for ax, dm, x_label in zip(axs, all_metrics, axis_labels):\n",
    "\n",
    "        data_by_model = []\n",
    "        labels, clrs = [], []\n",
    "\n",
    "        for model_key, line_style, color, label in model_specs[::-1]:\n",
    "            \n",
    "            data, ids = load_model_data(label, model_key, dm, regularization_type=regularization_type)     \n",
    "\n",
    "            if dm == 'NSE':\n",
    "                # data = 1 - data   \n",
    "                x_label = r'$$1 - \\text{NSE} \\text{ [-]}$$'\n",
    "            elif dm == 'PB':\n",
    "                x_label = r'$$\\text{PB} \\text{ [\\%]}$$'\n",
    "  \n",
    "            # guard against empty arrays\n",
    "            data = np.asarray(data)\n",
    "            if data.size == 0 or np.all(~np.isfinite(data)):\n",
    "                continue\n",
    "\n",
    "            data_by_model.append(data[np.isfinite(data)])\n",
    "            labels.append(label)\n",
    "            clrs.append(color)\n",
    "\n",
    "        # ---- plot: horizontal boxplots per model ----\n",
    "        if not data_by_model:\n",
    "            ax.set_visible(False)\n",
    "            continue\n",
    "\n",
    "        bp = ax.boxplot(\n",
    "            data_by_model, vert=False, patch_artist=True, showfliers=False, widths=0.6,\n",
    "            showmeans=True, \n",
    "            boxprops=dict(linewidth=1, facecolor=\"#dddddd\", alpha=0.8),\n",
    "            meanprops=dict(marker='^', markeredgecolor='black', markerfacecolor='white', markersize=7),\n",
    "            medianprops=dict(color=\"k\", linewidth=1.2),\n",
    "            whiskerprops=dict(color=\"0.3\", linewidth=1),\n",
    "            capprops=dict(color=\"0.3\", linewidth=1)\n",
    "        )\n",
    "\n",
    "        # jittered points (optional; comment out if you don’t want them)\n",
    "        rng = np.random.default_rng(0)\n",
    "        for yi, vals in enumerate(data_by_model, start=1):\n",
    "            jitter = (rng.random(vals.size) - 0.5) * 0.5\n",
    "            c = clrs[yi - 1]\n",
    "            ax.scatter(vals, yi + jitter, s=8, alpha=0.35, color=c, edgecolors=\"none\")\n",
    "\n",
    "        ax.set_yticklabels([''] * len(labels))  # Hide y-tick labels initially\n",
    "        if set_ylabels:\n",
    "            ax.set_yticks(np.arange(1, len(labels) + 1))\n",
    "            ax.set_yticklabels(labels)\n",
    "            set_ylabels = False\n",
    "\n",
    "        ax.axvline(0, color=\"0.8\", lw=1)\n",
    "        ax.grid(axis=\"x\", color=\"0.9\", lw=0.8)\n",
    "        ax.xaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "        if dm  == 'NSE':\n",
    "            ax.set_xlim(-0.01, 1)\n",
    "            # ax.axvline(0, color=\"green\", linestyle=\"--\", linewidth=2)\n",
    "        if dm in ['KLD']:\n",
    "            # ax.set_xscale('log')\n",
    "            # ax.set_xlim(1e-2, 1e1)\n",
    "            ax.set_xlim(0, 1.75)\n",
    "        elif dm == 'PB':\n",
    "            ax.set_xlim(-100, 100)\n",
    "            # ax.set_xlim(-1, 200)\n",
    "            ax.axvline(0, color=\"green\", linestyle=\"--\", linewidth=2)\n",
    "            ax.xaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n",
    "        elif dm == 'VE':\n",
    "            ax.set_xlim(-0.01, 1.0)\n",
    "            # ax.axvline(0, color=\"green\", linestyle=\"--\", linewidth=2)\n",
    "            # ax.axvline(1, color=\"red\", linestyle=\"--\", linewidth=1.2)\n",
    "            # create a light red shaded region for VE < 0%\n",
    "            ax.axvspan(1, 1.05, color=\"red\", alpha=0.2)\n",
    "        elif dm == 'RMSE':\n",
    "            ax.set_xlim(-1, 200)\n",
    "            # ax.axvline(0, color=\"green\", linestyle=\"--\", linewidth=2)\n",
    "            # ax.axvline(100, color=\"red\", linestyle=\"--\", linewidth=1.2)\n",
    "            # create a light red shaded region for RMSE > 100%\n",
    "            ax.xaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n",
    "            # ax.axvspan(100, 200, color=\"red\", alpha=0.2)\n",
    "            # ax.set_xscale('log')\n",
    "        elif dm == 'NAE':\n",
    "            ax.set_xlim(-1, 105)\n",
    "            # ax.axvline(0, color=\"green\", linestyle=\"--\", linewidth=2)\n",
    "            # ax.axvline(100, color=\"red\", linestyle=\"--\", linewidth=1.2)\n",
    "            # create a light red shaded region for NAE > 100%\n",
    "            ax.xaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n",
    "            # ax.axvspan(100, 200, color=\"red\", alpha=0.2)\n",
    "        \n",
    "        if dm == 'RB':\n",
    "            dm = 'PB'\n",
    "        elif dm == 'NSE':\n",
    "            dm = '1 - NSE'\n",
    "        elif dm == 'VE':\n",
    "            dm = \"Normalized Absolute Error\"\n",
    "        ax.set_title(dm, fontsize=14)\n",
    "        x_label = x_label[1:-1] \n",
    "        ax.set_xlabel(x_label)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a5666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_labels = {\"NSE\": \"NSE\", \"PB\": \"PB\", \"RMSE\": \"RMSE\", \"KLD\": \"KLD\", \"NAE\": \"NAE\"}\n",
    "xlims = {\"NSE\": (-10, 1), \"PB\": (-100, 100), \"RMSE\": (0, 500), \"KLD\": (0, 5), 'NAE': (0, 100)}\n",
    "order_models = ['MLE', 'MoM LogNorm', 'LogNorm', '2-NN', '8-NN', 'LSTM Time', 'LSTM Dist.', 'LSTM-4NN', 'LSTM-LN-4NN.']\n",
    "fig = fdc_panel_boxplot(\n",
    "    [\"PB\", \"NAE\", \"RMSE\",\"NSE\",\"KLD\"], axis_labels, model_specs, order_models, \n",
    ")\n",
    "main_image_fname = f\"images/main_result_fig_{bitrate:02d}_bits.png\"\n",
    "if regularization_type == 'kde':\n",
    "    main_image_fname = f\"images/main_result_fig_kde.png\"\n",
    "# plt.savefig(main_image_fname, dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915df3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the results by bitrate \n",
    "# and plot regressions\n",
    "plots = []\n",
    "formatted_folder = f'data/results/additional_results'\n",
    "model_labels = ['MLE', 'Mean_PMF', 'PredictedLog', 'PredictedMOM', 'RandomDraw', 'Uniform', 'frequency', 'time']\n",
    "metrics = ['PB', 'NAE', 'RMSE', 'NSE', 'KLD']\n",
    "for ml in model_labels:\n",
    "    \n",
    "    data_dfs = []\n",
    "\n",
    "    for b in [5, 8]:\n",
    "        formatted_result_file = f'/formatted_results_by_performance_measure_{b:02d}_bits.csv'\n",
    "        form_df = pd.read_csv(formatted_folder + formatted_result_file, index_col='Official_ID', dtype={'Official_ID': str})\n",
    "        form_df = form_df[['Label'] + metrics]\n",
    "        form_df = form_df[form_df['Label'] == ml].copy()\n",
    "        form_df.columns = ['Label'] + [f'{col}_{b}' for col in form_df.columns if col != 'Label']\n",
    "        data_dfs.append(form_df)\n",
    "    \n",
    "    # load the kde results\n",
    "    formatted_result_file = f'/formatted_results_by_performance_measure_kde.csv'\n",
    "    form_df = pd.read_csv(formatted_folder + formatted_result_file, index_col='Official_ID', dtype={'Official_ID': str})\n",
    "    form_df = form_df[['Label'] + metrics]\n",
    "    form_df = form_df[form_df['Label'] == ml].copy()\n",
    "    form_df.columns = ['Label'] + [f'{col}_kde' for col in form_df.columns if col != 'Label']\n",
    "    data_dfs.append(form_df)\n",
    "\n",
    "    data = pd.concat(data_dfs, axis=1)\n",
    "    for m in ['PB', 'NAE', 'RMSE', 'NSE', 'KLD']:\n",
    "        # x_type, y_type = 'log', 'log'# if m in ['RB', 'NAE', 'RMSE', 'NAW'] else 'linear', 'log' if m in ['RB', 'NAE', 'RMSE'] else 'linear'\n",
    "        bx, by = 'kde', 8\n",
    "        title = f'{ml} {m} {bx} vs {by} bits'\n",
    "        if ml == 'frequency':\n",
    "            title = f'LSTM avg. density {bx} vs {by} bits'\n",
    "        elif ml == 'time':\n",
    "            title = f'LSTM avg. time {bx} vs {by} bits'\n",
    "        p = figure(title=title, width=600, height=350)        \n",
    "\n",
    "        x, y = data[f'{m}_{bx}'].values, data[f'{m}_{by}'].values\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(x, y)        \n",
    "        \n",
    "        p.scatter(x, y, size=5, color='navy', alpha=0.5)\n",
    "        x_fit = np.linspace(x.min(), x.max(), 100)\n",
    "        y_fit = slope * x_fit + intercept\n",
    "        p.line(x_fit, y_fit, color='firebrick', line_width=2, legend_label=f'Y={slope:.2f}x + {intercept:.2f} (R²={r_value**2:.2f})')\n",
    "        # plot 1:1 line\n",
    "        x1 = np.linspace(min(x.min(), y.min()), max(x.max(), y.max()), 100)\n",
    "        p.line(x1, x1, color='grey', line_width=3, line_dash='dotted', legend_label='1:1 Line')\n",
    "        p.xaxis.axis_label = f'{m} ({bx} bit PMF)'\n",
    "        p.yaxis.axis_label = f'{m} ({by} bit PMF)'\n",
    "        p.legend.location = 'top_left'\n",
    "        p.legend.background_fill_alpha = 0.4\n",
    "        p = dpf.format_fig_fonts(p,font_size=11)\n",
    "        plots.append(p)\n",
    "\n",
    "lt = gridplot(plots, ncols=5, width=325, height=300)\n",
    "\n",
    "# show(lt)\n",
    "# save html file from bokeh\n",
    "performance_regression_file = f\"images/metric_sensitivity_to_bitrate_{bx}_vs_{by}_bits.html\"\n",
    "# output_file(performance_regression_file)\n",
    "# save(lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed5bf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the results by bitrate \n",
    "# and plot regressions\n",
    "plots = []\n",
    "metrics = ['PB', 'NAE', 'RMSE', 'NSE', 'KLD']\n",
    "tt = 'attribute'\n",
    "em = 'freqEnsemble'\n",
    "ew = 'ID1'\n",
    "\n",
    "data_dfs = []\n",
    "for k in [2, 4, 6, 8, 10]:\n",
    "    for b in [6, 8, 10]:\n",
    "        formatted_folder = f'data/results/additional_results'\n",
    "        # get the knn results also\n",
    "        knn_fpath = formatted_folder + f'/knn_all_results_formatted_{b:02d}_bits.csv'\n",
    "        if regularization_type == 'kde':\n",
    "            knn_fpath = formatted_folder + f'/knn_all_results_formatted_kde.csv'\n",
    "        knn_df = pd.read_csv(knn_fpath, index_col='Official_ID', dtype={'Official_ID': str})\n",
    "        knn_df = knn_df[(knn_df['tree_type'] == tt) & (knn_df['ensemble_method'] == em) & (knn_df['ensemble_weight'] == ew) & (knn_df['k'] == k)]\n",
    "        assert len(knn_df) == 712\n",
    "        knn_df = knn_df[metrics]\n",
    "        knn_df.columns = [f'{col}_{b}_{k}NN' for col in knn_df.columns if col != 'Label']\n",
    "        data_dfs.append(knn_df)\n",
    "\n",
    "    data = pd.concat(data_dfs, axis=1)\n",
    "    for m in ['PB', 'NAE', 'RMSE', 'NSE', 'KLD']:\n",
    "        # x_type, y_type = 'log', 'log'# if m in ['RB', 'NAE', 'RMSE', 'NAW'] else 'linear', 'log' if m in ['RB', 'NAE', 'RMSE'] else 'linear'\n",
    "        bx, by = 10, 6\n",
    "        title = f'{k}NN {m} {bx} vs {by} bits'\n",
    "        if ml == 'frequency':\n",
    "            title = f'{k}NN LSTM avg. density {bx} vs {by} bits'\n",
    "        elif ml == 'time':\n",
    "            title = f'{k}NN LSTM avg. time {bx} vs {by} bits'\n",
    "        p = figure(title=title, width=600, height=350)\n",
    "\n",
    "        x, y = data[f'{m}_{bx}_{k}NN'].values, data[f'{m}_{by}_{k}NN'].values\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "        \n",
    "        \n",
    "        p.scatter(x, y, size=5, color='navy', alpha=0.5)\n",
    "        x_fit = np.linspace(x.min(), x.max(), 100)\n",
    "        y_fit = slope * x_fit + intercept\n",
    "        p.line(x_fit, y_fit, color='firebrick', line_width=2, legend_label=f'Y={slope:.2f}x + {intercept:.2f} (R²={r_value**2:.2f})')\n",
    "        # plot 1:1 line\n",
    "        x1 = np.linspace(min(x.min(), y.min()), max(x.max(), y.max()), 100)\n",
    "        p.line(x1, x1, color='grey', line_width=3, line_dash='dotted', legend_label='1:1 Line')\n",
    "        p.xaxis.axis_label = f'{m} ({bx} bit PMF)'\n",
    "        p.yaxis.axis_label = f'{m} ({by} bit PMF)'\n",
    "        p.legend.location = 'top_left'\n",
    "        p.legend.background_fill_alpha = 0.4\n",
    "        p = dpf.format_fig_fonts(p,font_size=11)\n",
    "        plots.append(p)\n",
    "\n",
    "lt = gridplot(plots, ncols=5, width=325, height=300)\n",
    "from bokeh.io import output_file, save\n",
    "# output_file(f\"images/kNN_metric_sensitivity_to_bitrateee_{bx}_vs_{by}_bits.html\")\n",
    "# save(lt)\n",
    "# show(lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080f1efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with all the model results indexed by station\n",
    "all_results = []\n",
    "for m in main_result_vals.keys():\n",
    "    df = main_result_vals[m].copy()\n",
    "    df.rename(columns={'values': m}, inplace=True)\n",
    "    if np.any(['-' in e for e in df['ids'].values]):\n",
    "        df['ids'] = df['ids'].str.split('-').str[0]\n",
    "    df.set_index('ids', inplace=True)\n",
    "    \n",
    "    all_results.append(df)\n",
    "all_results_df = pd.concat(all_results, axis=1, join='inner')\n",
    "print(len(all_results_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2762f822-3805-4221-8c4c-289bb0dce285",
   "metadata": {},
   "source": [
    "### Model variant regressions\n",
    "\n",
    "Compare how the measures correlate within methods:\n",
    "\n",
    "* what does the temporal vs. distribution averaging do for each metric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c965b4-32b7-4eae-a742-4be3ad2c5781",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_labels = [c for c in main_result_vals.keys() if (('LSTM' in c) & ('NN' in c)) | ('LN' in c)]\n",
    "print(combined_labels)\n",
    "LN_method_labels = [c for c in main_result_vals.keys() if ('LogNorm' in c) & (c not in combined_labels)]\n",
    "print(LN_method_labels)\n",
    "kNN_method_labels = [c for c in main_result_vals.keys() if ('NN' in c) & (c not in combined_labels)]\n",
    "print(kNN_method_labels)\n",
    "LSTM_method_labels = [c for c in main_result_vals.keys() if ('LSTM' in c) & (c not in combined_labels)]\n",
    "print(LSTM_method_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c418592f-07d3-4dab-a287-9b7c0e4a3d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_l1_slope_test(x, y, n_bootstraps=1000, alpha=0.05, delta=0.1, random_state=42):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    n = len(x)\n",
    "    \n",
    "    # Pre-generate all bootstrap indices (n_bootstraps x n)\n",
    "    idxs = rng.integers(0, n, size=(n_bootstraps, n))\n",
    "    \n",
    "    # Pre-allocate array for slopes\n",
    "    slopes = np.empty(n_bootstraps)\n",
    "    \n",
    "    # Loop over bootstrap samples\n",
    "    x_reshaped = x.reshape(-1, 1)\n",
    "    for i in range(n_bootstraps):\n",
    "        xi = x_reshaped[idxs[i]]\n",
    "        yi = y[idxs[i]]\n",
    "        slopes[i] = l1_fit(xi.ravel(), yi)[0]\n",
    "\n",
    "    # Confidence interval\n",
    "    lower = np.percentile(slopes, 100 * alpha / 2)\n",
    "    upper = np.percentile(slopes, 100 * (1 - alpha / 2))\n",
    "    mean_slope = np.mean(slopes)\n",
    "\n",
    "    # Check for meaningful difference\n",
    "    is_meaningfully_different = np.abs(mean_slope - 1) > delta\n",
    "\n",
    "    return {\n",
    "        'mean_slope': float(mean_slope),\n",
    "        'ci': (float(lower), float(upper)),\n",
    "        'is_meaningfully_different': is_meaningfully_different,\n",
    "        'bootstrapped_slopes': slopes\n",
    "    }\n",
    "    \n",
    "\n",
    "def bootstrap_l2_slope_test(x, y, n_bootstraps=1000, alpha=0.05, delta=0.1, seed=None):\n",
    "    \"\"\"\n",
    "    Perform bootstrap test on OLS slope to assess if it is meaningfully different from 1.\n",
    "\n",
    "    Parameters:\n",
    "    - x, y: 1D arrays of input data\n",
    "    - n_bootstraps: number of bootstrap samples\n",
    "    - alpha: significance level (e.g., 0.05 for 95% CI)\n",
    "    - meaningful_diff: threshold for deviation from 1 to be considered meaningful\n",
    "    - seed: optional random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    - dict with:\n",
    "        - mean_slope: mean of bootstrap slopes\n",
    "        - ci: confidence interval (lower, upper)\n",
    "        - is_meaningfully_different: whether slope differs from 1 by more than `meaningful_diff`\n",
    "        - bootstrapped_slopes: full array of bootstrapped slope samples\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    x = np.asarray(x).reshape(-1, 1)\n",
    "    y = np.asarray(y)\n",
    "    n = len(x)\n",
    "\n",
    "    slopes = np.empty(n_bootstraps)\n",
    "    for i in range(n_bootstraps):\n",
    "        idx = np.random.randint(0, n, n)\n",
    "        xi, yi = x[idx], y[idx]\n",
    "        slope, intercept, r, p, se = linregress(xi.ravel(), yi)\n",
    "        slopes[i] = slope\n",
    "\n",
    "    mean_slope = np.mean(slopes)\n",
    "    lower, upper = np.percentile(slopes, [100 * alpha / 2, 100 * (1 - alpha / 2)])\n",
    "    is_meaningfully_different = (upper < 1 - delta) or (lower > 1 + delta)\n",
    "\n",
    "    return {\n",
    "        'mean_slope': float(mean_slope),\n",
    "        'ci': (float(lower), float(upper)),\n",
    "        'is_meaningfully_different': is_meaningfully_different,\n",
    "        'bootstrapped_slopes': slopes,\n",
    "    }\n",
    "    \n",
    "\n",
    "def l1_fit(x, y):\n",
    "    model = QuantileRegressor(quantile=0.5, alpha=0).fit(x.reshape(-1, 1), y)\n",
    "    slope = model.coef_[0]\n",
    "    intercept = model.intercept_\n",
    "    return slope, intercept\n",
    "\n",
    "\n",
    "def symlog_transform_with_linear_region(v, lin_thresh=1):\n",
    "    \"\"\"Manual symlog transform: linear near 0, log beyond.\"\"\"\n",
    "    return np.where(np.abs(v) <= lin_thresh,\n",
    "                    v,\n",
    "                    np.sign(v) * (np.log(np.abs(v)) + np.log(lin_thresh)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac708da-9ec2-4c3c-a028-ef60fce27949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_label(label, m):\n",
    "    # if 'LSTM' in label:\n",
    "    return label.replace('Dist', 'dist. avg.').replace('Time', 'time avg.').split(m)[0]\n",
    "\n",
    "\n",
    "def plot_all_matplotlib(metrics, dataframes, units, n_rows=3, n_cols=4, font_size=12):\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 4, n_rows * 3))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    def plot_loglog(ax, x, y, dm):\n",
    "        # mask = (x > 1e-3) & (y > 1e-3)\n",
    "        # x, y = x[mask], y[mask]\n",
    "        slope, intercept, r, *_ = linregress(np.log(x), np.log(y))\n",
    "        bootstrap_slope_results = bootstrap_l2_slope_test(np.log(x), np.log(y))\n",
    "        for k, v in bootstrap_slope_results.items():\n",
    "            if k == 'bootstrapped_slopes':\n",
    "                continue\n",
    "            else:\n",
    "                # print(k, v)\n",
    "                pass\n",
    "        xx = np.linspace(np.min(x), np.max(x), 100)\n",
    "        yy = np.exp(slope * np.log(xx) + intercept)\n",
    "        ci_low, ci_high = bootstrap_slope_results['ci']\n",
    "        yy_lower = np.exp(ci_low * np.log(xx) + intercept)\n",
    "        yy_upper = np.exp(ci_high * np.log(xx) + intercept)\n",
    "        \n",
    "        ax.plot([x.min(), x.max()], [x.min(), x.max()], linestyle='dotted', color='black', linewidth=2.5, label='1:1')\n",
    "        ax.scatter(x, y, s=5, alpha=0.6)\n",
    "        band_label = f\"95% CI m=[{ci_low:.2f}, {ci_high:.2f}]\"\n",
    "        ax.fill_between(xx, yy_lower, yy_upper, color='red', alpha=0.3, label=band_label)\n",
    "        label = f'Y={slope:.2f}x + {intercept:.1f}'\n",
    "        label = f'Y = exp({intercept:.2f})·X^{slope:.2f}'\n",
    "        ax.plot(xx, yy, color='red', label=label, linewidth=2.5)\n",
    "        ax.set_title(f'{dm}', fontsize=12)\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_yscale('log')\n",
    "        \n",
    "    def plot_symlog(ax, x, y, lin_thresh=1.):\n",
    "        \n",
    "        nan_x = np.sum(np.isnan(x))\n",
    "        nan_y = np.sum(np.isnan(y))\n",
    "        assert nan_x == 0, f'{nan_x} nan x values'\n",
    "        assert nan_y == 0, f'{nan_y} nan y values'\n",
    "        \n",
    "        log_x = np.sign(x) * np.log(np.abs(x))\n",
    "        log_y = np.sign(y) * np.log(np.abs(y))\n",
    "        bootstrap_slope_results = bootstrap_l1_slope_test(log_x, log_y)\n",
    "        bs_slope = bootstrap_slope_results['mean_slope']\n",
    "        ci_low, ci_high = bootstrap_slope_results['ci']\n",
    "                \n",
    "        slope, intercept = l1_fit(log_x, log_y)\n",
    "        # Sample more points around linear region\n",
    "        x_lin = np.linspace(-lin_thresh, lin_thresh, 100)\n",
    "        x_left = np.linspace(round(np.min(x), 1), -lin_thresh, 50, endpoint=False)\n",
    "        x_right = np.linspace(lin_thresh, np.max(x), 50)\n",
    "\n",
    "        xx = np.concatenate([x_left, x_lin, x_right])\n",
    "        yy = np.exp(slope * xx + intercept)\n",
    "        ax.add_patch(Rectangle((-lin_thresh, -lin_thresh), 2*lin_thresh, 2*lin_thresh,\n",
    "                               facecolor='grey', alpha=0.3, zorder=0, label='linear scale'))  \n",
    "        ax.plot([x.min(), x.max()], [x.min(), x.max()], linestyle='dotted', color='black', linewidth=2.5, label='1:1')\n",
    "        ax.scatter(x, y, s=5, alpha=0.6) \n",
    "\n",
    "        ax.set_xscale('symlog', linthresh=lin_thresh)\n",
    "        ax.set_yscale('symlog', linthresh=lin_thresh)      \n",
    "\n",
    "    for ax, m, df, unit in zip(axes, metrics, dataframes, units):\n",
    "        x, y = df.iloc[:, 0].values, df.iloc[:, 1].values    \n",
    "        plot_loglog(ax, x, y, m)\n",
    "\n",
    "        # ax.set_title(('MAE' if m == 'RB' else m) + f' {unit}', fontsize=font_size)\n",
    "        ax.set_xlabel(clean_label(df.columns[0], m), fontsize=font_size)\n",
    "        ax.set_ylabel(clean_label(df.columns[1], m), fontsize=font_size)\n",
    "        ax.tick_params(labelsize=font_size - 2)\n",
    "        ax.legend(fontsize=font_size - 3, framealpha=0.4)\n",
    "        ax.grid(True, which='both', linestyle=':', alpha=0.5)\n",
    "\n",
    "    for i in range(len(metrics), len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e7f985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_matplotlib_linear(metrics, dataframes, units, n_rows=3, n_cols=4, font_size=12,\n",
    "                    n_boot=1000, seed=0):\n",
    "    \"\"\"\n",
    "    Simpler scatter + OLS line + bootstrap band in *linear* space.\n",
    "    - metrics:   list of metric names (used only for titles)\n",
    "    - dataframes:list of 2-col DataFrames [x_col, y_col]\n",
    "    - units:     list of unit strings for titles\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 4, n_rows * 3))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for ax, m, df, unit in zip(axes, metrics, dataframes, units):\n",
    "        # --- data ---\n",
    "        x = df.iloc[:, 0].to_numpy(dtype=float)\n",
    "        y = df.iloc[:, 1].to_numpy(dtype=float)\n",
    "        mask = np.isfinite(x) & np.isfinite(y)\n",
    "        x, y = x[mask], y[mask]\n",
    "\n",
    "        # --- OLS fit ---\n",
    "        slope, intercept, r, p, se = linregress(x, y)\n",
    "        xx = np.linspace(x.min(), x.max(), 200)\n",
    "        yy = slope * xx + intercept\n",
    "\n",
    "        # --- bootstrap band (resample pairs) ---\n",
    "        n = x.size\n",
    "        boot_yhat = np.empty((n_boot, xx.size))\n",
    "        for b in range(n_boot):\n",
    "            idx = rng.integers(0, n, n)\n",
    "            sb, tb = x[idx], y[idx]\n",
    "            s, c, *_ = linregress(sb, tb)\n",
    "            boot_yhat[b] = s * xx + c\n",
    "        lo, hi = np.percentile(boot_yhat, [2.5, 97.5], axis=0)\n",
    "\n",
    "        # --- plot ---\n",
    "        ax.scatter(x, y, s=6, alpha=0.6)\n",
    "        ax.plot(xx, yy, lw=2.5, label=f\"y = {slope:.3f}x + {intercept:.3f}\")\n",
    "        ax.fill_between(xx, lo, hi, alpha=0.25, label=\"95% bootstrap band\")\n",
    "\n",
    "        # --- cosmetics ---\n",
    "        title_metric = 'MAE' if m == 'RB' else m  # keep your existing alias if needed\n",
    "        ax.set_title(f\"{title_metric} {unit}\", fontsize=font_size)\n",
    "        ax.set_xlabel(df.columns[0], fontsize=font_size)\n",
    "        ax.set_ylabel(df.columns[1], fontsize=font_size)\n",
    "        ax.tick_params(labelsize=font_size - 2)\n",
    "        ax.legend(fontsize=font_size - 3, framealpha=0.4)\n",
    "        ax.grid(True, linestyle=\":\", alpha=0.5)\n",
    "\n",
    "    # drop any unused axes\n",
    "    for i in range(len(metrics), len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51779e4b-92d1-42c1-b3c8-6293e7441885",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dfs = []\n",
    "units = ['[%]', '[L/s/km²]', '[-]', '[bits/sample]']\n",
    "units = ['[%]', '[%]', '[-]', '[bits/sample]']\n",
    "for m, unit in zip(plotting_metrics, units):\n",
    "    cols = list(sorted([c for c in LN_method_labels if m in c]))\n",
    "    dfs = []\n",
    "    for c in cols:\n",
    "        data = main_result_vals[c].copy().set_index('ids')\n",
    "        data.columns = [c]\n",
    "        dfs.append(data)\n",
    "    df = pd.concat(dfs, join='inner',axis=1)\n",
    "    # if m == 'RB':\n",
    "    #     df = df.map(np.abs)\n",
    "    df.columns = cols\n",
    "    plot_dfs.append(df)\n",
    "\n",
    "# figs = plot_all_matplotlib(plotting_metrics, plot_dfs, units)\n",
    "# figs.savefig(\"images/LN_mom_vs_direct_plots.png\", dpi=150, bbox_inches='tight')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5476ca-a3d9-4018-8b4a-b9053a488ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dfs = []\n",
    "for m in all_metrics:\n",
    "    cols = list(sorted([c for c in kNN_method_labels if m in c]))\n",
    "    dfs = []\n",
    "    for c in cols:\n",
    "        data = main_result_vals[c].copy().set_index('ids')\n",
    "        print(f'{c}: mean {m}={np.mean(data):.2f}')\n",
    "        dfs.append(data)\n",
    "    df = pd.concat(dfs, join='inner',axis=1)\n",
    "    df.columns = cols\n",
    "    plot_dfs.append(df)\n",
    "# figs = plot_all_matplotlib(plotting_metrics, plot_dfs, units)\n",
    "# figs.savefig(\"images/knn_8_vs_2.png\", dpi=150, bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c2be6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dfs = []\n",
    "for m in all_metrics:\n",
    "    # cols = list(sorted([c for c in LSTM_method_labels if m in c]))\n",
    "    # cols = [c for c in cols if 'knn_lstm' not in c]\n",
    "    cols = list(sorted([c for c in main_result_vals.keys() if c.startswith('LSTM')]))\n",
    "    cols = [c for c in cols if c.endswith(m)]\n",
    "    dfs = []\n",
    "    for c in cols:\n",
    "        data = main_result_vals[c].copy().set_index('ids')\n",
    "        # if '-' in any of the id strings, split to keep the id which is the first index\n",
    "        if np.any('-' in s for s in data.index):\n",
    "            data.index = data.index.str.split('-').str[0]\n",
    "        dfs.append(data)\n",
    "    df = pd.concat(dfs, join='inner',axis=1)\n",
    "    df.columns = cols\n",
    "    plot_dfs.append(df)\n",
    "\n",
    "# figs = plot_all_matplotlib(plotting_metrics, plot_dfs, units)\n",
    "# figs.savefig('images/LSTM_time_vs_distribution_plots.png', dpi=150, bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f38878-e353-464b-9e10-15482458a753",
   "metadata": {},
   "source": [
    "## Rank Correlations\n",
    "\n",
    "One interesting characteristic of the results is that about 20% of the sites can't do better than 0.4 bits/sample entropy, and that the parametric estimation is the best approach for these.  One question we can ask about this 20% is whether it's the same sample across the disparate methods, and we can figure this out directly by checking the size of the common set of ids in the worst (highest KLD/EMD) 20%.  \n",
    "\n",
    "* are there sites that are difficult to predict, regardless of the method?\n",
    "* are there methods that work better on some sites compared to others?\n",
    "* are there metrics that work better on some sites compared to others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18126aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_scatter_plots(md, main_result_vals, plot_type='rank'):\n",
    "    model_set = [e for e in main_result_vals.keys() if e.endswith(f' {md}')]\n",
    "    model_set = [e for e in model_set if not e.startswith('LN MoM')]\n",
    "    model_set = [e for e in model_set if not e.startswith('PredictedLMomentsGEV')]\n",
    "    model_set = [e for e in model_set if 'time' not in e]\n",
    "    model_set = [e for e in model_set if \"MLE\" not in e]\n",
    "    # model_set = [e for e in model_set if '1950' not in e]\n",
    "    # get unordered pairs of models\n",
    "    from itertools import combinations\n",
    "    model_pairs = list(combinations(model_set, 2))\n",
    "    rank_scatter_plots = []\n",
    "    model_labels = []\n",
    "    for m1, m2 in model_pairs:\n",
    "        if 'kNN' in m1 and 'kNN' in m2:\n",
    "            continue\n",
    "        if '3' in m1 or '3' in m2:\n",
    "            continue\n",
    "        # get ranking of md values and make a scatter plot\n",
    "        df1 = main_result_vals[m1].copy()\n",
    "        df2 = main_result_vals[m2].copy()\n",
    "        if df1.empty or df2.empty:\n",
    "            continue\n",
    "        ascending = False if md in ['NSE', 'KGE'] else True\n",
    "        df1.sort_values('values', ascending=ascending, inplace=True)\n",
    "        df2.sort_values('values', ascending=ascending, inplace=True)\n",
    "        df1['rank'] = np.arange(len(df1)) + 1\n",
    "        df2['rank'] = np.arange(len(df2)) + 1\n",
    "        # merge the two dataframes on ids\n",
    "        merged = pd.merge(df1, df2, on='ids', suffixes=('_1', '_2'))\n",
    "        # create a scatter plot of the ranks\n",
    "        p = figure(title=f\"{md}\", width=350, height=300)\n",
    "        if plot_type == 'rank':\n",
    "            p.scatter(merged['rank_1'], merged['rank_2'], size=3)\n",
    "            slope, intercept, r_value, p_value, std_err = linregress(merged['rank_1'], merged['rank_2'])\n",
    "            xvals = [1, len(merged)]\n",
    "            yvals = [1, len(merged)]\n",
    "        else:\n",
    "            p.scatter(merged['values_1'], merged['values_2'], size=3)\n",
    "            slope, intercept, r_value, p_value, std_err = linregress(merged['values_1'], merged['values_2'])\n",
    "            xvals = [merged['values_1'].min(), merged['values_1'].max()]\n",
    "            yvals = [slope * x + intercept for x in xvals]\n",
    "\n",
    "        p.line(xvals, yvals, line_dash='dashed', color='black', line_width=2, legend_label='1:1')\n",
    "        p.line(xvals, yvals, line_color='red', line_width=2, legend_label=f'R²={r_value**2:.2f}')\n",
    "        def get_plot_axis_label(m):\n",
    "            if 'LN' in m:\n",
    "                return 'LogNorm Direct Rank'\n",
    "            elif 'LSTM' in m:\n",
    "                print(m)\n",
    "                return 'LSTM Rank'\n",
    "            elif '9 kNN' in m:\n",
    "                return '9-NN Rank'\n",
    "            elif '3 kNN' in m:\n",
    "                return '3-NN Rank'\n",
    "            else:\n",
    "                return m\n",
    "        \n",
    "        if plot_type == 'rank':\n",
    "            x_label, y_label = get_plot_axis_label(m1), get_plot_axis_label(m2)\n",
    "            p.xaxis.axis_label = x_label\n",
    "            p.yaxis.axis_label = y_label\n",
    "        else:\n",
    "            p.xaxis.axis_label = f'{m1[:-3]} {md}'\n",
    "            p.yaxis.axis_label = f'{m2[:-3]} {md}'\n",
    "        if m1 not in model_labels:\n",
    "            model_labels.append(m1)\n",
    "        if m2 not in model_labels:\n",
    "            model_labels.append(m2)\n",
    "        p.legend.location = 'top_left'\n",
    "        p.legend.background_fill_alpha = 0.7\n",
    "        p = dpf.format_fig_fonts(p, font_size=16)\n",
    "        rank_scatter_plots.append(p)\n",
    "    return rank_scatter_plots, model_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef12f90",
   "metadata": {},
   "source": [
    "Above: the outliers skew the meaning of the correlation between metrics from different methods.  The rank correlation is a better measure of the relationship between the methods, since it is not affected by the outliers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b8ad25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a color mapper for all correlation plots\n",
    "n_colors = 10\n",
    "palette = list(reversed(RdBu[n_colors]))\n",
    "low, high = -0., 1.0\n",
    "mapper = LinearColorMapper(palette=palette, low=low, high=high)\n",
    "edges = np.linspace(low, high, n_colors + 1)\n",
    "centers = [(edges[i] + edges[i+1]) / 2 for i in range(n_colors)]\n",
    "# Labels to display (left edges 0.50..0.95), positioned at centers\n",
    "label_map = {edges[i]: f\"{edges[i]:.2f}\" for i in range(len(edges))}\n",
    "mapper = LinearColorMapper(palette=palette, low=low, high=high)\n",
    "corr_type = 'kendall'\n",
    "cbar_title = 'Kendall Tau Correlation' if corr_type == 'kendall' else 'Pearson Correlation'\n",
    "cbar = ColorBar(\n",
    "    color_mapper=mapper,\n",
    "    ticker=FixedTicker(ticks=edges),\n",
    "    major_label_overrides=label_map,\n",
    "    # add a title to the color bar\n",
    "    title=cbar_title,\n",
    "    major_label_text_align=\"center\",\n",
    "    major_label_text_baseline=\"middle\",\n",
    "    label_standoff=8,            # nudge labels off the bar a bit\n",
    "    padding=5,                   # bar padding inside its box\n",
    "    # orientation=\"vertical\",    # default; set \"horizontal\" if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d583d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_regression(df, all_metrics, cbar, correlation_type='pearson'):\n",
    "    plots = []\n",
    "    \n",
    "    for md in all_metrics:        \n",
    "        all_models = df.columns\n",
    "        model_cols = sorted([c for c in all_models if not c.startswith('MLE') and ('4NN' not in c)])\n",
    "        model_cols = [c for c in model_cols if (c.endswith(f' {md}'))]\n",
    "\n",
    "        model_df = df[model_cols].copy()\n",
    "\n",
    "        model_df = model_df[[f'MoM LogNorm {md}', f'LogNorm {md}', f'2-NN {md}', f'8-NN {md}', f'LSTM Time {md}', f'LSTM Dist. {md}']]\n",
    "\n",
    "        if md == 'RB': # scores nearest zero are best\n",
    "            model_df = np.abs(model_df)\n",
    "        \n",
    "        corr_df = model_df.corr(method=correlation_type)\n",
    "        # Melt correlation matrix to long-form\n",
    "        corr_long = corr_df.reset_index().melt(id_vars='index')\n",
    "        corr_long.columns = ['x', 'y', 'value']\n",
    "\n",
    "        ends = corr_long['x'].str.find(md)\n",
    "        corr_long['x'] = [e[:i] for e, i in zip(corr_long['x'], ends)]\n",
    "        ends = corr_long['y'].str.find(md)\n",
    "        corr_long['y'] = [e[:i] for e, i in zip(corr_long['y'], ends)]\n",
    "        # Keep ONLY upper triangle (incl. diagonal). For lower, flip the inequality.\n",
    "        x_label_idxs = [e.find(md) for e in corr_df.columns]\n",
    "        x_labels = [e[:i] for e, i in zip(corr_df.columns, x_label_idxs)]\n",
    "        # replace RB with MAE label\n",
    "        x_labels = [e.replace('RB', 'PB') for e in x_labels]\n",
    "        y_labels = list(reversed(x_labels))  # you already reverse for display\n",
    "\n",
    "        order = {name: i for i, name in enumerate(x_labels)}\n",
    "        corr_long['ix'] = corr_long['x'].map(order)\n",
    "        corr_long['iy'] = corr_long['y'].map(order)\n",
    "\n",
    "        upper_mask = corr_long['iy'] >= corr_long['ix']   # diagonal + upper triangle\n",
    "        corr_long = corr_long[upper_mask].copy()\n",
    "        # Create source\n",
    "        source = ColumnDataSource(corr_long)\n",
    "            \n",
    "        w, h = 270, 300\n",
    "        if len(plots) == len(all_metrics) - 1:\n",
    "            w = 220\n",
    "        if (len(plots) > 0) & (len(plots) < len(all_metrics) - 1):\n",
    "            w = 220\n",
    "        x_label_idxs = [e.find(md) for e in corr_df.columns]\n",
    "        x_labels = [e[:i] for e, i in zip(corr_df.columns, x_label_idxs)]\n",
    "        y_labels = list(reversed(x_labels))\n",
    "        fw, fh = w, h\n",
    "        if len(plots) in [1, 2, 3]:\n",
    "            fw += 70\n",
    "        title = 'PB' if md == 'RB' else md\n",
    "        p = figure(\n",
    "            frame_width=fw,\n",
    "            frame_height=fh,\n",
    "            title=title,\n",
    "            x_range=x_labels, y_range=y_labels, toolbar_location = 'above',\n",
    "            tools=\"hover,save\",  tooltips=[(\"Model Pair\", \"@x vs @y\"), (\"Corr\", \"@value{0.2f}\")]\n",
    "        )\n",
    "    \n",
    "        p.rect(x=\"x\", y=\"y\", width=1, height=1, source=source,\n",
    "            fill_color=transform('value', mapper), line_color=None)            \n",
    "    \n",
    "        if len(plots) == len(all_metrics) - 1:\n",
    "            p.add_layout(cbar, 'right')\n",
    "\n",
    "        if len(plots) > 0:\n",
    "            p.yaxis.visible = False\n",
    "    \n",
    "        # Axis styling\n",
    "        p.axis.major_label_text_font_size = \"10pt\"\n",
    "        p.axis.major_label_standoff = 0\n",
    "        p.xaxis.major_label_orientation = np.pi / 2\n",
    "        #hide gridlines\n",
    "        p.xgrid.grid_line_color = None\n",
    "        p.ygrid.grid_line_color = None\n",
    "        p = dpf.format_fig_fonts(p, font_size=14)\n",
    "        plots.append(p)\n",
    "        \n",
    "    return plots\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc23668e-e110-4b7b-95ef-21d31b10944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = ['NAE', 'RMSE', 'NSE', 'KLD']\n",
    "correlation_plots = plot_correlation_regression(all_results_df, all_metrics, cbar, correlation_type=corr_type)\n",
    "# lt = row(correlation_plots)\n",
    "lt = gridplot(correlation_plots, ncols=5, toolbar_location=None)\n",
    "fname = f\"rank_correlation_by_model_{bitrate:02d}_bits\"\n",
    "# dpf.save_fig(lt, fname)\n",
    "show(lt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2011142e",
   "metadata": {},
   "source": [
    "### Compute the model rank shifts by metric\n",
    "\n",
    "Use the KL divergence as the basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f85096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_permutation_baseline(n):\n",
    "    \"\"\"Analytic baseline for mean |rank diff| between two random rankings of n items.\"\"\"\n",
    "    mean = n/3 - 1/(3*n)\n",
    "    var  = ((n-1)*(n+1)*(n**2 + 2)) / (18 * n**2)\n",
    "    std  = np.sqrt(var)\n",
    "    return mean, std\n",
    "\n",
    "def numerical_shuffling(n):\n",
    "    \"\"\"Numerical estimate of mean |rank diff| between two random rankings of n items.\"\"\"\n",
    "    n_trials = 10000\n",
    "    diffs = []\n",
    "    for _ in range(n_trials):\n",
    "        r1 = np.random.permutation(n) + 1\n",
    "        r2 = np.random.permutation(n) + 1\n",
    "        diffs.append(np.mean(np.abs(r1 - r2)))\n",
    "    return np.mean(diffs), np.std(diffs)\n",
    "\n",
    "mean, std = random_permutation_baseline(len(all_results_df))\n",
    "print(f'Random permutation baseline for {len(all_results_df)} items: mean={mean:.2f}, std={std:.2f}')\n",
    "mean, std = numerical_shuffling(len(all_results_df))\n",
    "print(f'Numerical shuffling estimate for {len(all_results_df)} items: mean={mean:.2f}, std={std:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5713ff86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the average rank shift for each model across all metrics\n",
    "# i.e. what was the average absolute difference in rank for each model by changing the evaluation metric\n",
    "metric_rank_shifts = {}\n",
    "metrics = ['NAE', 'RMSE', 'NSE', 'KLD']\n",
    "models  = ['MoM LogNorm', 'LogNorm', '2-NN', '8-NN', 'LSTM Dist.', 'LSTM-4NN', 'LSTM-LN-4NN']\n",
    "for base_metric in metrics:\n",
    "    print(f'Base metric: {base_metric}')\n",
    "    for md in models:\n",
    "        # print(f'   evaluating rank differences across evaluation metrics by the {md} model')\n",
    "        model_cols = [c for c in all_results_df.columns if (c.startswith(md) and 'Time' not in c and '4NN' not in c)]\n",
    "\n",
    "        min_val = round(all_results_df[f'{md} {base_metric}'].min(), 0)\n",
    "\n",
    "        basis_rank = all_results_df[f'{md} {base_metric}'].rank(ascending=True)\n",
    "        # print(f'{md} {base_metric}', min_val)\n",
    "        # if base_metric == 'RB':\n",
    "        #     basis_rank = all_results_df[f'{md} {base_metric}'].abs().rank(ascending=True)\n",
    "        rank_diffs_df = pd.DataFrame(index=all_results_df.index)\n",
    "        for metric in ['NAE', 'RMSE', 'NSE', 'KLD']:\n",
    "            if metric == base_metric:\n",
    "                continue\n",
    "            target_col = f'{md} {metric}'\n",
    "            rank_diffs_df[f'{base_metric}-{metric} diff'] = (basis_rank - all_results_df[target_col].rank(ascending=True)).abs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86169ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['NAE', 'RMSE', 'NSE', 'KLD']\n",
    "models  = ['MoM LogNorm', 'LogNorm', '2-NN', '8-NN', 'LSTM Dist.', 'LSTM-4NN', 'LSTM-LN-4NN']\n",
    "\n",
    "def rank_metric(s, m):\n",
    "    if m == 'RB':          # closer to 0 is better\n",
    "        return s.abs().rank(ascending=True)\n",
    "    else:  # RMSE, KLD lower is better\n",
    "        return s.rank(ascending=True)\n",
    "\n",
    "# unique unordered pairs\n",
    "pairs = [(metrics[i], metrics[j]) for i in range(len(metrics)) for j in range(i+1, len(metrics))]\n",
    "\n",
    "rows = []\n",
    "for md in models:\n",
    "    row = {}\n",
    "    for a, b in pairs:\n",
    "        ra = rank_metric(all_results_df[f'{md} {a}'], a)\n",
    "        rb = rank_metric(all_results_df[f'{md} {b}'], b)\n",
    "        diffs = (ra - rb).abs()\n",
    "        mean_val = diffs.mean()\n",
    "        std_val  = diffs.std()\n",
    "        # Format as \"mean (std)\" with rounding\n",
    "        row[f'{a}-{b}'] = int(round(mean_val, 0))\n",
    "    rows.append({'Model': md, **row})\n",
    "\n",
    "df_table = pd.DataFrame(rows).set_index('Model')\n",
    "# add \"Mean\" row across models, computing mean and std of the mean values\n",
    "mean_row = {}\n",
    "for col in df_table.columns:\n",
    "    # extract numeric part from each cell for mean/std across models\n",
    "    nums = df_table[col].values\n",
    "    mean_row[col] = f\"{np.mean(nums):.0f} ± {np.std(nums):.0f}\"\n",
    "df_table.loc['Mean'] = mean_row\n",
    "\n",
    "# export to LaTeX\n",
    "latex_str = df_table.to_latex(\n",
    "    escape=False,   # allow parentheses and spaces\n",
    "    multicolumn=False,\n",
    "    caption='Average absolute rank shift (mean with stdev in brackets) per model and metric pair.',\n",
    "    label='tab:avg-rank-shift-mean-std',\n",
    "    bold_rows=True\n",
    ")\n",
    "\n",
    "print(latex_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c17d5ce",
   "metadata": {},
   "source": [
    "## Examine how ranks change between metrics\n",
    "\n",
    "We have seen how the rank correlations compare between methods for the same metric, and this tells us that some methods are complementary, that is they may capture different aspects of the data or model performance. Another perspective is to look at how the ranks change by metric, in other words to see where metrics agree and disagree about the relative performance of methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e85059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models.formatters import CustomJSTickFormatter\n",
    "\n",
    "def plot_correlation_regression(df, models, all_metrics, cbar, correlation_type='pearson'):\n",
    "    plots = []\n",
    "    for md in models:#, 'LSTM-4NN', 'LSTM-LN-4NN']:\n",
    "        model_cols = [c for c in df.columns if (md in c) and 'Time' not in c and 'MoM' not in c and 'MLE' not in c and 'PB' not in c]\n",
    "        model_df = df[model_cols].copy()\n",
    "        rb_col = [f for f in model_cols if f.endswith('RB')]\n",
    "        if rb_col:\n",
    "            # rename to PB for display\n",
    "            model_df.rename(columns={rb_col[0]: rb_col[0].replace('RB', 'PB')}, inplace=True)\n",
    "            model_cols = [c.replace('RB', 'PB') if c.endswith('RB') else c for c in model_cols]\n",
    "            # rank by absolute value for RB/PB\n",
    "            model_df[model_cols] = model_df[model_cols].abs()\n",
    "        assert model_cols, df.columns\n",
    "\n",
    "        corr_df = model_df[model_cols].corr(method=correlation_type)\n",
    "        # set absolute value for RB/PB correlations\n",
    "        \n",
    "        corr_df.loc[:, corr_df.columns.str.endswith('PB')] = corr_df.loc[:, corr_df.columns.str.endswith('PB')].abs()\n",
    "\n",
    "\n",
    "        if correlation_type == 'kendall':\n",
    "            label = 'Kendall Tau Correlation'\n",
    "        elif correlation_type == 'pearson':\n",
    "            label = 'Pearson Correlation'\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported correlation type: {correlation_type}\")\n",
    "        \n",
    "        # Melt correlation matrix to long-form\n",
    "        corr_long = corr_df.reset_index().melt(id_vars='index')\n",
    "        # print(corr_df)\n",
    "        corr_long.columns = ['x', 'y', 'value']\n",
    "\n",
    "        # Keep ONLY upper triangle (incl. diagonal). For lower, flip the inequality.\n",
    "        # x_label_idxs = [e for e in corr_df.columns]\n",
    "        x_labels = [e for e in corr_df.columns]\n",
    "        # replace RB with MAE label\n",
    "        y_labels = list(reversed(x_labels))  # you already reverse for display\n",
    "\n",
    "        order = {name: i for i, name in enumerate(x_labels)}\n",
    "        corr_long['ix'] = corr_long['x'].map(order)\n",
    "        corr_long['iy'] = corr_long['y'].map(order)\n",
    "\n",
    "        upper_mask = corr_long['iy'] >= corr_long['ix']   # diagonal + upper triangle\n",
    "        corr_long = corr_long[upper_mask].copy()\n",
    "        # Create source\n",
    "        source = ColumnDataSource(corr_long)\n",
    "    \n",
    "        # low, high = 0.1, 1.0\n",
    "        # min_val = min(corr_long['value'].min(), 0.1)\n",
    "\n",
    "        # Optional: format labels as strings with 2 decimals\n",
    "        w, h = 270, 300\n",
    "        if len(plots) == len(all_metrics) - 1:\n",
    "            w = 280\n",
    "        if (len(plots) > 0) & (len(plots) < len(all_metrics) - 1):\n",
    "            w = 200\n",
    "        # replace RB with MAE label\n",
    "        y_labels = list(reversed(x_labels))\n",
    "        fw, fh = w, h\n",
    "        if len(plots) in [1, 2]:\n",
    "            fw += 70\n",
    "        \n",
    "        p = figure(\n",
    "            frame_width=fw,\n",
    "            frame_height=fh,\n",
    "            title=md,\n",
    "            x_range=x_labels, y_range=y_labels, toolbar_location = 'above',\n",
    "            tools=\"hover,save\",  tooltips=[(\"Model Pair\", \"@x vs @y\"), (\"Corr\", \"@value{0.2f}\")]\n",
    "        )\n",
    "    \n",
    "        p.rect(x=\"x\", y=\"y\", width=1, height=1, source=source,\n",
    "            fill_color=transform('value', mapper), line_color=None)            \n",
    "    \n",
    "        if len(plots) == len(models) - 1:\n",
    "            p.add_layout(cbar, 'right')\n",
    "\n",
    "        if len(plots) > 0:\n",
    "            p.yaxis.visible = False        \n",
    "    \n",
    "        # Axis styling\n",
    "        p.axis.major_label_text_font_size = \"10pt\"\n",
    "        p.axis.major_label_standoff = 0\n",
    "        p.xaxis.major_label_orientation = np.pi / 2\n",
    "        p = dpf.format_fig_fonts(p, font_size=14)\n",
    "\n",
    "        # label_map = {label: label.split(\" \")[-1] for label in y_labels}\n",
    "        p.yaxis[0].formatter = CustomJSTickFormatter(code=\"\"\"\n",
    "            const s = String(tick);\n",
    "            const i = s.lastIndexOf(' ');\n",
    "            return i >= 0 ? s.slice(i+1) : s;\n",
    "        \"\"\")\n",
    "        p.xaxis[0].formatter = CustomJSTickFormatter(code=\"\"\"\n",
    "            const s = String(tick);\n",
    "            const i = s.lastIndexOf(' ');\n",
    "            return i >= 0 ? s.slice(i+1) : s;\n",
    "        \"\"\")\n",
    "        plots.append(p)\n",
    "        \n",
    "    return plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10741467",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['LogNorm', '8-NN', 'LSTM Dist']\n",
    "metrics = ['NAE', 'RMSE', 'NSE', 'KLD']\n",
    "correlation_plots = plot_correlation_regression(all_results_df, models, metrics, cbar, correlation_type='kendall')\n",
    "lt = gridplot(correlation_plots, ncols=4, toolbar_location=None)\n",
    "fname = f\"rank_correlation_by_metric_{bitrate:02d}_bits\"\n",
    "# dpf.save_fig(lt, fname)\n",
    "show(lt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfd515d",
   "metadata": {},
   "source": [
    "Rank correlation between metrics within a model tell us whether different metrics agree on which catchments are \"easiest\" and which are \"hardest\" to model.  This is about evaluation objectives, not about propriety of the metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d34e42",
   "metadata": {},
   "source": [
    "### Compute the distribution of rank changes by metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3f7fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_df = all_results_df.copy()\n",
    "rank_diffs = {}\n",
    "abs_rank_diffs = {}\n",
    "\n",
    "for m in ['LogNorm', '8-NN', 'LSTM']:\n",
    "\n",
    "    cols = [c for c in all_results_df.columns if m in c and 'Time' not in c and '4NN' not in c and 'MoM' not in c and '2-NN' not in c and 'PB' not in c]\n",
    "\n",
    "    if m == 'RB':\n",
    "        # for RB, values close to zero are best\n",
    "        rank_df[cols] = np.abs(rank_df[cols])\n",
    "\n",
    "    rank_df[cols] = rank_df[cols].rank(ascending=True, method='min')\n",
    "    kld_col = [c for c in cols if 'KLD' in c]\n",
    "    other_cols = [c for c in cols if c not in kld_col]\n",
    "    for c in other_cols:\n",
    "        # Compute the differences in rank between this column and the KLD column\n",
    "        rank_diffs[f'{c}'] = (rank_df[c] - rank_df[kld_col[0]]).values\n",
    "        abs_rank_diffs[f'{c}'] = np.abs(rank_df[c] - rank_df[kld_col[0]]).values\n",
    "        print(f'{c}')     \n",
    "\n",
    "rank_data = pd.DataFrame(rank_diffs, index=all_results_df.index)\n",
    "abs_rank_diffs_data = pd.DataFrame(abs_rank_diffs, index=all_results_df.index)\n",
    "print(\"Rank differences compared to KLD:\")\n",
    "print(\"Absolute rank differences compared to KLD:\")\n",
    "# abs_rank_diffs_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77d6444",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(title=f\"Rank Differences: {dm} vs KLD\", width=800, height=500)\n",
    "lines = ['solid', 'dashed', 'dotted', 'dotdash']\n",
    "for i, dm in enumerate(['RB', 'NAE', 'RMSE', 'NSE']):\n",
    "    line = lines[i]\n",
    "    rd = rank_data.copy()[[c for c in rank_data.columns if dm in c]]\n",
    "    for i, c in enumerate(rd.columns):\n",
    "        # plot a box plot of the rank differences for each model\n",
    "        v = rd[c].values\n",
    "        x, y = compute_empirical_cdf(v)\n",
    "        p.line(x, y, legend_label=c, line_width=2, color=Bokeh4[i], line_dash=line)\n",
    "\n",
    "    \n",
    "p.legend.location='top_left'\n",
    "p.legend.click_policy=\"hide\"\n",
    "p = dpf.format_fig_fonts(p, font_size=14)\n",
    "# show(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eaf351",
   "metadata": {},
   "source": [
    "### Plot example rank scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6199acda",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = []\n",
    "for m in ['LogNorm', '8-NN', 'LSTM']:\n",
    "    cols = sorted([c for c in rank_data.columns if m in c and 'Time' not in c and '4NN' not in c and 'MoM' not in c and '2-NN' not in c and 'PB' not in c])\n",
    "    print(cols)\n",
    "    \n",
    "    for i in [0, 1]:\n",
    "        c1, c2 = cols[i].split(' ')[-1], cols[i + 1].split(' ')[-1]\n",
    "        p = figure(title=f\"{m}: {c1} vs {c2} rank diffs vs. KLD\")\n",
    "        df = rank_data[[cols[i], cols[i+1]]].dropna(how='any').copy()\n",
    "        # rank the values\n",
    "        df = df.rank(ascending=True, method='min')\n",
    "        x, y = df[cols[i]].values, df[cols[i+1]].values\n",
    "        p.scatter(x, y, size=3, alpha=0.6, color='dodgerblue')\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "        xvals = [df[cols[i]].min(), df[cols[i]].max()]\n",
    "        yvals = [slope * x + intercept for x in xvals]\n",
    "        p.line(xvals, yvals, line_dash='dashed', color='red', line_width=2, legend_label=f'R²={r_value**2:.2f}')\n",
    "        p.line(xvals, xvals, line_color='black', line_dash='dotted', line_width=3, legend_label='1:1')\n",
    "        p.xaxis.axis_label = f'{c1} - DKL'\n",
    "        p.yaxis.axis_label = f'{c2} - DKL'\n",
    "        p.legend.location = 'top_left'\n",
    "        p.legend.background_fill_alpha = 0.7\n",
    "        p = dpf.format_fig_fonts(p, font_size=16)\n",
    "        plots.append(p)\n",
    "    \n",
    "lt = gridplot(plots, ncols=3, width=400, height=300)\n",
    "show(lt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51c669d",
   "metadata": {},
   "source": [
    "The next sets of plots represent different ways to control the preservation of information in the kNN method:\n",
    "\n",
    "1.  Look across k neighbours for the same method of selecting neighbours (e.g., IDW, CAS, etc.) to see how the kNN method performs as a function of k.  This will help us understand the trade-off between the number of neighbours and the performance of the kNN method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bd17d7",
   "metadata": {},
   "source": [
    "### Compare the sensitivity to metrics to the estimation performance\n",
    "\n",
    "Given the range of sensitivity expressed in the distributions above to sampling variability, to the choice of quantization, and to the edge trimming of the percentile range, we can compare the worst predicted FDCs to the sensitivity of the metrics on the reference distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942c890d",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 9 # our quantization is to 2^10 = 1024 bins (states)\n",
    "eps = 1e-3\n",
    "B = 1000\n",
    "\n",
    "spaces = {\n",
    "    'log':    (lambda v: v,          'log-units'),      # additive in log\n",
    "    'linear': (np.exp,               'L/s/km²'),        # additive in linear units (volume meaning)\n",
    "}\n",
    "all_results_dict = {}\n",
    "for m in [6, 8, 10]:\n",
    "    all_results_dict[m] = {e:{} for e in [1e-3, 1e-2]}\n",
    "    for eps in [1e-3, 1e-2]:\n",
    "        residuals_fpath = os.path.join('data', 'results', 'quantization_sensitivity', f'quantization_residuals_bootstrapped_{m}_bits_{str(eps)}_eps.csv')\n",
    "        print(f'   Loading existing results from {residuals_fpath}')\n",
    "        results_df = pd.read_csv(residuals_fpath, index_col=0, dtype={'official_id': str})\n",
    "        all_results_dict[m][eps] = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05413e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_metrics_boxplots(results_dict, all_metrics):\n",
    "    n_metrics = len(all_metrics)\n",
    "    fig, axes = plt.subplots(\n",
    "        1, n_metrics,\n",
    "        figsize=(5 * n_metrics, 6),\n",
    "        sharey=False, \n",
    "    )\n",
    "    lim_dict = {\n",
    "        'PB': (-50, 50),\n",
    "        'NAE': (-40, 40),\n",
    "        'RMSE': (-80, 60),\n",
    "        'NSE': (-0.5, 0.5),\n",
    "        'KLD': (-0.4, 0.4),\n",
    "    }\n",
    "\n",
    "    if n_metrics == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    rng = np.random.default_rng(0)\n",
    "    nn = 0\n",
    "    for ax, m in zip(axes, all_metrics):\n",
    "        xmin, xmax = lim_dict[m]\n",
    "        score_changes_df = results_dict[m]\n",
    "        cols = score_changes_df.columns\n",
    "        y = np.arange(1, len(cols) + 1)\n",
    "\n",
    "        # Horizontal boxplots\n",
    "        bp = ax.boxplot(\n",
    "            [score_changes_df[c].dropna().values for c in cols],\n",
    "            positions=y,\n",
    "            whis=1.5,\n",
    "            vert=False,\n",
    "            widths=0.6,\n",
    "            patch_artist=True,\n",
    "            showfliers=False,\n",
    "        )\n",
    "\n",
    "        # Style\n",
    "        for box in bp['boxes']:\n",
    "            box.set_alpha(0.8)\n",
    "            box.set_color('black')\n",
    "            box.set_facecolor('white')\n",
    "        for median in bp['medians']:\n",
    "            median.set_linewidth(2.0)\n",
    "            median.set_color('black')\n",
    "\n",
    "        # Jittered points (horizontal)\n",
    "        jitter = 0.12\n",
    "        for yi, c in zip(y, cols):\n",
    "            vals = score_changes_df[c].dropna().values\n",
    "            ax.scatter(\n",
    "                vals,\n",
    "                yi + rng.normal(0, jitter, size=vals.size),\n",
    "                s=12,\n",
    "                alpha=0.5,\n",
    "                linewidths=0\n",
    "            )\n",
    "\n",
    "        # Reference line at 0\n",
    "        ax.axvline(0, linestyle=\"--\", color=\"k\", linewidth=1)\n",
    "\n",
    "        # Labels\n",
    "        ax.set_yticks(y)\n",
    "        ax.set_yticklabels([e.upper() for e in cols], size=12)\n",
    "        # set the x-limits\n",
    "        ax.set_xlim(xmin, xmax)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "        ax.set_title(m, size=14)\n",
    "        nn += 1\n",
    "        if nn > 1:\n",
    "            ax.set_yticklabels([])\n",
    "\n",
    "    # fig.text(0.5, 0.04, \"Change in score (ensemble − component)\", ha=\"center\")\n",
    "    fig.tight_layout(rect=[0, 0.05, 1, 1])\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fddd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the change in performance from the individual model to the ensemble model\n",
    "score_changes = {}\n",
    "plots = []\n",
    "metrics = ['PB', 'NAE', 'RMSE', 'NSE', 'KLD']\n",
    "for m in metrics:\n",
    "    p = figure(title=f\"{m}\", width=800, height=400)\n",
    "    p2, k4, lstm2 = f'LogNorm {m}', f'4-NN {m}', f'LSTM Dist. {m}'\n",
    "    ln4nn = f'LN-4NN {m}'\n",
    "    l24nn = f'LSTM-4NN {m}'\n",
    "    l2ln4nn = f'LSTM-LN-4NN {m}'\n",
    "    changes = {\n",
    "        'p2-4nn_p2': (all_results_df[ln4nn] - all_results_df[p2]).values,\n",
    "        'p2-4nn_4nn': (all_results_df[ln4nn] - all_results_df[k4]).values,\n",
    "        'l2-4nn_l2': (all_results_df[l24nn] - all_results_df[lstm2]).values,\n",
    "        'l2-4nn_4nn': (all_results_df[l24nn] - all_results_df[k4]).values,\n",
    "        'l2-p2-4nn_l2': (all_results_df[l2ln4nn] - all_results_df[lstm2]).values,\n",
    "        'l2-p2-4nn_p2': (all_results_df[l2ln4nn] - all_results_df[p2]).values,\n",
    "        'l2-p2-4nn_4nn': (all_results_df[l2ln4nn] - all_results_df[k4]).values\n",
    "    }\n",
    "    score_changes_df = pd.DataFrame(changes)\n",
    "    score_changes[m] = score_changes_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c26439",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_all_metrics_boxplots(score_changes, all_metrics)\n",
    "fname = f'ensemble_model_score_changes_boxplots_{bitrate:02d}_bits'\n",
    "print(f'Saving figure to images/{fname}.png')\n",
    "fig.savefig(f'images/{fname}.png', dpi=150, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3c8e18",
   "metadata": {},
   "source": [
    "### Model Failure Analysis\n",
    "\n",
    "How often do the models fail to beat the \"null\" models?  If a catchment fails by one metric, is it likely to fail by others?  Is one metric more likely to produce failures? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45311808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# foo = all_results_df[[c for c in all_results_df.columns if c.endswith('RMSE')]].copy()\n",
    "failures = {'RMSE': 0, 'NSE': 0, 'KLD': 0, 'NAE': 0}\n",
    "fail_dfs = []\n",
    "stn_fail_dict = {'RMSE': [], 'NSE': [], 'KLD': [], 'NAE': []}\n",
    "for m in failures.keys():\n",
    "    uniform_scores = fdc_df.loc[fdc_df['Label'] == 'Uniform', ['Official_ID', m]].set_index('Official_ID')\n",
    "    mean_pmf_scores = fdc_df.loc[fdc_df['Label'] == 'Mean_PMF', ['Official_ID', m]].set_index('Official_ID')\n",
    "    randomDraw_scores = fdc_df.loc[fdc_df['Label'] == 'RandomDraw', ['Official_ID', m]].set_index('Official_ID')\n",
    "    metric_cols = sorted([c for c in all_results_df.columns if c.endswith(m)])\n",
    "    print(m)\n",
    "    fail_pcts, model_labels = [], []\n",
    "    for model in metric_cols:\n",
    "        model_scores = all_results_df[[model]].copy()\n",
    "        combined = pd.concat([model_scores, uniform_scores, mean_pmf_scores, randomDraw_scores], \n",
    "                             axis=1, join='inner')\n",
    "        combined.columns = [model, 'Uniform', 'Mean_PMF', 'RandomDraw']\n",
    "\n",
    "        if m == 'RB':\n",
    "            # find all rows where RB is worse than uniform (i.e. abs(RB_model) > abs(RB_uniform))\n",
    "            fails = combined[\n",
    "                (np.abs(combined[model]) > np.abs(combined[['Uniform', 'Mean_PMF', 'RandomDraw']]).min(axis=1))\n",
    "            ]\n",
    "        elif m == 'NSE':\n",
    "            max_nse = combined[model].max()\n",
    "            fails = combined[combined[model] < 0]\n",
    "            if max_nse > 1:\n",
    "                fails = combined[combined[model] > 1]\n",
    "        else:\n",
    "            # Check if the model column is worse than any of the benchmark columns, row-wise\n",
    "            fails = combined[\n",
    "                (combined[model] > combined[['Uniform', 'Mean_PMF', 'RandomDraw']].min(axis=1))\n",
    "            ]\n",
    "\n",
    "        n_fails = len(fails)\n",
    "        fail_pcts.append(n_fails / len(combined) * 100)\n",
    "        model_labels.append(model.split(m)[0])\n",
    "        print(f'    {model} has {n_fails} failures out of {len(combined)} stations. ({n_fails/715*100:.0f}%)')\n",
    "        \n",
    "        fail_stns = fails.index.values\n",
    "        for stn in fail_stns:\n",
    "            stn_fail_dict[m].append(stn)\n",
    "    fail_df = pd.DataFrame({'model': model_labels, m: fail_pcts})\n",
    "    fail_dfs.append(fail_df.set_index('model'))\n",
    "\n",
    "    print('')\n",
    "\n",
    "failure_df = pd.concat(fail_dfs, axis=1).round(0).astype(int)\n",
    "failure_df.to_latex()\n",
    "\n",
    "metric_fail_counts = {}\n",
    "for model, failed_stns in stn_fail_dict.items():\n",
    "    # get the stations that failed for this model\n",
    "    for stn in list(set(failed_stns)):\n",
    "        if stn not in metric_fail_counts:\n",
    "            metric_fail_counts[stn] = 0\n",
    "        metric_fail_counts[stn] += 1\n",
    "fail_count_df = pd.DataFrame.from_dict(metric_fail_counts, orient='index', columns=['n_failed_metrics'])\n",
    "fail_count_df.sort_values(by='n_failed_metrics', ascending=False, inplace=True)\n",
    "for i in range(1, 5):\n",
    "    n_stns = len(fail_count_df[fail_count_df['n_failed_metrics'] == i])\n",
    "    print(f'{n_stns} stations ({100*n_stns/715:.0f}%) failed {i} metrics.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69018ce3",
   "metadata": {},
   "source": [
    "### Format a results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802e24ae-33a8-4a47-a36e-33e6c92feb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_latex_table(df, all_models, metrics):\n",
    "    # Initialize a list to store rows of the LaTeX table\n",
    "    table_rows = []\n",
    "\n",
    "    # Define the models and metrics we want to extract\n",
    "    # Loop through each model and metric to format the table\n",
    "    for model in all_models:\n",
    "        row = [model]  # Start row with the model name\n",
    "        for m in metrics:\n",
    "            metric_cols = [c for c in df.columns if c.endswith(m)]\n",
    "            metric_data = df[metric_cols].copy()\n",
    "            label = f'{model}{m}'\n",
    "            assert label in df.columns, f'{label} not in columns'\n",
    "            if m == 'NSE':\n",
    "                nse_col = [c for c in metric_cols if c.endswith('NSE')][0]\n",
    "                max_val = metric_data[nse_col].max()\n",
    "                if max_val > 1:\n",
    "                    metric_data[nse_col] = 1 - metric_data[nse_col]\n",
    "            if model.startswith('MLE') and m == 'NSE':\n",
    "                metric_data[f'{model}{m}'] = 1 - metric_data[f'{model}{m}']\n",
    "            # elif m == 'RB':\n",
    "            #     # make values absolute for RB\n",
    "            #     metric_data[label] = np.abs(metric_data[label])\n",
    "            \n",
    "            mean = metric_data[label].mean()\n",
    "            median = metric_data[label].median()\n",
    "            lower, upper = np.percentile(metric_data[label], [2.5, 97.5])\n",
    "\n",
    "            best_val = False\n",
    "            if np.abs(mean) == abs(metric_data.mean()).min():\n",
    "                print('best val', label, round(mean, 2))\n",
    "                best_val = True \n",
    "            \n",
    "            # Format the mean to 1 decimal place and the CI as a tuple\n",
    "            mean_formatted = f\"{mean:.1f}, {median:.1f}\"\n",
    "            ci_formatted = f\"({lower:.1f}, {upper:.1f})\"\n",
    "            if m in ['PB', 'RMSE', 'NAE']:\n",
    "                mean_formatted = f\"{mean:.0f}, {median:.0f}\"\n",
    "                ci_formatted = f\"({lower:.0f}, {upper:.0f})\"\n",
    "            elif m in ['KLD', 'NSE']:\n",
    "                mean_formatted = f\"{mean:.2f}, {median:.2f}\"\n",
    "                ci_formatted = f\"({lower:.2f}, {upper:.1f})\"\n",
    "            else:\n",
    "                mean_formatted = f\"{mean:.2f}, {median:.2f}\"\n",
    "                ci_formatted = f\"({lower:.2f}, {upper:.2f})\"\n",
    "\n",
    "            formatted_row = f\"**{mean_formatted} {ci_formatted}**\" if best_val else f\"{mean_formatted} {ci_formatted}\"\n",
    "            # Append the formatted values as a single entry (mean and CI together)\n",
    "            row.append(formatted_row)\n",
    "\n",
    "        # Append the completed row to the table rows list\n",
    "        table_rows.append(row)\n",
    "    \n",
    "    # Create a DataFrame from the table rows for easy formatting\n",
    "    table_df = pd.DataFrame(table_rows, columns=[\"Model\"] + \n",
    "                            [f\"{metric}\" for metric in metrics])\n",
    "\n",
    "    # Now convert this DataFrame into LaTeX table format\n",
    "    # print(table_df)\n",
    "    latex_table = table_df.to_latex(index=False, escape=False)\n",
    "    # latex_table = table_df.to_markdown(index=False)\n",
    "    \n",
    "    return latex_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db03c546-408a-4913-80ad-f7211ab69e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = ['MLE* LogNorm ', 'MoM LogNorm ', 'LogNorm ', '8-NN ', '2-NN ', 'LSTM Time ', 'LSTM Dist. ', 'LN-4NN ', 'LSTM-4NN ', 'LSTM-LN-4NN '] \n",
    "print(all_results_df.columns)\n",
    "all_metrics = ['PB', 'NAE', 'RMSE', 'NSE', 'KLD']\n",
    "tb = format_latex_table(all_results_df, all_models, all_metrics)\n",
    "tb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d100663",
   "metadata": {},
   "source": [
    "## Check the percent change in land cover over 2010 to 2020 compared to the error metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33947873",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcub_attrs_fname = Path('data') / 'BCUB_watershed_attributes_updated_20250227.csv'\n",
    "bcub_df = pd.read_csv(bcub_attrs_fname, dtype={'official_id': str, 'watershed_id': str})\n",
    "bcub_df = bcub_df[[c for c in bcub_df.columns if c not in ['Unnamed: 0']]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290c5fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcub_df['forest_change'] = bcub_df['land_use_forest_frac_2010'] - bcub_df['land_use_forest_frac_2020']\n",
    "bcub_df['ice_change'] = bcub_df['land_use_snow_ice_frac_2010'] - bcub_df['land_use_snow_ice_frac_2020']\n",
    "bcub_df['water_change'] = bcub_df['land_use_water_frac_2010'] - bcub_df['land_use_water_frac_2020']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9cb20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of change values:\n",
    "f = figure(title=f'Distribution of land cover change (2010-2020)', width=600, height=400)\n",
    "for i, s in enumerate(['forest_change', 'ice_change', 'water_change']):\n",
    "    x, y = compute_empirical_cdf(bcub_df[s].values)\n",
    "    f.line(x, y, line_width=2, color=Bokeh6[2*i], legend_label=s)\n",
    "f.xaxis.axis_label = '% change'\n",
    "f.yaxis.axis_label = 'P(x)'\n",
    "f.legend.location = 'top_left'\n",
    "f.legend.click_policy = 'hide'\n",
    "f = dpf.format_fig_fonts(f, font_size=14)\n",
    "# show(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385950f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a correlation between land cover change and the DKL/EMD values\n",
    "# models = list(main_result_vals.keys())\n",
    "figs = []\n",
    "for dm in all_metrics:\n",
    "    f = figure(title=f'Correlation between land cover change and {dm}', width=600, height=400, y_axis_type='log')\n",
    "    # models = [m for m in models if m.endswith(f'_{dm}')]\n",
    "    models = [f'PredictedLog_{dm}', f'kNN1980_{dm}', f'LSTM_freq_{dm}'] \n",
    "    for i, model in enumerate(models):    \n",
    "        # 'LN MoM KLD', 'LN Direct KLD', '3 kNN KLD', '9 kNN KLD', 'LSTM time KLD', 'LSTM freq KLD'\n",
    "        data = main_result_vals[f'LSTM Dist. {dm}'].copy()\n",
    "        # convert to a dict with the ids as keys\n",
    "        model_dict = dict(zip(data['ids'], data['values']))\n",
    "        # map the model values to the bcub_df\n",
    "        model_df = bcub_df.copy()\n",
    "        model_df[model] = model_df['official_id'].map(model_dict, None)\n",
    "        model_df = model_df[model_df[model].notna()].copy()\n",
    "        data = model_df[[model, 'forest_change']]\n",
    "        f.scatter(data['forest_change'], data[model], size=5, color=Bokeh6[i], legend_label=model, alpha=0.4)\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(data['forest_change'], data[model])\n",
    "        x = np.linspace(data['forest_change'].min(), data['forest_change'].max(), 100)\n",
    "        y = slope * x + intercept\n",
    "        f.line(x, y, line_width=2, color=Bokeh6[2*i], legend_label=f'{model} (R²={r_value**2:.2f})')\n",
    "    f.xaxis.axis_label = 'Forest Change (2010-2020)'\n",
    "    f.yaxis.axis_label = f'{dm} Value'\n",
    "    f.legend.location = 'top_left'\n",
    "    f.legend.click_policy = 'hide'\n",
    "    f.legend.background_fill_alpha = 0.5\n",
    "    f = dpf.format_fig_fonts(f, font_size=14)\n",
    "    figs.append(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3167e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lt = gridplot(figs, ncols=2, width=600, height=400)\n",
    "# show(lt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75bbaff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dddbaca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dcf412",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
