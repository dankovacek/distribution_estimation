{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a180e32",
   "metadata": {},
   "source": [
    "# FDC Estimation\n",
    "\n",
    "This notebook contains the main code for computing FDCs by the different methods. \n",
    "\n",
    "To run this notebook, several pre-requisites must be met.\n",
    "\n",
    "1. The data must be downloaded and pre-processed (baseline distributions, global mean FDC computed) as described in Notebook 1.\n",
    "2. The the network partitioning file must be created as described in Notebook 2.\n",
    "3. The runoff statistics must be computed as described in Notebook 3.\n",
    "\n",
    "In this section:\n",
    "\n",
    "1. We construct flow duration curves (FDCs) for each catchment and define the discrete probability representation used for model evaluation.\n",
    "\n",
    "2. We apply the three FDC-estimation approaches—parametric log-normal predictions (introduced in Notebook 2 (Methods)), $k$-nearest/physically similar donor ensembles, and weather-driven LSTM runoff simulations—to generate ungauged-site estimates.\n",
    "\n",
    "3. We compute performance metrics for each method, including error-based measures and the Kullback–Leibler divergence, and examine how they capture different notions of model skill.\n",
    "\n",
    "4. We compare the distribution of outcomes across methods, assess where models disagree, and evaluate whether ensemble combinations recover complementary information.\n",
    "\n",
    "Finally, we do **sensitivity analysis** reusing the main FDC estimation code as much as possible to ensure consistent procedures.  This includes testing the effects of:\n",
    "\n",
    "* variability on performance measures against the baseline distributions,\n",
    "* sampling variability on the reference distribution and how it propagates to performance scores when evaluated against FDCs estimated by kNN,\n",
    "* varying the number of quantiles defining the FDC support,\n",
    "* random model initialization on the LSTM and log-normal FDC estimates\n",
    "* sensitivity of kNN performance scores on quantile tail trimming $[\\epsilon, 1-\\epsilon]$ and uniform noise mixture strength $P = (1 - \\lambda) P_ + \\lambda \\mathbb{U}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558cd32f-b562-4502-ab63-826aede8f321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from time import time\n",
    "# import geopandas as gpd\n",
    "\n",
    "import xgboost as xgb\n",
    "xgb.config_context(verbosity=2)\n",
    "\n",
    "# from collections import defaultdict\n",
    "# from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# import jax\n",
    "# import jax.numpy as jnp\n",
    "\n",
    "from utils.kde_estimator import KDEEstimator\n",
    "from utils.knn_estimator import kNNEstimator\n",
    "from utils.LSTM_estimator import LSTMFDCEstimator\n",
    "from utils.parametric_estimator import ParametricFDCEstimator\n",
    "from utils.fdc_estimator_context import FDCEstimationContext \n",
    "from utils.fdc_data import StationData\n",
    "from utils.evaluation_metrics import EvaluationMetrics\n",
    "\n",
    "import utils.data_processing_functions as dpf\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.layouts import gridplot\n",
    "from math import comb\n",
    "# from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.models import ColorBar, BasicTicker, HoverTool, ColumnDataSource, Whisker\n",
    "from bokeh.transform import linear_cmap\n",
    "from bokeh.palettes import Viridis256\n",
    "\n",
    "import xyzservices.providers as xyz\n",
    "tiles = xyz['USGS']['USTopo']\n",
    "output_notebook()\n",
    "\n",
    "BASE_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894b3bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the catchment characteristics\n",
    "fname = f'catchment_attributes_with_runoff_stats.csv'\n",
    "attr_df = pd.read_csv(os.path.join('data', fname), dtype={'official_id': str, 'drainage_area_km2': float})\n",
    "attr_df.columns = [c.lower() for c in attr_df.columns]\n",
    "attr_df['log_drainage_area_km2'] = np.log(attr_df['drainage_area_km2'])\n",
    "# attr_df = attr_df[~attr_df['official_id'].isin(exclude)]\n",
    "# attr_df.columns = [c.lower() for c in attr_df.columns]\n",
    "attr_df['tmean'] = (attr_df['tmin'] + attr_df['tmax']) / 2.0\n",
    "station_ids = attr_df['official_id'].values\n",
    "stn_da_dict = attr_df.set_index('official_id')['drainage_area_km2'].to_dict()\n",
    "# assert '12414900' in station_ids\n",
    "\n",
    "print(f'There are {len(station_ids)} monitored basins in the attribute set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c385feeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-computed dictionary of complete years of record for each station\n",
    "complete_year_stats_fpath = os.path.join('data', 'complete_year_stats.npy')\n",
    "complete_year_stats = np.load(complete_year_stats_fpath, allow_pickle=True).item()\n",
    "\n",
    "meet_min_hyd_years = []\n",
    "for stn in complete_year_stats.keys():\n",
    "    if len(complete_year_stats[stn]['hyd_years']) >= 5:\n",
    "        meet_min_hyd_years.append(stn)\n",
    "    else:\n",
    "        print(f'Station {stn} has {len(complete_year_stats[stn][\"hyd_years\"])} complete hydrological years of data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b463650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# streamflow folder from (updated) HYSETS\n",
    "HYSETS_DIR = Path('/home/danbot/code/common_data/HYSETS')\n",
    "# STREAMFLOW_DIR = HYSETS_DIR / 'streamflow'\n",
    "\n",
    "hs_df = pd.read_csv('data/HYSETS_watershed_properties.txt', sep=';', dtype={'Official_ID': str})\n",
    "hs_df = hs_df[hs_df['Official_ID'].isin(station_ids)]\n",
    "hs_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8da5b67-fe95-4d78-94b1-c79ab46c48a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude catchments found to be regulated (missed by QC)\n",
    "# 12143700 isnot actually a dam, it's just basically a seepage monitoring station\n",
    "# from a small catchment next to a dam \n",
    "exclude_stations = ['08FA009', '08GA037', '08NC003', '12052500', '12090480', '12107950', '12108450', '12119300', \n",
    "                    '12119450', '12200684', '12200762', '12203000', '12409500', '15056070', '15081510',\n",
    "                    '12323760', '12143700', '12143900', '12398000', '12058800', '12137800', '12100000']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6751a465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve LSTM ensemble predictions\n",
    "LSTM_ensemble_result_folder = '/home/danbot/code/neuralhydrology/data/ensemble_results_20250514' # based on mean NSE loss\n",
    "# LSTM_ensemble_result_folder = '/home/danbot/code/neuralhydrology/data/ensemble_results_20250627' # based on 95% NSE loss\n",
    "lstm_result_files = os.listdir(LSTM_ensemble_result_folder)\n",
    "lstm_result_stns = [e.split('_')[0] for e in lstm_result_files]\n",
    "assert '12414900' in lstm_result_stns\n",
    "\n",
    "# find any non-matching station ids in the lstm result files\n",
    "stns_to_remove_from_lstm_set = []\n",
    "for stn in lstm_result_stns:\n",
    "    if stn not in station_ids:\n",
    "        # try adding a leading zero\n",
    "        ending_in = [e for e in station_ids if e.endswith(stn)]\n",
    "        if len(ending_in) > 0:\n",
    "            print(stn, 'matches', ending_in)\n",
    "        modified_stn = stn.zfill(8)\n",
    "        if modified_stn in station_ids:\n",
    "            print(f'Found modified station id: {modified_stn} for {stn}')\n",
    "        else:\n",
    "            print(f'Warning: {stn} is in LSTM results but not in the station attributes.')\n",
    "            stns_to_remove_from_lstm_set.append(stn)\n",
    "\n",
    "# filter for the common stations between BCUB region and LSTM-compatible (i.e. 1980-)\n",
    "daymet_concurrent_stations = sorted(list(np.intersect1d(station_ids, np.intersect1d(lstm_result_stns, meet_min_hyd_years))))\n",
    "print('08JE005' in daymet_concurrent_stations)\n",
    "# assert '12414900' in daymet_concurrent_stations\n",
    "# print(f'There are {len(pmf_stations)} monitored basins with baseline PMFs.')\n",
    "print(f'There are {len(daymet_concurrent_stations)} monitored basins concurrent with LSTM ensemble results.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36bb2e9-6a2c-452d-af42-aa49ea32d109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple check to make sure \"DAM\" is not in the station name \n",
    "#  -- commonly used to indicate regulated stations\n",
    "for stn in daymet_concurrent_stations:\n",
    "    hs_data = hs_df[hs_df['Official_ID'] == stn].copy()\n",
    "    name = hs_data['Name']\n",
    "    if 'DAM' in name:\n",
    "        print(stn, name)\n",
    "    if 'RESERVOIR' in name:\n",
    "        print(stn, name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6f0e6e-e089-4951-a047-e0b49b6410c1",
   "metadata": {},
   "source": [
    "## Non-Parametric Simulation\n",
    "\n",
    "### Time-based ensemble\n",
    "\n",
    "A probability distribution $\\hat p = f(\\tilde x(t))$ is estimated for a target (ungauged location) by a weighted mean of runoff time-series from k nearest neighbour stations, $\\tilde x(t) = \\textbf{X}(t)\\cdot w$ where $X(t) \\in \\mathbb{R}^{N \\times k}$ and $w \\in \\mathbb{R}^{k\\times 1}$ is a vector of k weights.  So $\\hat p = f(\\textbf{X}(t) \\cdot w )$  Weights $w$ are computed in two ways, described in the next subsection, and k-nearest neighbours are selected using the criteria defined below.  Each gauged station in the monitoring network is treated as an ungauged location to generate a large sample of simulations across hydrologically diverse catchments, or rather as many catchments as can be tested.\n",
    "\n",
    "### Frequency-based ensembles\n",
    "\n",
    "A simulated probability density function is estimated from observations of k nearest neighbour stations.  First, k simulated series are generated by equal unit area runoff , $\\hat p = \\hat P \\cdot w$ where $\\hat P = [\\hat p_1, \\hat p_2, \\cdots, \\hat p_k]$ and each $\\hat p_i = f(X_i(t))$.\n",
    "\n",
    "In both cases, the function $f \\rightarrow \\hat p(x)$ represents kernel density estimation, which defines the probability density as $$\\hat p(x) = \\frac{1}{n \\cdot h(x)} \\sum_{i=1}^{n}K\\left( \\frac{x-x_i}{h(x)}\\right)$$ \n",
    "\n",
    "Where $h(x)$ reflects an adaptive kernel bandwidth that addresses vestiges of precision in the observed data to reflect the nature of streamflow as a continuous variable, and additionally incorporates piecewise linear model to represent overall measurement uncertainty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7d4695",
   "metadata": {},
   "source": [
    "## Notes on k-nearest neighbours\n",
    "\n",
    "Time series streamflow records vary widely in their temporal coverage, and finding k-nearest neighbours presents a tradeoff between selecting nearest neighbours and maximizing the number of observations concurrent with the target.  From the literature, concurrency is assured by pre-selecting a subset of stations with continuous records over a common period of record, or by infilling gaps with k-nearest neighbours simulation.  Some kind of tradeoff must be made, and we aim to use a method that maximizes information content while minimizing the number of assumptions.  The following notes are intended to clarify the implications of using k-nearest neighbours to fill gaps in the time series.\n",
    "\n",
    "1. **Infilled-by-kNN != Independent Proxy**: If a gap in an observation record is inferred from neighbors, its inclusion in the ensemble is functionally equivalent to increasing the ensemble size.\n",
    "\n",
    "2. **Information leakage risk**: Excessive infilling of missing data, risks suppressing variability by biasing toward the central tendency of the ensemble.  This defeats one of the motivations for using kNN: to preserve structure and variability from observations at neighboring stations.\n",
    "\n",
    "To address the nuance above, three time-based methods are defined for selecting k-nearest neighbours beyond the strict interpretation of nodes in the network.  The problem is related to the set-cover problem where the goal is to select a subset of stations that maximizes the intersection of their data availability over a specified time period.  The following sections outline the three methods for selecting k-nearest neighbours based on availability of concurrent data.\n",
    "\n",
    "|                                        | Description |\n",
    "|----------------------------------------|-------------|\n",
    "| Set Intersection Selection             | Select \\( k \\) nodes whose intersection satisfies a completeness constraint. |\n",
    "| Maximum Coverage under Cardinality Constraint | Choose \\( k \\) nodes to maximize the coverage (or completeness) of their intersection. |\n",
    "| Recursive k-Subset Validation          | If the initial \\( k \\) nodes fail, iteratively add more candidates and evaluate all \\( \\binom{k+1}{k} \\) combinations, and so on. |\n",
    "| NP-Hard Nature                         | This problem is computationally hard and shares structure with the Set Cover and Maximum Coverage problems. |\n",
    "\n",
    "\n",
    "### Summary: Set-Theoretic implications of strict k-NN concurrency selection\n",
    "\n",
    "This problem is closely related to classic combinatorial and set-theoretic optimization problems.\n",
    "\n",
    "#### Set-Theoretic Definition\n",
    "\n",
    "Let each column $( S_i \\subseteq T )$ represent the set of timestamps where station $( i )$ has valid (non-NaN) data.  \n",
    "Let $( \\mathcal{S} = \\{ S_1, S_2, \\dots, S_n \\} )$ be the collection of all such subsets, sorted by proximity (e.g., distance or attribute similarity).  \n",
    "The goal is to select a subset $( \\mathcal{K} \\subset \\mathcal{S} )$ such that:\n",
    "- $( |\\mathcal{K}| = k )$\n",
    "- $( \\bigcap_{S \\in \\mathcal{K}} S )$ satisfies a temporal completeness constraint (e.g., ≥5 years with ≥10 observations in each of 12 months)\n",
    "\n",
    "This is a constrained subset selection problem on the intersection of sets.\n",
    "\n",
    "#### Practical Implication\n",
    "\n",
    "This formulation justifies using greedy or approximate subset selection strategies when exhaustively testing all combinations becomes computationally infeasible.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50a7f09",
   "metadata": {},
   "source": [
    "\n",
    "## Define a universal parametric prior\n",
    "\n",
    "In order to fairly test how parametric and non-parametric pdf estimation methods compare to each other, we need a consistent way to deal with indeterminate cases where the simulated distribution does not provide support coverage of the \"ground truth\" observations.  I feel two ways about this: the KL divergence is the culprit here, and the problem could be avoided by choosing another divergence measure.  However the definintion of KL divergence in information theoretic terms of compression and optimal encoding seem more foundational than other measures that reflect utility in some way. Should we look to math statistics to make more direct links between f-divergences and what we use as a discriminant for a particular application?  Should we be more concerned about \"Bayesian consistency\" of the discriminant (or surrogate loss function) with the choice of divergence measure?\n",
    "\n",
    "\n",
    "1.  **Quantify the distribution of unsupported mass across all models**.  It is important to describe the extent of the problem across the sample **and** across various methods.  i.e. discrete distributions have the issue of support coverage, but so do all methods!\n",
    "2.  Even in kNN / ensemble simulation approaches, the problem of incomplete support coverage necessitates specifying some form of prior in the likelihood function.  The issue is that setting a uniform prior over the observed range takes advantage of information about the observed range.  We got around this by setting a global support assumption and discretizing the range into bins according to an assumption about measurement precision.  i.e. given the global range, we used $2^{10}$ bins spaced evenly on a log scale to reflect roughly 1% relative precision.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008336e7-cca2-440e-99da-e3919f351793",
   "metadata": {},
   "source": [
    "### Global support quantization\n",
    "\n",
    "$$f(x) = \\frac{1}{b-a}, \\quad x\\in (a, b) \\text{ and } f(x) = 0 \\text{ otherwise.}$$\n",
    "$$\\int_a^b f(x)\\text{dx} = 1$$\n",
    "\n",
    "Given the target range is a sub interval $(c, d) \\subseteq (a, b)$, then the **total** probability mass over (c, d) is:\n",
    "\n",
    "$$M_\\text{target} = \\int_c^d \\frac{1}{b-a}\\text{dx} = \\frac{d-c}{b-a}$$\n",
    "\n",
    "Over the set of intervals $\\Delta x_i$ covering the **target range**, the probability mass associated with each interval (bin) is given by: \n",
    "\n",
    "$$\\Delta x_i \\frac{d-c}{b-a}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2231b97e",
   "metadata": {},
   "source": [
    "### Compute the mean PDF across all stations\n",
    "\n",
    "Given a state space $\\Omega$ with $M$ discrete states, and $N$ spatially distributed sensors (streamflow monitoring stations) with PMFs $P=\\{p_j\\}_{j=1}^N$, we can define the mean PDF across all sensors in terms of the observed states $\\omega$ as follows:\n",
    "\n",
    "$$\\Omega=\\{\\omega_i\\}_{i=1}^M,\\quad P\\in\\mathbb{R}^{M\\times N},\\quad P_{i j}=p_j(\\omega_i),\\ \\sum_{i=1}^M P_{i j}=1.$$\n",
    "\n",
    "$$P_{i j}=\\Pr_j(B_i),\\qquad \\bar p_i =\\frac{1}{N}\\sum_{j=1}^N P_{i j}\\quad(\\text{mean PMF over sensors}).$$\n",
    "\n",
    "where $B_i$ denotes the $i$-th quantization bin (interval) in log-unit-area-runoff (L-UAR), i.e., $B_i = [y_i, y_{i+1})$ with $y_i = \\log(x_i)$. Each $P_{ij}$ is the probability assigned by sensor $j$ to bin $B_i$.\n",
    "\n",
    "$$\\text{Density per log-}x\\ (\\text{piecewise constant on }B_i):\\quad h_i=\\frac{\\bar p_i}{\\Delta y_i},\\qquad \\sum_i h_i\\,\\Delta y_i=1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61f2337",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Entropy of the mean PDF and fraction of $b$-bit quantization capacity\n",
    "\n",
    "$$M = 2^{b},\\qquad \\bar{\\mathbf p} = (\\bar p_1,\\ldots,\\bar p_M)^T,\\quad \\bar p_i \\ge 0,\\ \\sum_{i=1}^M \\bar p_i = 1$$\n",
    "\n",
    "$$\\text{Shannon entropy (bits):}\\quad H_2(\\bar{\\mathbf p}) \\;=\\; -\\sum_{i:\\,\\bar p_i>0} \\bar p_i \\log_2 \\bar p_i \\;\\le\\; b.\n",
    "$$\n",
    "\n",
    "$$\\text{Normalized entropy (fraction of capacity):}\\quad \\rho_b \\;=\\; \\frac{H_2(\\bar{\\mathbf p})}{b} \\in [0,1].$$\n",
    "\n",
    "$$\\text{Perplexity (effective bins):}\\quad \\mathcal P(\\bar{\\mathbf p}) \\;=\\; 2^{H_2(\\bar{\\mathbf p})},\\qquad\n",
    "\\text{occupancy fraction } \\phi_b \\;=\\; \\frac{\\mathcal P}{2^b} \\;=\\; 2^{H_2(\\bar{\\mathbf p})-b}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2c621f-81ff-4b85-a15f-f3b9565ebf90",
   "metadata": {},
   "source": [
    "## Notes on FDC estimation from distribution mixtures\n",
    "\n",
    "A desirable property of the estimated FDC is that it prevents predicting zero probabilities over the support $\\Omega$ while reflecting the strength of belief in the model (data).  To avoid predicting zero probabilities over the support, a component of the FDC estimation model is to add a uniform distribution component in the form of a mixture model: $\\hat Q = (1 - \\lambda) Q + \\lambda \\mathbb{U}$, where $U$ is the uniform distribution over the support.  The mixture ratio $\\lambda$ reflects the strength of belief in the model (data), where a smaller $\\lambda$ reflects stronger belief in the data/model and vice versa.  The strength of belief in the uniform component is a way of controlling how the addition of information by uniform noise influences the estimated distribution.  More importantly, the \"surprise\" of observing a state $p_i > 0$ where $q_i \\rightarrow 0$ grows to infinity, so care must be taken not just to prevent infinite divergence but also to consider what the addition of uniform noise in its place represents as far as a certainty in the frequency itself.  Simply assigning a very small number is not appropriate since it has a clear interpretation in terms of an expectation of how long it would take on average to observe the state. The mixture with a uniform component should not be so small as to represent a) a high degree of certainty in an event and b) an event that is far more rare than the observed duration without supporting (prior) information.\n",
    "\n",
    "\n",
    "1.  Incomplete support coverage, or underspecification, is infinitely penalized when the ground truth probability is nonzero..  The method does not tolerate a model that cannot predict the full observed range.\n",
    "2.  A **proper** probability distribution sums (discrete) or integrates (continuous) to 1.  Very small probabilities are in a sense associated with a high degree of certainty since they reflect the expectation of the system being observed in a particular state.\n",
    "3.  The penalty of underestimating a state frequency is that storing and transmitting information about the state requires (the log ratio) more bandwidth/disk space because it is assigned a longer bit string than the actual frequency calls for under optimal encoding.\n",
    "4.  Assigning a very small probability to a state ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a48368-9dda-4442-a09c-c506db94ba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the predicted parameter results\n",
    "parameter_prediction_results_folder = os.path.join('data', 'results', 'parameter_prediction_results', )\n",
    "predicted_params_fpath   = os.path.join(parameter_prediction_results_folder, 'OOS_parameter_predictions.csv')\n",
    "rdf = pd.read_csv(predicted_params_fpath, index_col=['official_id'], dtype={'official_id': str})\n",
    "predicted_param_dict = rdf.to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646ea56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = []\n",
    "predicted_param_sample = {}\n",
    "for l, al in zip(['log_uar_mean_predicted', 'log_uar_std_predicted'], [r'$$\\text{Log Mean UAR }(L/s/\\text{km}^2)$$', r'$$\\text{Log SD UAR }(L/s/\\text{km}^2)$$']):\n",
    "    vals = [d[l] for _, d in predicted_param_dict.items()]\n",
    "    predicted_param_sample[l] = vals\n",
    "    # plot the histogram of the mean_uar values\n",
    "    hist, edges = np.histogram(vals, bins=40, density=True)\n",
    "    # create a scatter plot of the predicted parameter vs the target parameter\n",
    "    f = figure(title=f'Predicted {l}', width=600, height=400)\n",
    "    f.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:], fill_color='lightblue', line_color='black', legend_label='')\n",
    "    f.xaxis.axis_label = al\n",
    "    f.yaxis.axis_label = r'$$P(x)$$'\n",
    "    f = dpf.format_fig_fonts(f, font_size=14)\n",
    "    plots.append(f)\n",
    "# retrieve all the mean_uar values \n",
    "\n",
    "lt = gridplot(plots, ncols=2, width=400, height=400)\n",
    "show(lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b4bbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FDCEstimatorRunner:\n",
    "    def __init__(self, stn_id, ctx, methods, k_nearest, parametric_target_cols, estimator_classes, **kwargs):\n",
    "        self.stn_id = stn_id\n",
    "        self.ctx = ctx\n",
    "        self.methods = methods\n",
    "        self.k_nearest = k_nearest\n",
    "        self.parametric_target_cols = parametric_target_cols\n",
    "        self.ESTIMATOR_CLASSES = estimator_classes\n",
    "        self._create_results_folders()\n",
    "        self._create_readme()\n",
    "        \n",
    "\n",
    "    def _create_results_folders(self):\n",
    "        # create a results foder for each method if it doesn't exist\n",
    "        self.results_folder = os.path.join('data', 'results', f'fdc_estimation_results_{self.ctx.regularization_type}',)\n",
    "        if self.ctx.regularization_type == 'discrete':\n",
    "            self.results_folder = os.path.join('data', 'results', f'fdc_estimation_results_{self.ctx.bitrate:02d}_bits',)\n",
    "\n",
    "        if not os.path.exists(self.results_folder):\n",
    "            os.makedirs(self.results_folder)\n",
    "            \n",
    "        for method in self.methods:\n",
    "            method_folder = os.path.join(self.results_folder, method)\n",
    "            if method == 'lstm':\n",
    "                self.lstm_rev_date = self.ctx.LSTM_ensemble_result_folder.split('_')[-1]\n",
    "                method_folder = os.path.join(self.results_folder, f'{method}_{self.lstm_rev_date}')\n",
    "            if not os.path.exists(method_folder):\n",
    "                os.makedirs(method_folder)\n",
    "\n",
    "\n",
    "    def _create_readme(self):\n",
    "        # create a readme file in the results folder to list constraints\n",
    "        readme_file = os.path.join(self.results_folder, 'README.txt')\n",
    "        \n",
    "        with open(readme_file, 'w') as file:\n",
    "            file.write(\"This folder contains the results of the FDC estimation.\\n\")\n",
    "            file.write(f\"Methods evaluated: {', '.join(self.methods)}\\n\")\n",
    "            # add the concurrency constraint and number of stations represented in the network\n",
    "            N = len(self.ctx.official_ids)\n",
    "            if self.ctx.include_pre_1980_data == False:\n",
    "                file.write(f'Uses only stations within Daymet input period of record / LSTM results: N={N} stations in the network.\\n')\n",
    "                file.write(f'Global start date on streamflow data: {self.ctx.global_start_date}\\n')\n",
    "            else:\n",
    "                file.write(f'Uses all available network stations in the BCUB region (1950-2024): N={N} stationsin the network.')\n",
    "                \n",
    "\n",
    "    def _save_result(self, result):\n",
    "        with open(self.result_file, 'w') as file:\n",
    "            json.dump(result, file, indent=4)\n",
    "\n",
    " \n",
    "    def setup_shared_data(self):\n",
    "        \"\"\"Set up the shared data object for the station.\n",
    "        Note the distinct handling of regularization on the baseline distribution \n",
    "        has a significant impact on the downstream estimators.\"\"\"\n",
    "        self.data = StationData(self.ctx, self.stn_id)\n",
    "        if self.ctx.regularization_type == 'kde':\n",
    "             self.data.kde_estimator = KDEEstimator(self.data.log_edges_extended)\n",
    "        self.data.k_nearest = self.k_nearest\n",
    "        self.data.parametric_target_cols = self.parametric_target_cols\n",
    "        self.data.eval_metrics = EvaluationMetrics(data=self.data, bitrate=self.ctx.bitrate)\n",
    "        self.data.baseline_pmf = self.data.baseline_pmf_df[stn].values\n",
    "\n",
    "\n",
    "    def run_selected(self):\n",
    "        # connect the station data object to this model runner\n",
    "        self.setup_shared_data()\n",
    "\n",
    "        for method in self.methods:\n",
    "            self.result_file = os.path.join(self.results_folder, method, f'{self.stn_id}_fdc_results.json')\n",
    "            if method == 'lstm':\n",
    "                method_folder = f'{method}_{self.lstm_rev_date}'\n",
    "                self.result_file = os.path.join(self.results_folder, method_folder, f'{self.stn_id}_fdc_results.json')\n",
    "            \n",
    "            if os.path.exists(self.result_file):\n",
    "                continue\n",
    "            else: \n",
    "                EstimatorClass = self.ESTIMATOR_CLASSES[method]\n",
    "                estimator = EstimatorClass(\n",
    "                    self.ctx, self.data\n",
    "                )\n",
    "                result = estimator.run_estimators()\n",
    "                self._save_result(result)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ecf64f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "target_cols = ['mean_uar', 'sd_uar', 'mean_logx', 'sd_logx']\n",
    "\n",
    "regularization_type = 'kde'  # 'discrete' or 'kde'\n",
    "\n",
    "for bitrate in [5, 6, 8, 10]:\n",
    "    if regularization_type == 'kde':\n",
    "        if bitrate < 10:\n",
    "            continue\n",
    "    global_min_uar = 5e-5   # see Notebook 1: data\n",
    "    global_max_uar = 1e4    # see Notebook 1: data\n",
    "\n",
    "    # set paths for catchment attributes and meteorological forcing data\n",
    "    attr_df_fpath = os.path.join('data', f'catchment_attributes_with_runoff_stats.csv')\n",
    "    LSTM_forcings_folder = '/home/danbot/neuralhydrology/data/BCUB_catchment_mean_met_forcings_20250320'\n",
    "    baseline_distribution_folder = os.path.join('data', 'baseline_distributions',)\n",
    "\n",
    "    # set which (subset of) methods to run\n",
    "    methods = ('parametric', 'lstm', 'knn')\n",
    "    k_nearest = 10\n",
    "    include_pre_1980_data = True  # use only stations with data 1980-present concurrent with Daymet\n",
    "    daymet_start_date = '1980-01-01'  # default start date for Daymet data\n",
    "    if include_pre_1980_data:\n",
    "        daymet_start_date = '1950-01-01'\n",
    "\n",
    "    processed = []\n",
    "    ESTIMATOR_CLASSES = {\n",
    "        'parametric': ParametricFDCEstimator,\n",
    "        'lstm': LSTMFDCEstimator,\n",
    "        'knn': kNNEstimator,\n",
    "    }\n",
    "\n",
    "    input_data = {\n",
    "        'attr_df_fpath': attr_df_fpath,\n",
    "        'LSTM_forcings_folder': LSTM_forcings_folder,\n",
    "        'LSTM_ensemble_result_folder': LSTM_ensemble_result_folder,\n",
    "        'include_pre_1980_data': include_pre_1980_data,  # use only stations with data 1980-present concurrent with Daymet\n",
    "        'predicted_param_dict': predicted_param_dict,\n",
    "        'eps': 1e-12,\n",
    "        'min_record_length': 5, # minimum record length (years)\n",
    "        'minimum_days_per_month': 20, # minimum number of days with valid data per month\n",
    "        'parametric_target_cols': target_cols,\n",
    "        'all_station_ids': daymet_concurrent_stations,\n",
    "        'baseline_distribution_folder': baseline_distribution_folder,\n",
    "        'delta': 0.001, # maximum uncertainty (by KL divergence) added to the predicted PMF by the uniform mixture ratio\n",
    "        'regularization_type': regularization_type, # use 'kde' or 'discrete'.  if discrete, bitrate must be specified\n",
    "        'bitrate': bitrate,\n",
    "        'complete_year_stats': complete_year_stats,\n",
    "        'year_type': 'hydrological',  # 'calendar' or 'hydrological'\n",
    "        'zero_flow_threshold': 1e-4,  # threshold below which flow is indistinguishable from zero\n",
    "        'global_min_uar': global_min_uar,\n",
    "        'global_max_uar': global_max_uar,\n",
    "    }\n",
    "\n",
    "    context = FDCEstimationContext(**input_data)\n",
    "\n",
    "    processed = []\n",
    "    t0 = time()\n",
    "    process_fdcs = False\n",
    "    if process_fdcs:\n",
    "        print('Processing FDCs...')\n",
    "        for stn in daymet_concurrent_stations:\n",
    "            runner = FDCEstimatorRunner(stn, context, methods, k_nearest, target_cols, ESTIMATOR_CLASSES)\n",
    "            runner.run_selected()\n",
    "            processed.append(stn)\n",
    "            if len(processed) % 50 == 0:\n",
    "                t1 = time()\n",
    "                elapsed = t1 - t0\n",
    "                unit_time = elapsed / len(processed)\n",
    "                print(f'Processed {len(processed)}/{len(daymet_concurrent_stations)} stations in {unit_time:.2f} seconds per station')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c615cbf6",
   "metadata": {},
   "source": [
    "### Compute the complexity of searching the space of k-nearest neighbours for the optimal ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711879b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_search_space_heatmap(N_min=10, N_max=1000, N_step=10, k_max=10):# output_path=\"search_space_heatmap.html\"):\n",
    "    \"\"\"\n",
    "    Generate and save a Bokeh heatmap of log10(search space size) for ensembles of size 1 to k_max\n",
    "    from a network of size N, excluding the target node.\n",
    "\n",
    "    Parameters:\n",
    "    - N_min (int): Minimum network size.\n",
    "    - N_max (int): Maximum network size.\n",
    "    - N_step (int): Step between network sizes.\n",
    "    - k_max (int): Maximum ensemble size to consider.\n",
    "    - output_path (str): Path to save the HTML output.\n",
    "    \"\"\"\n",
    "    N_values = np.arange(N_min, N_max + 1, N_step)\n",
    "    k_values = np.arange(1, k_max + 1)\n",
    "\n",
    "    data = []\n",
    "    for N in N_values:\n",
    "        for k in k_values:\n",
    "            max_j = min(k, N - 1)\n",
    "            size = sum(comb(N - 1, j) for j in range(1, max_j + 1))\n",
    "            data.append((N, k, size))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['N', 'k', 'search_space'])\n",
    "    df['log_search_space'] = np.log10(df['search_space'].astype(float).replace(0, np.nan))\n",
    "\n",
    "    source = ColumnDataSource(df)\n",
    "\n",
    "    mapper = linear_cmap(field_name='log_search_space', palette=Viridis256,\n",
    "                         low=df['log_search_space'].min(), high=df['log_search_space'].max())\n",
    "\n",
    "    hover = HoverTool(\n",
    "        tooltips=[\n",
    "            (\"N\", \"@N\"),\n",
    "            (\"k\", \"@k\"),\n",
    "            (\"Search space\", \"@search_space{0,0}\"),\n",
    "            (\"log10(Search space)\", \"@log_search_space{0.00}\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    p = figure(title=\"Log10 of Search Space Size vs N and k\",\n",
    "               x_axis_label='Network size N',\n",
    "               y_axis_label='Ensemble size k',\n",
    "               x_range=(N_min, N_max), y_range=(1, k_max),\n",
    "               width=900, height=550,\n",
    "               tools=['pan', 'wheel_zoom', 'box_zoom', 'reset', hover])\n",
    "\n",
    "    p.rect(x=\"N\", y=\"k\", width=N_step, height=1, source=source,\n",
    "           line_color=None, fill_color=mapper)\n",
    "\n",
    "    color_bar = ColorBar(color_mapper=mapper['transform'], ticker=BasicTicker(),\n",
    "                         label_standoff=12, location=(0, 0), title='log10(search space)')\n",
    "    p.add_layout(color_bar, 'right')\n",
    "    return p\n",
    "# fig = plot_search_space_heatmap()\n",
    "# show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f369708-c149-47fc-b0a3-060c1deff995",
   "metadata": {},
   "source": [
    "## Check sensitivity of variance estimation to the selection of quantiles\n",
    "\n",
    "The Nash-Sutcliffe Efficiency is the RMSE ($\\frac{1}{N}\\sum(y-\\hat y)^2)$) normalized by the observed variance $\\sigma^2 = \\sum(y-\\mu_y)^2$. When the NSE is computed on timeseries observations, the variance at least represents the sample.  When the NSE is used to describe the accuracy of an FDC described on a subjective set of quantiles, i.e. $\\{1, 2, \\dots, 99\\}$, the mean is an approximation that assumes the data are normally distributed.  This is not often the case, so here we examine the sensitivity of this metric to the choice of percentiles.  \n",
    "\n",
    "In short, the NSE ($1 - \\frac{RMSE}{\\sigma^2}$), which is already sensitive to outliers, will be overestimated if $\\sigma^2$ is overestimated, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6262d54e-8589-45f2-a0e8-f4690e0e05f0",
   "metadata": {},
   "source": [
    "### Evaluate how biased a quantile-based spread estimate is at different resolutions\n",
    "\n",
    "Generate a dummy set of percentiles based on a wide range of number of points used to represent the FDC, and vary the inclusion of extremes.  In other words, vary the start end end points of the percentiles used to represent the FDC.  Including 0 and 100 in the set introduces the most structural uncertainty, and as we (symmetrically) exclude probability from the ends of the distribution, our estimate becomes less susceptible to extreme outlier errors based on the quadratic loss.  Likewise, having too many points (high number of quantiles near 0 and 100) has the same problem if some of the \"edge probability\" is not trimmed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcac79c",
   "metadata": {},
   "source": [
    "### Conventional Representation of FDCs\n",
    "\n",
    "In the literature, FDC estimation accuracy is computed in several different ways. One important difference among previous studies is the set of quantiles $\\{ p_j \\}$ over which residuals of the estimated and observed exceedance duration are evaluated.  The residuals are denoted $e_j = \\hat q(p_j) - q(p_j)$, where $\\hat q(p_j) = F^{-1}(p_j)$ are the estimated quantiles at exceedance percentiles $p_j$ and $q(p_j)$ are the observed quantiles.  The choice of probabilities reflects different research objectives in representing flow availability.\n",
    "\n",
    "* [0, 1, ..., 99, 100] {cite}`booker2014comparing`\n",
    "* [10, 20, ..., 80, 90] {cite}`li2010new`\n",
    "* [50, 51, ..., 99] {cite}`fennessey1990regional`\n",
    "\n",
    "The choice of exceedance durations to evaluate FDC estimates is determined on the basis of the precision and range of representation being appropriate for some application or question.  However the interpretation of the performance metric also depends upon this choice, in particular the statistical structure of the values being estimated.  A common metric between studies is the Nash Sutcliffe Efficiency (NSE) which divides the RMSE by the standard deviation.  Normalizing by the standard deviation implicitly assumes the residuals are normally distributed.  Alternatively, by the sample size (N) represents simply the mean, treating all residuals equally regardless of their distribution, except if the residuals are squared, then it emphasizes large values.  {cite}`booker2012comparing` and {cite}`booker2014comparing` (and others...) standardize the observed variable (unit area runoff) by dividing by the mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f171330-5dc8-4865-8fd6-34110d8fe8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentile_spread_convergence(obs, percentiles_list=None):\n",
    "    \"\"\"\n",
    "    Compare the standard deviation of observed data to that estimated from percentiles.\n",
    "\n",
    "    Parameters:\n",
    "    - obs: 1D array-like of observed values\n",
    "    - percentiles_list: list of lists/arrays of percentiles to compute (0–100).\n",
    "      If None, defaults to increasingly fine resolutions.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame with columns:\n",
    "        'num_percentiles', 'percentiles', 'spread_std', 'sample_std', 'relative_error'\n",
    "    \"\"\"\n",
    "    obs = np.asarray(obs)\n",
    "    sample_std = np.std(obs, ddof=0)\n",
    "    sample_mean = np.mean(obs)\n",
    "\n",
    "    if percentiles_list is None:\n",
    "        percentiles_list = [np.arange(10, 100, 10),\n",
    "                            np.arange(5, 100, 5),\n",
    "                            np.arange(2, 100, 2),\n",
    "                            np.arange(1, 100, 1),\n",
    "                           np.arange(0.1, 99.9, 0.1),\n",
    "                           np.arange(0.01, 99.99, 0.01), \n",
    "                           np.linspace(0, 100, 1000)]\n",
    "\n",
    "    results = []\n",
    "    for pct_set in percentiles_list:\n",
    "        q_vals = np.percentile(obs, pct_set)\n",
    "        spread_std = np.std(q_vals)\n",
    "        spread_mean = np.mean(q_vals)\n",
    "        rel_error = (spread_std - sample_std) / sample_std\n",
    "        mean_rel_error = (spread_mean - sample_mean) / sample_mean\n",
    "        results.append({\n",
    "            'num_percentiles': len(pct_set),\n",
    "            'percentiles': pct_set,\n",
    "            'spread_std': spread_std,\n",
    "            'sample_std': sample_std,\n",
    "            'relative_error': rel_error,\n",
    "            'mean_relative_error': mean_rel_error\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3313fcd-3ed8-4791-b395-e4cc8b808b8f",
   "metadata": {},
   "source": [
    "Note that this test represents a single site.  We are testing the \"precision of fdc representation\" and the \"sensitivity to extremes\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb4ec9f-e9e0-442a-91a4-a52f60267021",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id = '08MH147'\n",
    "target_da= stn_da_dict[target_id]\n",
    "test_data = StationData(context, target_id)\n",
    "\n",
    "# get the unique UAR values\n",
    "df =  test_data.hyd_df.copy()\n",
    "vals = df[f'uar'].dropna().values\n",
    "sdf = percentile_spread_convergence(vals)\n",
    "years = list(set(df.index.year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b401cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_positions(n: int, method: str = \"weibull\") -> np.ndarray:\n",
    "    \"\"\"Return p_k for k=1..n under a given plotting-position method.\"\"\"\n",
    "    k = np.arange(1, n + 1, dtype=float)\n",
    "    if method == \"weibull\":\n",
    "        p = k / (n + 1.0)\n",
    "    elif method == \"hazen\":\n",
    "        p = (k - 0.5) / n\n",
    "    elif method == \"gringorten\":\n",
    "        p = (k - 0.44) / (n + 0.12)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown method: %r\" % method)\n",
    "    # ensure open interval (0,1)\n",
    "    eps = 1e-12\n",
    "    return np.clip(p, eps, 1 - eps)\n",
    "\n",
    "\n",
    "def compute_entropy(pmf):\n",
    "    \"\"\"Compute the entropy of a discrete probability mass function.\"\"\"\n",
    "    pmf = np.asarray(pmf)\n",
    "    pmf = pmf[pmf > 0]  # Exclude zero probabilities to avoid log(0)\n",
    "    return np.sum(pmf * np.log2(1/pmf))\n",
    "\n",
    "\n",
    "def compute_empirical_cdf(data):\n",
    "    \"\"\"Compute the empirical CDF of the data.\"\"\"\n",
    "    sorted_data = np.sort(data)\n",
    "    n = len(data)\n",
    "    ecdf = np.arange(1, n + 1) / n\n",
    "    return sorted_data, ecdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9b4d0f",
   "metadata": {},
   "source": [
    "Compute the effect of sampling variability on the entropy of the estimated distribution.  This is done by bootstrapping the observed data and re-computing the entropy of the estimated distribution.  This is done for a range of quantile set sizes to evaluate how the choice of quantiles affects the sensitivity of the entropy estimate.  Choosing the number of quantiles represents a bias-variance tradeoff as fewer bins smooth out variance from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fe0d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_set = [6, 7, 8, 9, 10, 11]  # log2 of number of grid points\n",
    "# eps_set = [1e-4, 1e-3, 1e-2, 1e-1]  # edge trimming parameter for [eps, 1-eps]\n",
    "results = []\n",
    "context.bitrate = 8\n",
    "h_results_fpath = os.path.join('data', 'results', 'additional_results', f'entropy_estimates_bootstrapped_{bitrate:02d}_bits.csv')\n",
    "if os.path.exists(h_results_fpath):\n",
    "    h_df = pd.read_csv(h_results_fpath, index_col=0, dtype={'station_id': str})\n",
    "else:\n",
    "    for stn in daymet_concurrent_stations:\n",
    "\n",
    "        test_data = StationData(context, stn)\n",
    "\n",
    "        log_edges = test_data.log_edges_extended\n",
    "        \n",
    "        log_x = test_data.log_x_extended\n",
    "\n",
    "        max_H = np.log2(len(log_x))  # entropy of uniform distribution over the grid\n",
    "        assert max_H == bitrate, f'Expected max_H {max_H} to equal bitrate {bitrate}'\n",
    "\n",
    "        S = np.log(test_data.hyd_df[f'uar'].dropna().values)\n",
    "        # print(f\"Station {stn}: n={S.size} unique={len(np.unique(S))} min={S.min():.4g} max={S.max():.4g}\")\n",
    "        \n",
    "        # initialize a matrix to hold B boostrap samples of len(S) each\n",
    "        B = 1000\n",
    "        rng = np.random.default_rng(42)\n",
    "        idx = np.random.choice(S.size, size=(S.size, B), replace=True)    \n",
    "        S_b  = S[idx] # (n, B) bootstrap samples (column vectors of resampled data)\n",
    "\n",
    "        # for each sample, compute a histogram over log_edges\n",
    "        histograms = np.apply_along_axis(lambda x: np.histogram(x, bins=log_edges, density=True)[0], arr=S_b, axis=0)\n",
    "        # convert pdfs to pmfs\n",
    "        histograms *= test_data.log_w[:, None]  # (M, B) multiply by the bin width\n",
    "        H = np.apply_along_axis(compute_entropy, arr=histograms, axis=0) # B entropy values\n",
    "\n",
    "        # format and save results\n",
    "        results.append((stn, np.mean(H), *np.percentile(H, (2.5, 50, 95))))\n",
    "        h_df = pd.DataFrame(results, columns=['official_id', 'mean_entropy', 'p2.5', 'p50', 'p97.5'])\n",
    "        h_df = h_df.set_index('official_id').round(5)\n",
    "        # save the output to a csv file\n",
    "        h_df.to_csv(h_results_fpath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1410c582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each station, compute the 95% confidence interval as a percentage of the median\n",
    "h_df['dh_95'] = h_df['p97.5'] - h_df['p2.5']\n",
    "p = figure(title='', width=600, height=400, toolbar_location=None)\n",
    "p.scatter(h_df['mean_entropy'], 100 * h_df['dh_95'] / h_df['mean_entropy'], size=3, color='black', alpha=0.5)\n",
    "p.xaxis.axis_label = 'Mean Entropy H [bits]'\n",
    "p.yaxis.axis_label = '95% CI as % of Mean H [%]'\n",
    "p = dpf.format_fig_fonts(p, font_size=14)\n",
    "# save the figure to disk\n",
    "# dpf.save_fig(p, f'sampling_variability_{bitrate:02d}_bits_quantization')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a1133f",
   "metadata": {},
   "source": [
    "Test the sensitivity of the residual-based metrics to the choice of quantiles used to represent the FDC.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03a3b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_quantiles(sorted_x, B, p, rng):\n",
    "    \"\"\"\n",
    "    Bayesian/Dirichlet bootstrap via Exp(1) weights (no per-bootstrap sort).\n",
    "    Same interface/shape as multinomial version.\n",
    "    \"\"\"\n",
    "    n = sorted_x.size\n",
    "    w = rng.exponential(scale=1.0, size=(n, B))       # (n,B)\n",
    "    w /= w.sum(axis=0, keepdims=True)\n",
    "    cw = np.cumsum(w, axis=0)                          # (n,B)\n",
    "\n",
    "    P = p.size\n",
    "    qb = np.empty((P, B), dtype=sorted_x.dtype)\n",
    "    for b in range(B):\n",
    "        k1 = np.searchsorted(cw[:, b], p, side='left')\n",
    "        k1 = np.clip(k1, 0, n - 1)\n",
    "        k0 = np.clip(k1 - 1, 0, n - 1)\n",
    "\n",
    "        cw0 = np.where(k1 == 0, 0.0, cw[k0, b])\n",
    "        cw1 = cw[k1, b]\n",
    "        denom = np.maximum(cw1 - cw0, 1e-15)\n",
    "        t = (p - cw0) / denom\n",
    "\n",
    "        x0 = sorted_x[k0]\n",
    "        x1 = sorted_x[k1]\n",
    "        qb[:, b] = (1.0 - t) * x0 + t * x1\n",
    "    return qb\n",
    "\n",
    "\n",
    "def summarize_residuals(resid, ref, which_space, q=(2.5, 50, 97.5), eps=1e-12):\n",
    "    rmse = np.sqrt((resid**2).mean(0))\n",
    "    sse  = (resid**2).sum(0)\n",
    "    sst  = ((ref - ref.mean(0))**2).sum(0)\n",
    "    eff  = np.where(sst > 0, sse / sst, np.nan)\n",
    "\n",
    "    out = {\n",
    "        f'{which_space}_RMSE_mean': float(np.nanmean(rmse)),\n",
    "        f'{which_space}_EFF_mean':  float(np.nanmean(eff)),\n",
    "    }\n",
    "    for name, arr in (('RMSE', rmse), ('EFF', eff)):\n",
    "        prc = np.percentile(arr, q)\n",
    "        out.update({f'{which_space}_{name}_{qi}ci': float(v) for qi, v in zip(q, prc)})\n",
    "\n",
    "    if which_space == 'linear':\n",
    "        den = np.maximum(ref.sum(0), eps)\n",
    "        nae = 100 * np.abs(resid).sum(0) / den\n",
    "        out[f'linear_NAE_mean'] = float(np.nanmean(nae))\n",
    "        prc = np.percentile(nae, q)\n",
    "        out.update({f'linear_NAE_{qi}ci': float(v) for qi, v in zip(q, prc)})\n",
    "\n",
    "    elif which_space == 'log':\n",
    "        pct = np.exp(np.abs(resid)) - 1.0          # multiplicative error\n",
    "        mae_pct  = pct.mean(0) * 100.0\n",
    "        rmse_pct = np.sqrt((pct**2).mean(0)) * 100.0\n",
    "        out.update({'log_MAE%_mean': float(np.nanmean(mae_pct)),\n",
    "                    'log_RMSE%_mean': float(np.nanmean(rmse_pct))})\n",
    "        for name, arr in (('MAE%', mae_pct), ('RMSE%', rmse_pct)):\n",
    "            prc = np.percentile(arr, q)\n",
    "            out.update({f'log_{name}_{qi}ci': float(v) for qi, v in zip(q, prc)})\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd1b2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M = 9 # our quantization is to 2^10 = 1024 bins (states)\n",
    "eps = 1e-3\n",
    "B = 1000\n",
    "\n",
    "spaces = {\n",
    "    'log':    (lambda v: v,          'log-units'),      # additive in log\n",
    "    'linear': (np.exp,               'L/s/km²'),        # additive in linear units (volume meaning)\n",
    "}\n",
    "all_results_dict = {}\n",
    "# bit_test = [6, 7, 8, 9, 10]\n",
    "bit_test = [6, 8, 10]\n",
    "eps_test = [1e-3, 1e-2]\n",
    "\n",
    "input_data['bitrate'] = 8\n",
    "input_data['regularization_type'] = 'discrete'\n",
    "context = FDCEstimationContext(**input_data)\n",
    "for m in bit_test:\n",
    "    all_results_dict[m] = {e:{} for e in eps_test}\n",
    "    for eps in eps_test:\n",
    "        residuals_fpath = os.path.join('data', 'results', 'quantization_sensitivity', f'quantization_residuals_bootstrapped_{m}_bits_{str(eps)}_eps.csv')\n",
    "        if not os.path.exists(residuals_fpath):\n",
    "            residual_results = []\n",
    "            for stn in daymet_concurrent_stations:                \n",
    "                test_data = StationData(context, stn)\n",
    "                S = np.log(test_data.hyd_df[f'{stn}_uar_clipped'].dropna().values)\n",
    "                target_da= stn_da_dict[stn]\n",
    "\n",
    "                sorted_x = np.sort(S)\n",
    "                p_quantized = np.linspace(eps, 1.0-eps, 2**M)\n",
    "                interpolated_log_x = np.quantile(S, p_quantized, method='inverted_cdf')\n",
    "\n",
    "                # create a bootstrap sample of the observed data\n",
    "                # interpolate the quantization positions to find the corresponding log_x values\n",
    "                # compute the quantization in log space and express as pct residuals\n",
    "                \n",
    "                quantized_log_x = bootstrap_quantiles(sorted_x, B, p_quantized, np.random.default_rng(42))\n",
    "                log_residuals = interpolated_log_x[:, None] - quantized_log_x  # (P, B)\n",
    "                \n",
    "                results_by_space = {'official_id': stn}\n",
    "                for space_name, (to_space, units_label) in spaces.items():\n",
    "                    ref_space  = to_space(interpolated_log_x)           # (P,)\n",
    "                    boot_space = to_space(quantized_log_x)              # (P, B)\n",
    "                    resid = ref_space[:, None] - boot_space             # (P, B)\n",
    "\n",
    "                    stats = summarize_residuals(resid, boot_space, space_name)\n",
    "                    stats[space_name] = units_label\n",
    "                    results_by_space.update(stats)\n",
    "\n",
    "                    # concise printout (97.5% only)\n",
    "                    nae_m, nae_hi = None, None\n",
    "                    if 'log' not in space_name:\n",
    "                        nae_m, nae_hi = round(stats[f'{space_name}_NAE_mean'], 2),  round(stats[f'{space_name}_NAE_97.5ci'], 2)\n",
    "                    rmse_m, rmse_hi= stats[f'{space_name}_RMSE_mean'], stats[f'{space_name}_RMSE_97.5ci']\n",
    "                    eff_m, eff_hi = stats[f'{space_name}_EFF_mean'],  stats[f'{space_name}_EFF_97.5ci']\n",
    "\n",
    "                    if len(residual_results) % 50 == 0:\n",
    "                        print(f\"    {len(residual_results)}/{len(daymet_concurrent_stations)} {stn} {space_name:6s}  NAE={nae_m} ({nae_hi})  RMSE={rmse_m:.3g} ({rmse_hi:.3g})  1-NSE={eff_m:.3g} ({eff_hi:.3g})\")        \n",
    "                \n",
    "                residual_results.append(results_by_space)\n",
    "                # compute the absolute error between quantized_x and the actual sorted_x values at the positions\n",
    "            results_df = pd.DataFrame(residual_results)\n",
    "            results_df = results_df.set_index('official_id').round(4)\n",
    "            results_df.head()\n",
    "            # save results to a csv file\n",
    "            results_df.to_csv(residuals_fpath)\n",
    "            # add to the all_results_dict\n",
    "            all_results_dict[m][eps] = results_df\n",
    "        else:\n",
    "            print(f'   Loading existing results from {residuals_fpath}')\n",
    "            results_df = pd.read_csv(residuals_fpath, index_col=0, dtype={'official_id': str})\n",
    "            all_results_dict[m][eps] = results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab07e778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from matplotlib.patches import Patch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b39fb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quant_scatter_agreement_fixed(all_results_dict,\n",
    "                                  which_space='linear',     # 'linear' or 'log'\n",
    "                                  center='median',          # 'median' or 'mean'\n",
    "                                  x_col=None,               # optional single-sample column\n",
    "                                  figsize_per_panel=(4.8, 4.0),\n",
    "                                  alpha=0.75):\n",
    "    \"\"\"\n",
    "    One subplot per metric. x = single-sample estimate (if given) else center,\n",
    "    y = bootstrap center, vertical whiskers = 95% CI. Uses only M=10 and eps=0.001.\n",
    "    \"\"\"\n",
    "    # fixed selection\n",
    "    m, eps = 8, 0.001\n",
    "    print(all_results_dict.keys())\n",
    "    df = all_results_dict[m][eps]\n",
    "\n",
    "    # metrics per space\n",
    "    metrics = ['NAE', 'RMSE', 'EFF'] if which_space == 'linear' else ['RMSE', 'EFF']\n",
    "    labels = {'NAE':'NAE [%]', 'RMSE':'RMSE [%]', 'EFF':'1 − NSE [-]'}\n",
    "\n",
    "    fig, axs = plt.subplots(1, len(metrics),\n",
    "                            figsize=(figsize_per_panel[0]*len(metrics), figsize_per_panel[1]),\n",
    "                            squeeze=False)\n",
    "    axs = axs.ravel()\n",
    "\n",
    "    # collect global limits (so panels are comparable)\n",
    "    gx_min = np.inf; gx_max = -np.inf\n",
    "    gy_min = np.inf; gy_max = -np.inf\n",
    "    cache = {}\n",
    "    xranges = {}\n",
    "    for metric in metrics:\n",
    "        base = f\"{which_space}_{metric}\"\n",
    "        y_col  = f\"{base}_{center}\" if f\"{base}_{center}\" in df.columns else f\"{base}_mean\"\n",
    "        lo_col = f\"{base}_2.5ci\"\n",
    "        hi_col = f\"{base}_97.5ci\"\n",
    "\n",
    "        if y_col not in df.columns:\n",
    "            cache[metric] = None\n",
    "            continue\n",
    "\n",
    "        y  = df[y_col].to_numpy(float)\n",
    "        x  = (df[x_col].to_numpy(float) if x_col and x_col in df.columns else y.copy())\n",
    "        if lo_col in df.columns and hi_col in df.columns:\n",
    "            ylo = df[lo_col].to_numpy(float)\n",
    "            yhi = df[hi_col].to_numpy(float)\n",
    "        elif hi_col in df.columns:\n",
    "            yhi = df[hi_col].to_numpy(float)\n",
    "            ylo = 2*y - yhi  # symmetric fallback\n",
    "        else:\n",
    "            ylo = y.copy(); yhi = y.copy()\n",
    "\n",
    "        mask = np.isfinite(x) & np.isfinite(y) & np.isfinite(ylo) & np.isfinite(yhi)\n",
    "        x, y, ylo, yhi = x[mask], y[mask], ylo[mask], yhi[mask]\n",
    "\n",
    "        cache[metric] = (x, y, ylo, yhi, min(x), max(x))\n",
    "\n",
    "    # plot\n",
    "    for ax, metric in zip(axs, metrics):\n",
    "        entry = cache.get(metric)\n",
    "        if not entry: continue\n",
    "        x, y, ylo, yhi, gx_min, gx_max = entry\n",
    "        yerr = np.vstack([y - ylo, yhi - y])\n",
    "\n",
    "        ax.errorbar(x, y, yerr=yerr, fmt='o', ms=3, alpha=alpha,\n",
    "                    ecolor='0.25', elinewidth=0.8, color='0.15', lw=0)\n",
    "\n",
    "        # identity line\n",
    "        lo = min(gx_min, gy_min); hi = max(gx_max, gy_max)\n",
    "        ax.plot([lo, hi], [lo, hi], ls='--', lw=1, color='0.4', label='1:1 line')\n",
    "\n",
    "        ax.set_xlabel(f\"Single-sample (baseline) {labels[metric]}\")\n",
    "        ax.set_ylabel(f\"Bootstrap {labels[metric]} (95% CI)\")\n",
    "        ax.set_xlim(0, gx_max)\n",
    "        print(metric)\n",
    "        if metric == 'EFF':\n",
    "            ax.set_xlim(0, 0.04)\n",
    "            ax.set_ylim(-0.01, 0.3)\n",
    "\n",
    "        xs = np.linspace(np.nanmin(x), np.nanmax(x), 200)\n",
    "        ax.fill_between(xs, 0.5*xs, 2.0*xs,\n",
    "                        color='grey', alpha=0.4, label='±100% region', zorder=0)\n",
    "\n",
    "        # clean look\n",
    "        ax.spines['top'].set_visible(False); ax.spines['right'].set_visible(False)\n",
    "        ax.grid(True, color='0.85', lw=0.8, zorder = 1)\n",
    "        ax.legend(loc='upper left', frameon=False)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig, axs\n",
    "\n",
    "# fig, axs = quant_scatter_agreement_fixed(all_results_dict,\n",
    "#                                          which_space='linear',\n",
    "#                                          center='median',\n",
    "#                                          x_col=None)\n",
    "# save the figure to disk\n",
    "# fig.savefig(os.path.join('images', f'quantization_sensitivity_baseline_vs_bootstrapp_{bitrate}_bits.png'), dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8deaea6",
   "metadata": {},
   "source": [
    "### Examine model sensitivity to sampling variability\n",
    "\n",
    "1.  **Log-normal**: the log normal model does not generate daily time series.  We have 20 iterations of the xgboost model that yield 20 sets of predicted statistics, and thus 20 sets of log-normal parameters.  We can compute the FDC for each set of parameters and evaluate the variability of scores $S$ over each metric compared to the reference FDC.  but the reference FDC is also estimated from a finite sample, so we should also consider the variability of the reference FDC. A double-bootstrap is a bit much..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63009a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the predicted mean uar and std for all 20 trials\n",
    "predicted_param_trials_folder = Path('data/results/parameter_prediction_results')\n",
    "predicted_mean_trials_fname = 'log_uar_mean_summary_group_3.csv'\n",
    "predicted_std_trials_fname = 'log_uar_std_summary_group_3.csv'\n",
    "predicted_lin_mean_trials_fname = 'uar_mean_summary_group_3.csv'\n",
    "predicted_lin_std_trials_fname = 'uar_std_summary_group_3.csv'\n",
    "\n",
    "predicted_mean_df = pd.read_csv(predicted_param_trials_folder / predicted_mean_trials_fname, dtype={'official_id': str})\n",
    "predicted_std_df  = pd.read_csv(predicted_param_trials_folder / predicted_std_trials_fname, dtype={'official_id': str})\n",
    "predicted_lin_mean_df = pd.read_csv(predicted_param_trials_folder / predicted_lin_mean_trials_fname, dtype={'official_id': str})\n",
    "predicted_lin_std_df  = pd.read_csv(predicted_param_trials_folder / predicted_lin_std_trials_fname, dtype={'official_id': str})\n",
    "\n",
    "pred_mean_dict = predicted_mean_df.groupby(\"official_id\")[\"predicted\"].apply(list).to_dict()\n",
    "pred_std_dict  = predicted_std_df.groupby(\"official_id\")[\"predicted\"].apply(list).to_dict()\n",
    "actual_mean_dict = predicted_mean_df.groupby('official_id')['actual'].apply(list).to_dict()\n",
    "actual_std_dict  = predicted_std_df.groupby('official_id')['actual'].apply(list).to_dict()\n",
    "predicted_lin_mean_dict = predicted_lin_mean_df.groupby(\"official_id\")[\"predicted\"].apply(list).to_dict()\n",
    "predicted_lin_std_dict  = predicted_lin_std_df.groupby(\"official_id\")[\"predicted\"].apply(list).to_dict()\n",
    "actual_lin_mean_dict = predicted_lin_mean_df.groupby('official_id')['actual'].apply(list).to_dict()\n",
    "actual_lin_std_dict  = predicted_lin_std_df.groupby('official_id')['actual'].apply(list).to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e30870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.parametric_estimator import ParametricFDCEstimator\n",
    "from utils.evaluation_metrics import EvaluationMetrics\n",
    "import multiprocessing as mp\n",
    "\n",
    "input_data['bitrate'] = 8\n",
    "input_data['regularization_type'] = 'discrete'\n",
    "context = FDCEstimationContext(**input_data)\n",
    "\n",
    "output_fpath = os.path.join('data', 'results', 'quantization_sensitivity', f'quantization_sensitivity_test_parametric_{bitrate}_bits.csv')\n",
    "if os.path.exists(output_fpath):\n",
    "    output_df = pd.read_csv(output_fpath, dtype={'official_id': str}, index_col=0)\n",
    "    print(output_df.head())\n",
    "else:\n",
    "    all_trials = []\n",
    "    for stn in daymet_concurrent_stations:\n",
    "        \n",
    "        # get the target data\n",
    "        test_data = StationData(context, stn)\n",
    "        test_data.eval_metrics = EvaluationMetrics(log_x=test_data.log_x_extended, bitrate=bitrate, min_measurable_log_uar=test_data.min_measurable_log_uar)\n",
    "        S = np.log(test_data.hyd_df[f'{stn}_uar'].dropna().values)\n",
    "        target_da= stn_da_dict[stn]\n",
    "        \n",
    "        # manually set the baseline observed pmf to the observed instead of the default kde fit\n",
    "        test_data.baseline_pmf = test_data.baseline_pmf_df[stn].values\n",
    "        assert len(test_data.baseline_pmf) == len(test_data.log_x_extended) == 2**bitrate, f'{2**bitrate} expected but got {len(test_data.baseline_pmf)}'\n",
    "        # test_data.baseline_pdf = test_data.baseline_pdf_df[stn].values\n",
    "\n",
    "        # get the parametric estimates\n",
    "        lognorm_estimator = ParametricFDCEstimator(context, test_data)\n",
    "        lognorm_estimator.include_random_test = False        \n",
    "\n",
    "        predicted_mu_vals = pred_mean_dict[stn]\n",
    "        predicted_sigma_vals = pred_std_dict[stn]\n",
    "        actual_mu = actual_mean_dict[stn]\n",
    "        actual_sigma = actual_std_dict[stn]\n",
    "        predicted_lin_mu_vals = predicted_lin_mean_dict[stn]\n",
    "        predicted_lin_sigma_vals = predicted_lin_std_dict[stn]\n",
    "        actual_lin_mu = actual_lin_mean_dict[stn]\n",
    "        actual_lin_sigma = actual_lin_std_dict[stn]\n",
    "\n",
    "        for mu, sigma, act_mu, act_sigma, lin_mu, lin_sigma, lin_act_mu, lin_act_sigma in zip(\n",
    "            predicted_mu_vals, predicted_sigma_vals, actual_mu, actual_sigma,\n",
    "            predicted_lin_mu_vals, predicted_lin_sigma_vals, actual_lin_mu, actual_lin_sigma\n",
    "        ):\n",
    "            lognorm_estimator.predicted_param_dict = {\n",
    "            stn: {\n",
    "                'log_uar_mean_predicted': mu,\n",
    "                'log_uar_std_predicted': sigma,\n",
    "                'log_uar_mean_actual': act_mu,\n",
    "                'log_uar_std_actual': act_sigma,\n",
    "                'uar_mean_predicted': lin_mu,\n",
    "                'uar_std_predicted': lin_sigma,\n",
    "                'uar_mean_actual': lin_act_mu,\n",
    "                'uar_std_actual': lin_act_sigma,\n",
    "                'mu_random': np.random.choice(actual_mu),\n",
    "                'sigma_random': np.random.choice(actual_sigma),\n",
    "                }\n",
    "            }\n",
    "            result = lognorm_estimator.run_estimators()\n",
    "            for k, r in result.items():\n",
    "                if k == 'MLE':\n",
    "                    continue\n",
    "                test_result = r['eval']\n",
    "                result_df = pd.DataFrame(test_result, index=[k])\n",
    "                result_df['official_id'] = stn\n",
    "                all_trials.append(result_df)\n",
    "                if len(all_trials) % (20 * 50) == 0:\n",
    "                    print(f'   Processed {len(all_trials)/20:.0f}/{len(daymet_concurrent_stations)} trials')\n",
    "    \n",
    "    output_df = pd.concat(all_trials)\n",
    "    output_df.to_csv(output_fpath, index=True)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a7b677",
   "metadata": {},
   "source": [
    "Test performance on B=1000 median values computed by bootstrap resampling of the predicted parameters to test sensitivity to sampling variability of the actual process used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9285fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bootstrap_median_ci(values, B=1000, alpha=0.05, rng=None):\n",
    "    \"\"\"Bootstrap CI for the median of `values`.\"\"\"\n",
    "    values = np.asarray(values)\n",
    "    n = len(values)\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    boot_medians = np.empty(B, dtype=float)\n",
    "    for b in range(B):\n",
    "        idx = rng.integers(0, n, size=n)\n",
    "        boot_medians[b] = np.median(values[idx])\n",
    "\n",
    "    median = np.median(values)\n",
    "    lower = np.quantile(boot_medians, alpha / 2.0)\n",
    "    upper = np.quantile(boot_medians, 1.0 - alpha / 2.0)\n",
    "    return median, lower, upper\n",
    "\n",
    "\n",
    "B = 1000\n",
    "\n",
    "res_dict = {0: 'median', 1: 'lb', 2: 'ub'}\n",
    "\n",
    "input_data['bitrate'] = 8\n",
    "input_data['regularization_type'] = 'discrete'\n",
    "context = FDCEstimationContext(**input_data)\n",
    "\n",
    "output_fpath = os.path.join('data', 'results', 'quantization_sensitivity', f'parametric_bootstrap_median_sensitivity_{input_data[\"bitrate\"]}_bits.csv')\n",
    "if os.path.exists(output_fpath):\n",
    "    output_df = pd.read_csv(output_fpath, dtype={'official_id': str}, index_col=0)\n",
    "    print(output_df.head())\n",
    "else:\n",
    "    all_trials = []\n",
    "    for stn in daymet_concurrent_stations:\n",
    "        \n",
    "        # get the target data\n",
    "        test_data = StationData(context, stn)\n",
    "        test_data.eval_metrics = EvaluationMetrics(log_x=test_data.log_x_extended, bitrate=bitrate, min_measurable_log_uar=test_data.min_measurable_log_uar)\n",
    "        S = np.log(test_data.hyd_df[f'{stn}_uar'].dropna().values)\n",
    "        target_da= stn_da_dict[stn]\n",
    "        \n",
    "        # manually set the baseline observed pmf to the observed instead of the default kde fit\n",
    "        test_data.baseline_pmf = test_data.baseline_pmf_df[stn].values\n",
    "        assert len(test_data.baseline_pmf) == len(test_data.log_x_extended) == 2**bitrate, f'{2**bitrate} expected but got {len(test_data.baseline_pmf)}'\n",
    "        # test_data.baseline_pdf = test_data.baseline_pdf_df[stn].values\n",
    "\n",
    "        # get the parametric estimates\n",
    "        lognorm_estimator = ParametricFDCEstimator(context, test_data)\n",
    "        lognorm_estimator.include_random_test = False\n",
    "\n",
    "        predicted_mu_vals = bootstrap_median_ci(pred_mean_dict[stn])\n",
    "        predicted_sigma_vals = bootstrap_median_ci(pred_std_dict[stn])\n",
    "        actual_mu = bootstrap_median_ci(actual_mean_dict[stn])\n",
    "        actual_sigma = bootstrap_median_ci(actual_std_dict[stn])\n",
    "        predicted_lin_mu_vals = bootstrap_median_ci(predicted_lin_mean_dict[stn])\n",
    "        predicted_lin_sigma_vals = bootstrap_median_ci(predicted_lin_std_dict[stn])\n",
    "        actual_lin_mu = bootstrap_median_ci(actual_lin_mean_dict[stn])\n",
    "        actual_lin_sigma = bootstrap_median_ci(actual_lin_std_dict[stn])\n",
    "\n",
    "\n",
    "        for mu, sigma, act_mu, act_sigma, lin_mu, lin_sigma, lin_act_mu, lin_act_sigma in zip(\n",
    "            predicted_mu_vals, predicted_sigma_vals, actual_mu, actual_sigma,\n",
    "            predicted_lin_mu_vals, predicted_lin_sigma_vals, actual_lin_mu, actual_lin_sigma\n",
    "        ):\n",
    "            lognorm_estimator.predicted_param_dict = {\n",
    "            stn: {\n",
    "                'log_uar_mean_predicted': mu,\n",
    "                'log_uar_std_predicted': sigma,\n",
    "                'log_uar_mean_actual': act_mu,\n",
    "                'log_uar_std_actual': act_sigma,\n",
    "                'uar_mean_predicted': lin_mu,\n",
    "                'uar_std_predicted': lin_sigma,\n",
    "                'uar_mean_actual': lin_act_mu,\n",
    "                'uar_std_actual': lin_act_sigma,\n",
    "                'mu_random': np.random.choice(actual_mu),\n",
    "                'sigma_random': np.random.choice(actual_sigma),\n",
    "                }\n",
    "            }\n",
    "            result = lognorm_estimator.run_estimators()\n",
    "            i = 0\n",
    "            for k, r in result.items():\n",
    "                if k == 'MLE':\n",
    "                    continue\n",
    "                test_result = r['eval']\n",
    "                result_df = pd.DataFrame(test_result, index=[k])\n",
    "                result_df['official_id'] = stn\n",
    "                result_df['label'] = res_dict[i]\n",
    "                all_trials.append(result_df)\n",
    "                if len(all_trials) % (20 * 50) == 0:\n",
    "                    print(f'   Processed {len(all_trials)/20:.0f}/{len(daymet_concurrent_stations)} trials')\n",
    "                i += 1\n",
    "    \n",
    "    output_df = pd.concat(all_trials)\n",
    "    output_df.to_csv(output_fpath, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce17949",
   "metadata": {},
   "source": [
    "Now import the LSTM simulation ensemble and test results on individual simulated outputs.  These are also somewhat limited since there are only 20 independent simulations.\n",
    "\n",
    "We already use the ensemble mixture distribution of all simulations which should address some of the variability, however this is based on a limited sample.   So we test the variability of scores by bootstrap resampling the 20 simulations with replacement to generate a larger sample of FDC estimates with which to generate the the ensemble mixture.  This approach should express sensitivity of the ensemble mixture to sampling variability given a small sample of simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d3e1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.LSTM_estimator import LSTMFDCEstimator\n",
    "from utils.evaluation_metrics import EvaluationMetrics\n",
    "import multiprocessing as mp\n",
    "\n",
    "B = 200\n",
    "input_data['bitrate'] = 8\n",
    "input_data['regularization_type'] = 'discrete'\n",
    "context = FDCEstimationContext(**input_data)\n",
    "\n",
    "output_fpath = os.path.join('data', 'results', 'quantization_sensitivity', f'quantization_sensitivity_test_LSTM_{bitrate}_bits.csv')\n",
    "if os.path.exists(output_fpath):\n",
    "    lstm_test_df = pd.read_csv(output_fpath, dtype={'official_id': str})\n",
    "else:\n",
    "    all_trials = []\n",
    "    for stn in daymet_concurrent_stations:\n",
    "        \n",
    "        # get the target data\n",
    "        test_data = StationData(context, stn)\n",
    "        test_data.eval_metrics = EvaluationMetrics(data=test_data, bitrate=bitrate)\n",
    "        S = np.log(test_data.hyd_df[f'{stn}_uar'].dropna().values)\n",
    "        target_da= stn_da_dict[stn]\n",
    "        # manually set the baseline pmf to the observed empirical, not the kde fit\n",
    "        # baseline_pmf = test_data.baseline_pmf\n",
    "        test_data.baseline_pmf = test_data.baseline_pmf_df[stn].values\n",
    "        # test_data.baseline_pdf = test_data.baseline_pdf_df[stn].values\n",
    "\n",
    "        # get the parametric estimates\n",
    "        lstm_estimator = LSTMFDCEstimator(context, test_data)\n",
    "        \n",
    "        sim_df = lstm_estimator.df.copy()\n",
    "\n",
    "        sim_cols = [c for c in sim_df.columns if c.startswith('streamflow_sim_')]\n",
    "        # set up bootstrap resampling of the sim_cols to test sampling variability of the LSTM ensemble\n",
    "        rng = np.random.default_rng(42)\n",
    "        sim_col_sets = [rng.choice(sim_cols, size=len(sim_cols), replace=True) for _ in range(B)]\n",
    "        for i, simcol_set in enumerate(sim_col_sets):\n",
    "            pmf = lstm_estimator._compute_ensemble_pmf_by_bincount(simcol_set, sim_df, context.bitrate)\n",
    "            prior_adjusted_pmf = test_data._compute_adjusted_distribution_with_mixed_uniform(pmf)\n",
    "\n",
    "            result = test_data.eval_metrics._evaluate_fdc_metrics_from_pmf(prior_adjusted_pmf, test_data.baseline_pmf)\n",
    "            result['official_id'] = stn\n",
    "            all_trials.append(result)\n",
    "            \n",
    "        if len(all_trials) % (200 * 20 * 50) == 0:\n",
    "            print(f'   Processed {len(all_trials)/20:.0f}/{len(daymet_concurrent_stations)} trials')\n",
    "\n",
    "    lstm_test_df = pd.DataFrame(all_trials)\n",
    "    lstm_test_df.to_csv(output_fpath, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cedcdab",
   "metadata": {},
   "source": [
    "Now test on one kNN model.  We can pick a single model variant.  Perhaps based on a bounding condition, like which model should be most sensitive -- 1 nearest neighbour will be most sensitive. Pick nearest neighbour in attribute space.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc8f147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.knn_estimator import kNNEstimator\n",
    "from utils.evaluation_metrics import EvaluationMetrics\n",
    "import multiprocessing as mp\n",
    "\n",
    "B = 200\n",
    "delta_values = [1e-12, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2]\n",
    "\n",
    "output_fpath = os.path.join('data', 'results', 'quantization_sensitivity', f'quantization_sensitivity_test_knn_{bitrate}_bits.csv')\n",
    "\n",
    "input_data['bitrate'] = 8\n",
    "input_data['regularization_type'] = 'discrete'\n",
    "context = FDCEstimationContext(**input_data)\n",
    "\n",
    "if os.path.exists(output_fpath):\n",
    "    knn_result_df = pd.read_csv(output_fpath, dtype={'official_id': str})\n",
    "else:\n",
    "    all_trials = []\n",
    "    for nn, stn in enumerate(daymet_concurrent_stations):\n",
    "        print(f'    Processing station {nn+1}/{len(daymet_concurrent_stations)}: {stn}')\n",
    "        \n",
    "        # get the target data\n",
    "        test_data = StationData(context, stn)\n",
    "        test_data.eval_metrics = EvaluationMetrics(data=test_data, bitrate=bitrate)\n",
    "        test_data.k_nearest = 1\n",
    "        S = np.log(test_data.hyd_df[f'{stn}_uar'].dropna().values)\n",
    "        target_da= stn_da_dict[stn]\n",
    "        test_data.baseline_pmf = test_data.baseline_pmf_df[stn].values\n",
    "        # test_data.baseline_pdf = test_data.baseline_pdf_df[stn].values\n",
    "\n",
    "        # get the parametric estimates\n",
    "        knn_estimator = kNNEstimator(context, test_data)\n",
    "        # knn_estimator.baseline_pmf_type = 'obs' # use the observed or kde approximated pmf as the baseline\n",
    "        knn_estimator.k_nearest = 4\n",
    "        knn_estimator._initialize_nearest_neighbour_data()\n",
    "        knn_estimator.weight_schemes = [2] # only look at inverse square distance weighting\n",
    "\n",
    "        nbr_df = knn_estimator.nbr_dfs['attribute_dist']['nbr_df'].copy()\n",
    "        nbr_data = knn_estimator.nbr_dfs['attribute_dist']['nbr_data'].copy()\n",
    "        # 2 is for weighting by inverse square distance\n",
    "        # set a range of deltas to test the sensitivity to the uniform mixture\n",
    "        \n",
    "        for d in delta_values:\n",
    "            test_data.delta = d\n",
    "            knn_estimator._compute_ensemble_distributions(2, nbr_df, nbr_data, 'attribute_dist')\n",
    "            sim_data = knn_estimator.knn_simulation_data\n",
    "            for c in sim_data.keys():\n",
    "                result = sim_data[c]['eval']\n",
    "                result['official_id'] = stn\n",
    "                result['delta'] = d\n",
    "                result['k'] = sim_data[c]['k']\n",
    "                result['weight_scheme'] = 2\n",
    "                all_trials.append(result)\n",
    "    knn_result_df = pd.DataFrame(all_trials)\n",
    "    knn_result_df.to_csv(output_fpath, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599d208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_dict = {\n",
    "    'pct_vol_bias': (-200, 50),\n",
    "    've': (0, 150),\n",
    "    'rmse': (0, 400),\n",
    "    'nse': (0, 2),\n",
    "    'kld': (0, 3)\n",
    "}\n",
    "label_dict = {\n",
    "    'pct_vol_bias': 'PB [%]',\n",
    "    've': 'NAE [%]',\n",
    "    'rmse': 'RMSE [%]',\n",
    "    'nse': '1 - NSE [-]',\n",
    "    'kld': 'KLD [bits/sample]'\n",
    "}\n",
    "from bokeh.palettes import Category10, Sunset10, Bokeh4, Colorblind5, Colorblind8\n",
    "palette = Sunset10\n",
    "palette = Colorblind8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18a065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_vals = [1e-12, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2]\n",
    "d_vals = [1e-12, 1e-5,  1e-4,  1e-3,1e-2, 5e-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fabed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import Range1d\n",
    "bitrate = 8\n",
    "\n",
    "knn_data = {}\n",
    "plots = []\n",
    "k_set = sorted(list(set(knn_result_df['k'].values)))\n",
    "k_set = [1, 2, 4]\n",
    "del_df = pd.DataFrame()\n",
    "for k in k_set:\n",
    "\n",
    "    model_output_df = knn_result_df[knn_result_df['k'] == k].copy()\n",
    "    \n",
    "    knn_data[str(k)] = {}\n",
    "    for m in label_dict.keys():\n",
    "        if m == 'pct_vol_bias':\n",
    "            continue\n",
    "        p = figure(title=f'{k}NN', width=600, height=400,\n",
    "                x_axis_type='log')\n",
    "        \n",
    "        m_df = pd.DataFrame()     \n",
    "        nn = 0\n",
    "        for i, d in enumerate(d_vals):#enumerate(sorted(list(set(model_output_df['delta'].values)))):\n",
    "            nn += 1 if nn == 2 else 0\n",
    "\n",
    "            if d <= 1e-10:\n",
    "                continue\n",
    "            # elif d == 1e-3:\n",
    "            #     d = 1e-12\n",
    "\n",
    "            by_station = model_output_df[model_output_df['delta'] == d].copy()\n",
    "\n",
    "            del_df[f'{k}NN_d{d}_{m}'] = by_station[m].values\n",
    "                        \n",
    "            # max_pt = 1\n",
    "            # N = len(m_df)\n",
    "            values = by_station[m].values\n",
    "            \n",
    "            label = m.upper()\n",
    "            xmin, xmax = 0.1, 10\n",
    "            if m == 've':\n",
    "                label = 'NAE [%]'\n",
    "                values = 100 * (1 - values) # express as %, 0 is perfect        \n",
    "                xmin, xmax = 1, 200\n",
    "            elif m == 'pct_vol_bias':\n",
    "                label = 'PB [%]'\n",
    "                xmin, xmax = 1, 200\n",
    "            elif m == 'nse':\n",
    "                label = ' 1 - NSE [-]'\n",
    "                values = 1 - values         # 1-NSE, 0 is perfect\n",
    "                # sort by median again\n",
    "                xmin, xmax = 0.0001, 10\n",
    "            elif m == 'rmse':\n",
    "                label = 'RMSE [%]'\n",
    "                values = 100*(np.exp(values) - 1)       # express as %, 0 is perfect\n",
    "                xmin, xmax = 1, 1000\n",
    "            elif m == 'kld':\n",
    "                label = 'KLD [bits/sample]'\n",
    "                xmin, xmax = 0.001, 10\n",
    "\n",
    "            m_df[str(d)] = sorted(values)\n",
    "            m_df['y'] = (np.arange(len(m_df)) + 0.5) / len(m_df)\n",
    "            knn_data[str(k)][str(d)] = ColumnDataSource(m_df)\n",
    "            # CI bars aligned to each median’s ECDF position\n",
    "\n",
    "            if len(plots) == 3:\n",
    "                p.legend.location = 'top_left'\n",
    "                p.legend.click_policy=\"hide\"\n",
    "                p.legend.background_fill_alpha = 0.5\n",
    "                p.line(str(d), 'y', source=knn_data[str(k)][str(d)], line_width=2, color=palette[nn], legend_label=f'δ={d}')\n",
    "            else:\n",
    "                p.line(str(d), 'y', source=knn_data[str(k)][str(d)], line_width=2, color=palette[nn])\n",
    "            nn += 1       \n",
    "        # set the xrange\n",
    "        p.x_range = Range1d(xmin, xmax)\n",
    "\n",
    "        p = dpf.format_fig_fonts(p, font_size=13)\n",
    "        if len(plots) >= 8:\n",
    "            p.xaxis.axis_label = label\n",
    "        if len(plots) % 4 == 0:\n",
    "            p.yaxis.axis_label = 'Cumulative Density'\n",
    "        plots.append(p)\n",
    "        \n",
    "lt = gridplot(plots, ncols=4, width=275, height=275, toolbar_location=None)\n",
    "fname = f'knn_delta_mixture_uncertainty_{bitrate}_bits'\n",
    "# dpf.save_fig(lt, fname)\n",
    "show(lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07228f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in del_df.columns:\n",
    "    vals = del_df[c].values\n",
    "    if c.endswith('kld'):\n",
    "        print(f'{c}: mean={np.mean(vals):.4f}, std={np.std(vals):.4f} min={np.min(vals):.4f}, max={np.max(vals):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe9f5e4",
   "metadata": {},
   "source": [
    "Test the sensitivity of FDCs predicted by kNN to sampling variability on the observations informing the reference distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1391def",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "B = 100\n",
    "input_data['delta'] = 0.001\n",
    "\n",
    "bitrate = 8\n",
    "input_data['bitrate'] = bitrate\n",
    "input_data['regularization_type'] = 'discrete'\n",
    "context = FDCEstimationContext(**input_data)\n",
    "\n",
    "output_fpath = os.path.join('data', 'results', 'quantization_sensitivity', f'quantization_sensitivity_test_knn_bootstrap_P_{bitrate}_bits.csv')\n",
    "\n",
    "if os.path.exists(output_fpath):\n",
    "    knn_bootstrap_df = pd.read_csv(output_fpath, dtype={'official_id': str})\n",
    "else:\n",
    "    all_trials = []\n",
    "    for nn, stn in enumerate(daymet_concurrent_stations):\n",
    "        print(f'    Processing station {nn+1}/{len(daymet_concurrent_stations)}: {stn}')\n",
    "        \n",
    "        # get the target data\n",
    "        test_data = StationData(context, stn)\n",
    "        test_data.eval_metrics = EvaluationMetrics(data=test_data, bitrate=bitrate)\n",
    "\n",
    "        test_data.k_nearest = 1 \n",
    "        target_da= stn_da_dict[stn]\n",
    "\n",
    "        \n",
    "        # create a bootstrap resampling of the station data (use complete years)\n",
    "        # complete_years = test_data.complete_year_dict[stn]['hyd_years']\n",
    "    \n",
    "        S = np.log(test_data.hyd_df[f'{stn}_uar_clipped'].dropna().values)\n",
    "        # initialize a random state\n",
    "        rng = np.random.default_rng(42)\n",
    "        # vectorized bootstrap resampling with replacement\n",
    "        idx = np.random.choice(S.size, size=(S.size, B), replace=True)    \n",
    "        S_b  = S[idx] # (n, B) bootstrap samples (column vectors of resampled data)\n",
    "\n",
    "        bootstrap_histograms = np.apply_along_axis(lambda x: np.histogram(x, bins=test_data.log_edges_extended, density=True)[0], arr=S_b, axis=0)\n",
    "\n",
    "        # convert pdfs to pmfs\n",
    "        bootstrap_histograms *= test_data.log_w[:, None]  # (M, B) multiply by the bin width\n",
    "\n",
    "        # assert all PMFs sum to 1 \n",
    "        assert np.allclose(np.sum(bootstrap_histograms, axis=0), 1), f'   sums do not equal 1:{np.sum(bootstrap_histograms, axis=0)}'\n",
    "\n",
    "        # get the parametric estimates\n",
    "        knn_estimator = kNNEstimator(context, test_data)\n",
    "        knn_estimator.baseline_pmf_type = 'obs' # use the observed or kde approximated pmf as the baseline\n",
    "        knn_estimator.k_nearest = 4\n",
    "        knn_estimator._initialize_nearest_neighbour_data()\n",
    "        \n",
    "        knn_estimator.weight_schemes = [2] # only look at inverse square distance weighting\n",
    "\n",
    "        nbr_df = knn_estimator.nbr_dfs['attribute_dist']['nbr_df'].copy()\n",
    "        nbr_data = knn_estimator.nbr_dfs['attribute_dist']['nbr_data'].copy()\n",
    "        # 2 is for weighting by inverse square distance\n",
    "        # set a range of deltas to test the sensitivity to the uniform mixture        \n",
    "        \n",
    "        test_data.delta = input_data['delta']\n",
    "        bootstrap_sample_results = knn_estimator._compare_bootstrap_P_to_ensemble_distributions(2, nbr_df, nbr_data, bootstrap_histograms, 'attribute_dist')\n",
    "        \n",
    "        for c in bootstrap_sample_results.keys():\n",
    "            result = {}\n",
    "            result['k'] = c\n",
    "            result['official_id'] = stn\n",
    "            result['delta'] = float(d)\n",
    "            result['weight_scheme'] = 2\n",
    "            sample_results = bootstrap_sample_results[c]\n",
    "            sample_result_df = pd.DataFrame(sample_results)\n",
    "            for pct in (2.5, 50, 97.5):\n",
    "                ps = np.percentile(sample_result_df.values, pct, axis=0)\n",
    "                # update the result dict\n",
    "                result.update({f\"{col}_{pct}\": round(float(val), 5) for col, val in zip(sample_result_df.columns, ps)})\n",
    "            all_trials.append(result)\n",
    "\n",
    "    knn_bootstrap_df = pd.DataFrame(all_trials)\n",
    "    knn_bootstrap_df.to_csv(output_fpath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff9c769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_bootstrap_cdfs(\n",
    "    dfs,\n",
    "    metric_col,\n",
    "    model_names=None,\n",
    "    id_col=\"official_id\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    dfs : list of pandas.DataFrame\n",
    "        Each DataFrame contains bootstrap samples for one model.\n",
    "        Must have columns [id_col, metric_col].\n",
    "    metric_col : str\n",
    "        Name of the column containing the metric values (e.g. 'rmse', 'kld').\n",
    "    model_names : list of str, optional\n",
    "        Names for each model (same length as dfs). If None, uses 'Model 1', 'Model 2', ...\n",
    "    id_col : str, default 'official_id'\n",
    "        Column identifying sites/stations.\n",
    "    show_quantile_bounds : bool, default True\n",
    "        If True, plot faint CDFs for 2.5 and 97.5 percentiles for each model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    summary_df : pandas.DataFrame\n",
    "        Long-format table with columns:\n",
    "        ['model', id_col, 'lb', 'median', 'ub'].\n",
    "    fig : bokeh.plotting.Figure\n",
    "        Bokeh figure object with Tufte-style CDFs.\n",
    "    \"\"\"\n",
    "    if model_names is None:\n",
    "        model_names = [f\"Model {i+1}\" for i in range(len(dfs))]\n",
    "    assert len(model_names) == len(dfs), \"model_names must match length of dfs\"\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # 1. Build summary dataframe: per-model, per-site quantiles\n",
    "    # ------------------------------------------------\n",
    "    records = {}\n",
    "\n",
    "    for df, model in zip(dfs, model_names):\n",
    "        # group by site, compute bootstrap percentiles for the metric\n",
    "        print(model, metric_col)\n",
    "        if '4NN' in model:\n",
    "            median_col = f'{metric_col}_50'\n",
    "            lb_col = f'{metric_col}_2.5'\n",
    "            ub_col = f'{metric_col}_97.5'\n",
    "            grouped = df[[id_col, median_col, lb_col, ub_col]].copy()\n",
    "            grouped.rename(columns={median_col: 'median', lb_col: 'lb', ub_col: 'ub'}, inplace=True)\n",
    "            grouped.set_index(id_col, inplace=True)\n",
    "        else:\n",
    "            grouped = df.groupby(id_col)[metric_col].quantile([0.025, 0.5, 0.975]).unstack()\n",
    "            grouped.columns = ['lb', 'median', 'ub']\n",
    "            grouped = grouped.reset_index()\n",
    "            grouped.set_index(id_col, inplace=True)\n",
    "\n",
    "        records[model] = grouped\n",
    "    return records\n",
    "\n",
    "        \n",
    "def plot_cdf_bootstrap_summary(\n",
    "    summary_dict,\n",
    "    model_names,\n",
    "    metric_col,\n",
    "    show_quantile_bounds=True,\n",
    "):\n",
    "    # ------------------------------------------------\n",
    "    # 2. Prepare a Tufte-style CDF plot\n",
    "    # ------------------------------------------------\n",
    "    # Colors for up to a few models\n",
    "    palette = [\"navy\", \"darkgreen\", \"orange\"]\n",
    "    model_color = {\n",
    "        model: palette[i % len(palette)] for i, model in enumerate(model_names)\n",
    "    }\n",
    "    p = figure(\n",
    "        width=700,\n",
    "        height=400,\n",
    "        toolbar_location=None,\n",
    "        x_axis_type='log',\n",
    "    )\n",
    "\n",
    "    # Plot one CDF per model\n",
    "    for model in model_names:\n",
    "        df_m = summary_dict[model]\n",
    "        \n",
    "        if m == 've':\n",
    "            label = 'NAE [%]'\n",
    "            xmin, xmax = 1, 150\n",
    "            df_m = 100 * (1 - df_m) # express as %, 0 is perfect\n",
    "        elif m == 'pct_vol_bias':\n",
    "            label = 'PB [%]'\n",
    "            xmin, xmax = -150, 75\n",
    "        elif m == 'nse':\n",
    "            label = '1 - NSE [-]'\n",
    "            # sort by median again\n",
    "            xmin, xmax = 0.0001, 10\n",
    "            df_m = 1 - df_m         # 1-NSE, 0 is perfect\n",
    "        elif m == 'rmse':\n",
    "            label = 'RMSE [%]'\n",
    "            xmin, xmax = 1, 400\n",
    "            df_m = 100*(np.exp(df_m) - 1)       # express as %, 0 is perfect\n",
    "        elif m == 'kld':\n",
    "            label = 'KLD [bits/sample]'\n",
    "            xmin, xmax = 0.001, 2\n",
    "\n",
    "        # Sort by median so we get a clean ECDF\n",
    "        medians = sorted(df_m['median'].values)\n",
    "        lbs = sorted(df_m[\"lb\"].values)\n",
    "        ubs = sorted(df_m[\"ub\"].values)\n",
    "        df_m['lb'] = lbs\n",
    "        df_m['ub'] = ubs\n",
    "        df_m['median'] = medians\n",
    "        N = len(df_m)\n",
    "        df_m[\"y\"] = (np.arange(N) + 0.5) / N\n",
    "\n",
    "        src = ColumnDataSource(df_m)\n",
    "        color = model_color[model]\n",
    "\n",
    "        # Main CDF: median of bootstrap for each station\n",
    "        p.step(\n",
    "            x=\"median\",\n",
    "            y=\"y\",\n",
    "            source=src,\n",
    "            mode=\"after\",\n",
    "            line_width=2,\n",
    "            line_color=color,\n",
    "            legend_label=model,\n",
    "        )\n",
    "\n",
    "        # Optional faint bounds: CDFs of lower/upper station-wise quantiles\n",
    "        if show_quantile_bounds:\n",
    "            p.harea(\n",
    "                x1=\"lb\",\n",
    "                x2=\"ub\",\n",
    "                y=\"y\",\n",
    "                source=src,\n",
    "                fill_color=color,\n",
    "                fill_alpha=0.35,\n",
    "            )\n",
    "\n",
    "    p.x_range = Range1d(xmin, xmax)\n",
    "    # Axes labels (generic; customize as needed)\n",
    "    p.xaxis.axis_label = label\n",
    "    p.yaxis.axis_label = \"Cumulative sample fraction\"\n",
    "\n",
    "    # remove heavy frame and grids\n",
    "    p.outline_line_color = None\n",
    "    p.xgrid.visible = False\n",
    "    p.ygrid.visible = False\n",
    "\n",
    "    p.legend.location = \"bottom_right\"\n",
    "    p.legend.background_fill_alpha = 0.0  # transparent legend background\n",
    "    p.legend.border_line_alpha = 0.0\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e64b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_list = [output_df, lstm_test_df, knn_bootstrap_df]\n",
    "plots = []\n",
    "model_names = [\"LogNorm\", \"LSTM\", \"4NN\"]\n",
    "for m in label_dict.keys():\n",
    "    if m == 'pct_vol_bias':\n",
    "        continue\n",
    "    records = summarize_bootstrap_cdfs(\n",
    "        df_list,\n",
    "        metric_col=m,\n",
    "        model_names=model_names)\n",
    "    p = plot_cdf_bootstrap_summary(\n",
    "        records,\n",
    "        model_names=model_names,\n",
    "        metric_col=m,\n",
    "    )\n",
    "    plots.append(p)\n",
    "lt = gridplot(plots, ncols=4, width=350, height=300, toolbar_location=None)\n",
    "fname = f'comparison_parametric_lstm_knn_uncertainty_{bitrate}_bits'\n",
    "# dpf.save_fig(lt, fname)\n",
    "show(lt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f3f77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_epsilon_sensitivity(\n",
    "    df,\n",
    "    epsilons,\n",
    "    m,\n",
    "    show_legend=False,\n",
    "):\n",
    "    # ------------------------------------------------\n",
    "    # 2. Prepare a Tufte-style CDF plot\n",
    "    # ------------------------------------------------\n",
    "    # Colors for up to a few models\n",
    "    palette = Sunset10\n",
    "    model_color = {\n",
    "        model: palette[i % len(palette)] for i, model in enumerate(epsilons)\n",
    "    }\n",
    "    p = figure(\n",
    "        width=700,\n",
    "        height=400,\n",
    "        toolbar_location=None,\n",
    "        x_axis_type='log',\n",
    "    )\n",
    "    for e in epsilons:\n",
    "        df_m = df[df['epsilon'] == e][m]\n",
    "        \n",
    "        if m == 've':\n",
    "            label = 'NAE [%]'\n",
    "            xmin, xmax = 1, 150\n",
    "            df_m = 100 * (1 - df_m) # express as %, 0 is perfect\n",
    "        elif m == 'pct_vol_bias':\n",
    "            label = 'PB [%]'\n",
    "            xmin, xmax = -150, 75\n",
    "        elif m == 'nse':\n",
    "            label = '1 - NSE [-]'\n",
    "            # sort by median again\n",
    "            xmin, xmax = 0.0001, 10\n",
    "            df_m = 1 - df_m         # 1-NSE, 0 is perfect\n",
    "        elif m == 'rmse':\n",
    "            label = 'RMSE [%]'\n",
    "            xmin, xmax = 1, 400\n",
    "            df_m = 100*(np.exp(df_m) - 1)       # express as %, 0 is perfect\n",
    "        elif m == 'kld':\n",
    "            label = 'KLD [bits/sample]'\n",
    "            xmin, xmax = 0.001, 4\n",
    "\n",
    "        # Sort by median so we get a clean ECDF\n",
    "        N = len(df_m)\n",
    "        y_vals = (np.arange(N) + 0.5) / N\n",
    "\n",
    "        # Main CDF: median of bootstrap for each station\n",
    "        p.line(\n",
    "            x=np.sort(df_m),\n",
    "            y=y_vals,\n",
    "            line_width=2,\n",
    "            line_color=model_color[e],\n",
    "            legend_label=str(e),\n",
    "        )\n",
    "\n",
    "        p.x_range = Range1d(xmin, xmax)\n",
    "        # Axes labels (generic; customize as needed)\n",
    "        p.xaxis.axis_label = label\n",
    "        p.yaxis.axis_label = \"Cumulative sample fraction\"\n",
    "\n",
    "        # remove heavy frame and grids\n",
    "        p.outline_line_color = None\n",
    "        p.xgrid.visible = False\n",
    "        p.ygrid.visible = False\n",
    "\n",
    "        if show_legend:\n",
    "\n",
    "            p.legend.location = \"bottom_right\"\n",
    "            p.legend.background_fill_alpha = 0.0  # transparent legend background\n",
    "            p.legend.border_line_alpha = 0.0\n",
    "            p.legend.click_policy='hide'\n",
    "        else:\n",
    "            p.legend.visible = False\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a9deda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54c6d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "output_fpath = os.path.join('data', 'results', 'quantization_sensitivity', f'quantization_sensitivity_eps_knn_{bitrate}_bits.csv')\n",
    "\n",
    "input_data['bitrate'] = 8\n",
    "input_data['regularization_type'] = 'discrete'\n",
    "context = FDCEstimationContext(**input_data)\n",
    "\n",
    "if os.path.exists(output_fpath):\n",
    "    rdf = pd.read_csv(output_fpath, dtype={'official_id': str})\n",
    "else:\n",
    "    all_trials = []\n",
    "    for nn, stn in enumerate(daymet_concurrent_stations):\n",
    "        print(f'    Processing station {nn+1}/{len(daymet_concurrent_stations)}: {stn}')\n",
    "        \n",
    "        # get the target data\n",
    "        test_data = StationData(context, stn)\n",
    "        test_data.eval_metrics = EvaluationMetrics(data=test_data, bitrate=bitrate)\n",
    "        test_data.k_nearest = 4\n",
    "        # S = np.log(test_data.hyd_df[f'{stn}_uar'].dropna().values)\n",
    "        target_da= stn_da_dict[stn]\n",
    "        test_data.baseline_pmf = test_data.baseline_pmf_df[stn].values\n",
    "        # test_data.baseline_pdf = test_data.baseline_pdf_df[stn].values\n",
    "\n",
    "        # get the parametric estimates\n",
    "        knn_estimator = kNNEstimator(context, test_data)\n",
    "        # knn_estimator.baseline_pmf_type = 'obs' # use the observed or kde approximated pmf as the baseline\n",
    "        knn_estimator.k_nearest = test_data.k_nearest\n",
    "        knn_estimator._initialize_nearest_neighbour_data()\n",
    "        knn_estimator.weight_schemes = [2] # only look at inverse square distance weighting\n",
    "\n",
    "        nbr_df = knn_estimator.nbr_dfs['attribute_dist']['nbr_df'].copy()\n",
    "        nbr_data = knn_estimator.nbr_dfs['attribute_dist']['nbr_data'].copy()\n",
    "        # 2 is for weighting by inverse square distance\n",
    "        # set a range of deltas to test the sensitivity to the uniform mixture\n",
    "        \n",
    "        for e in eps:\n",
    "            knn_estimator._compute_ensemble_distributions(2, nbr_df, nbr_data, 'attribute_dist', epsilon=e)\n",
    "            sim_data = knn_estimator.knn_simulation_data\n",
    "            for c in sim_data.keys():\n",
    "                result = sim_data[c]['eval']\n",
    "                result['official_id'] = stn\n",
    "                result['k'] = sim_data[c]['k']\n",
    "                result['weight_scheme'] = 2\n",
    "                result['epsilon'] = e\n",
    "                all_trials.append(result)\n",
    "    rdf = pd.DataFrame(all_trials)\n",
    "    rdf.to_csv(output_fpath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ba2440",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_plots = []\n",
    "eps = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "for m in label_dict.keys():\n",
    "    show_legend=True if m=='kld' else False\n",
    "    if m == 'pct_vol_bias':\n",
    "        continue\n",
    "    p = plot_epsilon_sensitivity(rdf, eps, m, show_legend=show_legend)\n",
    "    eps_plots.append(p)\n",
    "\n",
    "lt = gridplot(eps_plots, ncols=4, width=350, height=300, toolbar_location=None)\n",
    "fname = f'epsilon_sensitivity_{bitrate}_bits'\n",
    "# dpf.save_fig(lt, fname)\n",
    "show(lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68f5606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d346f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf8e55d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
