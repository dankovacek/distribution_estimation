{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec6f101-baba-407b-bbd0-da0b48b55d58",
   "metadata": {},
   "source": [
    "## Parametric FDC Estimation\n",
    "\n",
    "Consider a sequence of methods for estimating the flow duration curve at an unmonitored location. Starting from the simplest method and increasing in complexity.  The goal is to estimate a distribution, then evaluate the expectation of the method over a large sample by computing the divergence between the \"true\" distribution $P$ and the corresponding estimate $Q$.  Consider several divergences from the class of f-divergences for their unique properties, namely their associated conjugate dual or surrogate loss functions. \n",
    "\n",
    ">**Note**: clarify the relationship between individual f-divergences, their conjugate dual functions, and their surrogate loss functions.\n",
    "\n",
    "1.  **Naive or maximum uncertainty**: as a baseline, the divergence of a probability distribution from the uniform distribution is related to the entropy.  This may be interpretable as a baseline that describes the maximum reduction in uncertainty, i.e. from maximum uncertainty to observation.  (we may take some kind of result from asymptotic convergence towards the true entropy/divergence from the Jordan results as a way to describe \"uncertainty about uncertainty\")  *\"This problem includes as a special case the problem of estimating the mutual information, corresponding to the KL divergence between a joint distribution and the product of its marginals, as well as the problem of estimating the Shannon entropy of a distribution P, which is related to the KL divergence between P and the uniform distribution.\"* (see Nguyen et al. 2009 **Estimating divergence functionals and the likelihood  ratio by convex risk minimization**) \n",
    "2.  **k-Nearest neighbour**: take the k nearest streamflow stations and approximate the target location based on some kind of average.  Subsets of this method include a) different k, b) weighting schemes on top of different k, or c) other intermediate models on k.\n",
    "3.  **\"Best\" neighbour**: how much better could the \"best\" neighbour be if we knew how to pick it?  Note: we don't really know how to pick it. (see Poole et al. 2021 Regionalization for Ungauged Catchments â€” Lessons Learned From a Comparative Large-Sample Study).\n",
    "    * An \"upper\" bound for this is computing what the \"extra bits per sample\" cost of using a uniform distribution as a model, i.e. $D_K(P||U)$ because some number of neighbours will be worse than this (one question is now many).\n",
    "    * A \"lower\" bound for this is estimating the uncertainty in the distribution estimate itself, because there is a limit to the precision we can use to rank neighbours based on KL divergence.  \n",
    "5.  **Parametric distribution estimation**: (following previous results) we show that the mean runoff can be predicted well.  We know that the standard deviation is well correlated to the mean, so we have two parameters that we can use to estimate the distribution using any number of parametric forms. **Non-parametric distributions do not represent generators, so KDE can't be used to estimate a distribution for an ungauged catchment**.\n",
    "6.  **Divergence Prediction**: what if we ignore the specifics how *how* the two distributions differ and instead focus on *how much* they differ.\n",
    "\n",
    "\n",
    "One model that predicts sufficient statistics, then compute KL divergence.\n",
    "Next model that predicts the computed KL divergence.\n",
    "Both models use only catchment characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d713f9b2-5203-452c-b336-83a0d633496c",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "Let $P$ represent the \"ground truth\" distribution of unit area runoff for a monitored catchment $i$.  Let $\\hat P_i$ represent an estimate of $P_i$.\n",
    "\n",
    "For all $i$ in $M$ monitored catchments:\n",
    "\n",
    "1. Compute an estimate of the entropy of $P_i$ based on $\\hat P_i$:\n",
    "    * The entropy is sensitive to the method \n",
    "3. Estimate\n",
    "4. Use the parameters as a model Q in pairwise comparisons, these eliminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cf7cb74-1b3a-448d-be6e-50656934d17d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"c09c522a-dee3-438d-b854-83bb7796c093\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"c09c522a-dee3-438d-b854-83bb7796c093\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.7.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.7.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.7.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.7.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.7.2.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"c09c522a-dee3-438d-b854-83bb7796c093\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from time import time\n",
    "from scipy.stats import entropy\n",
    "import multiprocessing as mp\n",
    "import data_processing_functions as dpf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# visualize the catchment centroid locations\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import ColumnDataSource, LinearAxis, Range1d\n",
    "from bokeh.palettes import Colorblind, Sunset10\n",
    "\n",
    "from scipy.stats import lognorm, expon, kappa4, gaussian_kde\n",
    "from scipy.special import kl_div, digamma\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from KDEpy import FFTKDE\n",
    "import jax.numpy as jnp\n",
    "from jax import vmap\n",
    "from jax.scipy.stats import gaussian_kde as jkde\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "BASE_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac8d53e4-38c2-4eaa-998b-40e7f0432afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import attributes \n",
    "rev_date = '20250227'\n",
    "attributes_fpath = os.path.join(os.path.join('data', f'BCUB_watershed_attributes_updated_{rev_date}.geojson'))\n",
    "attr_df = gpd.read_file(attributes_fpath)\n",
    "attr_df.columns = [e.lower() for e in attr_df.columns]\n",
    "filtered_stns = sorted(list(set(attr_df['official_id'].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4f1f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the predicted parameter results\n",
    "predict_result_folder = '/home/danbot2/code_5820/24/divergence_measures/docs/notebooks/data/prediction_results/runoff_prediction_results'\n",
    "# target_cols = ['mean_uar', 'sd_uar', 'logq_mean_mm', 'logq_sd_mm',]\n",
    "target_columns = ['mean_uar', 'sd_uar', 'mean_logx', 'sd_logx']\n",
    "\n",
    "predicted_param_dict = {}\n",
    "for t in target_columns:\n",
    "    print(t)\n",
    "    file = f'best_out_of_sample_{t}_predictions.csv'\n",
    "    rdf = pd.read_csv(os.path.join(predict_result_folder, file), index_col='official_id')\n",
    "    rdf = rdf[[c for c in rdf.columns if not c.startswith('Unnamed:')]].sort_values('official_id')\n",
    "    predicted_param_dict[t] = rdf.to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e78fa52-e9dd-4021-80be-b1b9dd61530d",
   "metadata": {},
   "source": [
    "## General data processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ac11f0c-b648-40df-8406-9311e718aa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bootstrap_samples(data, N):\n",
    "    \"\"\"\n",
    "    Generate a matrix of N bootstrap samples, each of length L.\n",
    "\n",
    "    Parameters:\n",
    "        data (array-like): 1D array of sample data.\n",
    "        N (int): Number of bootstrap samples to generate.\n",
    "        L (int): Length of each bootstrap sample.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Matrix of shape (N, L) containing bootstrap samples.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    indices = np.random.randint(0, len(data), size=(len(data), N))\n",
    "    bootstrap_samples = data[indices]\n",
    "    return bootstrap_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b99794d-5472-4ddb-b77d-def246a4b017",
   "metadata": {},
   "source": [
    "## Estimate entropy\n",
    "\n",
    "The entropy of a distribution $P$ is related to the KL divergence between $P$ and the uniform distribution $\\mathbb{U}$ by the following:\n",
    "\n",
    "The KL divergence between two distributions $P$ and $Q$ is:\n",
    "\n",
    "$$D_\\text{KL}(P||Q) = \\int p(x) \\log \\frac{p(x)}{q(x)} dx$$\n",
    "\n",
    "Where $p(x)$ and $q(x)$ are the probability density functions of $P$ and $Q$ respectively.  If $Q$ is a uniform distribution over $[a, b]$, the density is the constant $q(x) = (b-a)^{-1}$, $x \\in [a, b]$.  Substituting into the KL divergence expression:\n",
    "\n",
    "$$D_\\text{KL}(P||Q) = \\int p(x) \\log \\frac{p(x)}{1/(b-a)}dx = \\int p(x) \\log p(x)dx - \\int p(x)\\log \\frac{1}{b-a}dx$$\n",
    "\n",
    "Since $log\\frac{1}{b-a}$ is constant over the domain,\n",
    "\n",
    "$$\\int p(x)\\log \\frac{1}{b-a} dx = \\log(b-a)$$\n",
    "\n",
    "And the first term is the negative entropy of $p(x)$, i.e. $-H(P) = \\int p(x) \\log (x) dx$, then:\n",
    "\n",
    "$$D_\\text{KL}(P||Q) = -H(P) + \\log(b-a)$$\n",
    "\n",
    "So the maximum uncertainty $\\log(b-a)$ is reduced by $\\log(b-a) - H(P) = D_\\text{KL}(P||Q)$ bits per sample by observing the system. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199db173-7155-478e-a577-5661a5901546",
   "metadata": {},
   "source": [
    "## Define a measurement error model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e266bd8c-3b54-4550-8ae0-fec5ce5ff8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"c8017864-8374-409c-8d6f-3757ae8028e4\" data-root-id=\"p1048\" style=\"display: contents;\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function embed_document(root) {\n  const docs_json = {\"d45ab6f4-29ce-44f8-86c5-861a8b462fd1\":{\"version\":\"3.6.0\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1048\",\"attributes\":{\"width\":400,\"height\":300,\"x_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1049\"},\"y_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1050\"},\"x_scale\":{\"type\":\"object\",\"name\":\"LogScale\",\"id\":\"p1057\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1058\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1055\"},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1088\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1082\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1083\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1084\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"F7fROG8SgzoK1yM8zczMPQAAgD8AACBBAADIQgAAekQAQBxGAFDDRw==\"},\"shape\":[10],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"AAAgQQAAoEAAAIA/AAAAPwAAgD7NzMw9zczMPZqZGT7NzEw+AACAPg==\"},\"shape\":[10],\"dtype\":\"float32\",\"order\":\"little\"}]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1089\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1090\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1085\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"red\",\"line_width\":2}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1086\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"red\",\"line_alpha\":0.1,\"line_width\":2}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1087\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"red\",\"line_alpha\":0.2,\"line_width\":2}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1056\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1069\"},{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1070\",\"attributes\":{\"renderers\":\"auto\"}},{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1071\",\"attributes\":{\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1072\",\"attributes\":{\"syncable\":false,\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5,\"level\":\"overlay\",\"visible\":false,\"left\":{\"type\":\"number\",\"value\":\"nan\"},\"right\":{\"type\":\"number\",\"value\":\"nan\"},\"top\":{\"type\":\"number\",\"value\":\"nan\"},\"bottom\":{\"type\":\"number\",\"value\":\"nan\"},\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"handles\":{\"type\":\"object\",\"name\":\"BoxInteractionHandles\",\"id\":\"p1078\",\"attributes\":{\"all\":{\"type\":\"object\",\"name\":\"AreaVisuals\",\"id\":\"p1077\",\"attributes\":{\"fill_color\":\"white\",\"hover_fill_color\":\"lightgray\"}}}}}}}},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1079\"},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1080\"},{\"type\":\"object\",\"name\":\"HelpTool\",\"id\":\"p1081\"}]}},\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1064\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1065\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1066\"},\"axis_label\":\"$$\\\\text{Error } [/100]$$\",\"axis_label_text_font\":\"Bitstream Charter\",\"axis_label_text_font_size\":\"12pt\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1067\"},\"major_label_text_font\":\"Bitstream Charter\",\"major_label_text_font_size\":\"10pt\"}}],\"below\":[{\"type\":\"object\",\"name\":\"LogAxis\",\"id\":\"p1059\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"LogTicker\",\"id\":\"p1060\",\"attributes\":{\"num_minor_ticks\":10,\"mantissas\":[1,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"LogTickFormatter\",\"id\":\"p1061\"},\"axis_label\":\"$$\\\\text{Flow } m^3/s$$\",\"axis_label_text_font\":\"Bitstream Charter\",\"axis_label_text_font_size\":\"12pt\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1062\"},\"major_label_text_font\":\"Bitstream Charter\",\"major_label_text_font_size\":\"10pt\"}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1063\",\"attributes\":{\"axis\":{\"id\":\"p1059\"}}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1068\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1064\"}}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1091\",\"attributes\":{\"label_text_font\":\"Bitstream Charter\",\"label_text_font_size\":\"10pt\",\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1092\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Measurement Error Model\"},\"renderers\":[{\"id\":\"p1088\"}]}}]}}]}}]}};\n  const render_items = [{\"docid\":\"d45ab6f4-29ce-44f8-86c5-861a8b462fd1\",\"roots\":{\"p1048\":\"c8017864-8374-409c-8d6f-3757ae8028e4\"},\"root_ids\":[\"p1048\"]}];\n  void root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n  }\n  if (root.Bokeh !== undefined) {\n    embed_document(root);\n  } else {\n    let attempts = 0;\n    const timer = setInterval(function(root) {\n      if (root.Bokeh !== undefined) {\n        clearInterval(timer);\n        embed_document(root);\n      } else {\n        attempts++;\n        if (attempts > 100) {\n          clearInterval(timer);\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n        }\n      }\n    }, 10, root)\n  }\n})(window);",
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "p1048"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the ranges and associated errors\n",
    "error_points = jnp.array([0.0001, 0.001, 0.01, 0.1, 1.0, 10, 1e2, 1e3, 1e4, 1e5])  # Magnitude points\n",
    "error_values = jnp.array([10., 5.0, 1., 0.5, 0.25, 0.1, 0.1, 0.15, 0.20, 0.25])    # Associated errors (as proportions)\n",
    "\n",
    "efig = figure(width=400, height=300, x_axis_type='log')\n",
    "efig.line(error_points, error_values, line_color='red', line_width=2, legend_label='Measurement Error Model')\n",
    "efig.xaxis.axis_label = r'$$\\text{Flow } m^3/s$$'\n",
    "efig.yaxis.axis_label = r'$$\\text{Error } [/100]$$'\n",
    "efig = dpf.format_fig_fonts(efig, font_size=12)\n",
    "show(efig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4f93f04-fe26-4d8c-baf2-a56aee7f9b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_kde_integration(log_data, eval_grid):\n",
    "    \"\"\"\n",
    "    Perform integration for a single bootstrap sample using its KDE fit.\n",
    "    \n",
    "    Parameters:\n",
    "    - log_data: Log-transformed data points for one bootstrap sample (1D array).\n",
    "    - eval_grid: Shared evaluation grid (1D array).\n",
    "\n",
    "    Returns:\n",
    "    - Integrated densities over the evaluation grid (1D array).\n",
    "    \"\"\"\n",
    "    fit_object = jkde(log_data, bw_method='silverman')\n",
    "    return vmap(lambda le: fit_object.integrate_box_1d(-jnp.inf, le))(eval_grid)\n",
    "\n",
    "\n",
    "def compute_baseline_kde_fits(eval_grid, bootstrap_samples):\n",
    "    \"\"\"\n",
    "    Compute KDE for each bootstrap sample column in a vectorized manner.\n",
    "    \n",
    "    Parameters:\n",
    "    - eval_grid: Shared evaluation grid (array of shape (n_eval,)).\n",
    "    - bootstrap_samples: Matrix of bootstrap samples (shape (n_data, n_bootstrap)).\n",
    "    \n",
    "    Returns:\n",
    "    - Density estimates for each bootstrap sample (array of shape (n_eval, n_bootstrap)).\n",
    "    \"\"\"\n",
    "    # Log-transform the bootstrap samples\n",
    "    log_samples = jnp.log10(bootstrap_samples)\n",
    "\n",
    "    # Define a function to integrate a single column\n",
    "    def integrate_column(log_data):\n",
    "        return vectorized_kde_integration(log_data, jnp.array(eval_grid))\n",
    "    \n",
    "    # Apply the function to each column of the bootstrap samples\n",
    "    cdf = vmap(integrate_column, in_axes=1, out_axes=1)(log_samples)\n",
    "    pmf = jnp.diff(cdf, axis=0)\n",
    "    pmf /= np.sum(pmf, axis=0)\n",
    "    return pmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35265821-877c-4333-b3c3-bf91d1d26651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolated error function\n",
    "def interpolate_error_model(x):\n",
    "    \"\"\"\n",
    "    Interpolate measurement error as a function of x using the provided error ranges.\n",
    "    \"\"\"\n",
    "    return jnp.interp(x, error_points, error_values)\n",
    "\n",
    "def compute_fft_kde_fit_bootstrap(data, grid):\n",
    "    # input_data = jnp.log10(data)\n",
    "    fft_weights = jnp.asarray(vmap(interpolate_error_model)(data)).copy()\n",
    "\n",
    "    fft_weights /= fft_weights.sum(axis=0)\n",
    "    \n",
    "    log_data = np.log10(data) \n",
    "    cdfs = []\n",
    "    nn = 0\n",
    "    for d, w in zip(log_data.T, fft_weights.T):\n",
    "        nn += 1\n",
    "        fft_fit = FFTKDE().fit(np.array(d), weights=np.array(w))\n",
    "        x, y = fft_fit.evaluate()\n",
    "        fft_density = fft_fit.evaluate(grid)\n",
    "        cdfs.append(fft_density)\n",
    "        \n",
    "    fft_kde_cdf = jnp.cumsum(jnp.array(cdfs).T, axis=0) * (grid[1] - grid[0])\n",
    "    # Normalize CDF to ensure it sums to 1\n",
    "    fft_kde_cdf /= fft_kde_cdf[-1]\n",
    "    pmf = np.diff(fft_kde_cdf, axis=0)\n",
    "    pmf /= np.sum(pmf, axis=0)\n",
    "    return pmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1085359-d5b9-4774-bc4c-4c12456da7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New largest CI = 0.01 (N=1752)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 12:51:44.003774: W external/xla/xla/tsl/framework/bfc_allocator.cc:497] Allocator (GPU_0_bfc) ran out of memory trying to allocate 459.39MiB (rounded to 481702912)requested by op \n",
      "2025-01-16 12:51:44.004051: W external/xla/xla/tsl/framework/bfc_allocator.cc:508] ****************************************************************************************__*********_\n",
      "E0116 12:51:44.004086 2110204 pjrt_stream_executor_client.cc:3085] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 481702800 bytes.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "RESOURCE_EXHAUSTED: Out of memory while trying to allocate 481702800 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m eval_grid \u001b[38;5;241m=\u001b[39m create_pdf_eval_grid(data)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# here we generate bootstrap samples of the baseline \u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# distribution estimate\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m kde_fits \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_baseline_kde_fits\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# print(kde_fits.shape)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# compute distributions incorporating adaptive kernel\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# bandwidth based on an assumed measurement error model\u001b[39;00m\n\u001b[1;32m     23\u001b[0m adaptive_kde_fits \u001b[38;5;241m=\u001b[39m compute_fft_kde_fit_bootstrap(samples, eval_grid)\n",
      "Cell \u001b[0;32mIn[8], line 35\u001b[0m, in \u001b[0;36mcompute_baseline_kde_fits\u001b[0;34m(eval_grid, bootstrap_samples)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vectorized_kde_integration(log_data, jnp\u001b[38;5;241m.\u001b[39marray(eval_grid))\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Apply the function to each column of the bootstrap samples\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m cdf \u001b[38;5;241m=\u001b[39m \u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintegrate_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m pmf \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mdiff(cdf, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     37\u001b[0m pmf \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(pmf, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[8], line 32\u001b[0m, in \u001b[0;36mcompute_baseline_kde_fits.<locals>.integrate_column\u001b[0;34m(log_data)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mintegrate_column\u001b[39m(log_data):\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvectorized_kde_integration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m, in \u001b[0;36mvectorized_kde_integration\u001b[0;34m(log_data, eval_grid)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mPerform integration for a single bootstrap sample using its KDE fit.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m- Integrated densities over the evaluation grid (1D array).\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m fit_object \u001b[38;5;241m=\u001b[39m jkde(log_data, bw_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msilverman\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mle\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintegrate_box_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_grid\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m, in \u001b[0;36mvectorized_kde_integration.<locals>.<lambda>\u001b[0;34m(le)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mPerform integration for a single bootstrap sample using its KDE fit.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m- Integrated densities over the evaluation grid (1D array).\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m fit_object \u001b[38;5;241m=\u001b[39m jkde(log_data, bw_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msilverman\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vmap(\u001b[38;5;28;01mlambda\u001b[39;00m le: \u001b[43mfit_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintegrate_box_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mle\u001b[49m\u001b[43m)\u001b[49m)(eval_grid)\n",
      "File \u001b[0;32m~/code_5820/data_analysis/lib/python3.10/site-packages/jax/_src/scipy/stats/kde.py:162\u001b[0m, in \u001b[0;36mgaussian_kde.integrate_box_1d\u001b[0;34m(self, low, high)\u001b[0m\n\u001b[1;32m    160\u001b[0m low \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39msqueeze((low \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset) \u001b[38;5;241m/\u001b[39m sigma)\n\u001b[1;32m    161\u001b[0m high \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39msqueeze((high \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset) \u001b[38;5;241m/\u001b[39m sigma)\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m*\u001b[39m (\u001b[43mspecial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndtr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhigh\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m special\u001b[38;5;241m.\u001b[39mndtr(low)))\n",
      "File \u001b[0;32m~/code_5820/data_analysis/lib/python3.10/site-packages/jax/_src/scipy/special.py:907\u001b[0m, in \u001b[0;36mndtr\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (jnp\u001b[38;5;241m.\u001b[39mfloat32, jnp\u001b[38;5;241m.\u001b[39mfloat64):\n\u001b[1;32m    904\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    905\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx.dtype=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is not supported, see docstring for supported types.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    906\u001b[0m       \u001b[38;5;241m.\u001b[39mformat(dtype))\n\u001b[0;32m--> 907\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ndtr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code_5820/data_analysis/lib/python3.10/site-packages/jax/_src/scipy/special.py:919\u001b[0m, in \u001b[0;36m_ndtr\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    914\u001b[0m w \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m*\u001b[39m half_sqrt_2\n\u001b[1;32m    915\u001b[0m z \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39mabs(w)\n\u001b[1;32m    916\u001b[0m y \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39mselect(lax\u001b[38;5;241m.\u001b[39mlt(z, half_sqrt_2),\n\u001b[1;32m    917\u001b[0m                     dtype(\u001b[38;5;241m1.\u001b[39m) \u001b[38;5;241m+\u001b[39m lax\u001b[38;5;241m.\u001b[39merf(w),\n\u001b[1;32m    918\u001b[0m                     lax\u001b[38;5;241m.\u001b[39mselect(lax\u001b[38;5;241m.\u001b[39mgt(w, dtype(\u001b[38;5;241m0.\u001b[39m)),\n\u001b[0;32m--> 919\u001b[0m                                     \u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2.\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    920\u001b[0m                                     lax\u001b[38;5;241m.\u001b[39merfc(z)))\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dtype(\u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;241m*\u001b[39m y\n",
      "File \u001b[0;32m~/code_5820/data_analysis/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:1050\u001b[0m, in \u001b[0;36m_forward_operator_to_aval.<locals>.op\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mop\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m-> 1050\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code_5820/data_analysis/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:573\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    571\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 573\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "File \u001b[0;32m~/code_5820/data_analysis/lib/python3.10/site-packages/jax/_src/numpy/ufunc_api.py:177\u001b[0m, in \u001b[0;36mufunc.__call__\u001b[0;34m(self, out, where, *args)\u001b[0m\n\u001b[1;32m    175\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere argument of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    176\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__static_props[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_vectorized\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "File \u001b[0;32m~/code_5820/data_analysis/lib/python3.10/site-packages/jax/_src/pjit.py:1738\u001b[0m, in \u001b[0;36m_pjit_call_impl\u001b[0;34m(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1729\u001b[0m donated_argnums \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(i \u001b[38;5;28;01mfor\u001b[39;00m i, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(donated_invars) \u001b[38;5;28;01mif\u001b[39;00m d)\n\u001b[1;32m   1730\u001b[0m cache_key \u001b[38;5;241m=\u001b[39m pxla\u001b[38;5;241m.\u001b[39mJitGlobalCppCacheKeys(\n\u001b[1;32m   1731\u001b[0m     donate_argnums\u001b[38;5;241m=\u001b[39mdonated_argnums, donate_argnames\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1732\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1736\u001b[0m     out_layouts_treedef\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out_layouts_leaves\u001b[38;5;241m=\u001b[39mout_layouts,\n\u001b[1;32m   1737\u001b[0m     use_resource_env\u001b[38;5;241m=\u001b[39mresource_env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1738\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_xla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpjit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall_impl_cache_miss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtree_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_registry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpxla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcc_shard_arg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_get_cpp_global_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_key\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontains_explicit_attributes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 481702800 bytes."
     ]
    }
   ],
   "source": [
    "n_simulations = 50\n",
    "n = 1\n",
    "max_diff = 0\n",
    "results = []\n",
    "for stn in filtered_stns:\n",
    "    test_df = dpf.get_timeseries_data(stn)\n",
    "    test_df.dropna(subset=[stn], inplace=True)\n",
    "    data = test_df[stn].dropna().values\n",
    "    data = add_jitter(data)\n",
    "    samples = generate_bootstrap_samples(data, n_simulations)\n",
    "    log_samples = np.log10(samples)\n",
    "\n",
    "    # create a common grid to evaluate pdfs numerically\n",
    "    eval_grid = create_pdf_eval_grid(data)\n",
    "\n",
    "    # here we generate bootstrap samples of the baseline \n",
    "    # distribution estimate\n",
    "    kde_fits = compute_baseline_kde_fits(eval_grid, samples) \n",
    "    # print(kde_fits.shape)\n",
    "\n",
    "    # compute distributions incorporating adaptive kernel\n",
    "    # bandwidth based on an assumed measurement error model\n",
    "    adaptive_kde_fits = compute_fft_kde_fit_bootstrap(samples, eval_grid)\n",
    "    # print(adaptive_kde_fits.shape)\n",
    "    # print(asdf)\n",
    "    # compute the KL divergence between simple and weighted KDE fits\n",
    "    # for the bootstrap samples\n",
    "    kde_KLD = compute_kl_divergence(kde_fits, adaptive_kde_fits)\n",
    "    kde_CI = np.percentile(kde_KLD, (2.5, 50, 97.5))\n",
    "    results.append(kde_CI)\n",
    "    ci = kde_CI[2] - kde_CI[0]\n",
    "\n",
    "    # find the nearest k stations and use their distribution(s) as a model\n",
    "    # for k in [1, 2, 3, 4, 5]:\n",
    "        \n",
    "    \n",
    "    # uncertainty_reduction_kde = np.round(max_uncertainty - np.percentile(kde_H, (97.5, 50, 2.5)), 2)\n",
    "    # ci = uncertainty_reduction_kde[2] - uncertainty_reduction_kde[0]\n",
    "    # results.append([ci, len(test_df)])\n",
    "    if ci > max_diff:\n",
    "        max_diff = ci\n",
    "        print(f'New largest CI = {ci:.2f} (N={len(test_df)})')\n",
    "    if n % 25 == 0:\n",
    "        print(f'    {n}/{len(filtered_stns)} completed')\n",
    "        # print(asdf)\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da76a6ed-f40c-4456-b0cd-9eea859d9cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ci_df = pd.DataFrame(results, columns=['ci', 'n'])\n",
    "fig = figure(title=f'KDE estimated H(P) CI vs. sample size, {n_simulations} bootstrap samples')#, x_axis_type='log')\n",
    "years_equiv = ci_df['n'] / 365.24\n",
    "fig.scatter(years_equiv, ci_df['ci'], size=2, legend_label='95% CI')\n",
    "fig.xaxis.axis_label = r'$$\\text{Sample size [x365]}$$'\n",
    "fig.yaxis.axis_label = r'$$\\text{[bits/sample]}$$'\n",
    "fig = dpf.format_fig_fonts(fig)\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084cb70f-2b44-4387-9a0c-6e0ee3d62478",
   "metadata": {},
   "source": [
    "The idea behind the above plot is to show how the uncertainty, as expressed by the 95% confidence interval of a bootstrap sample, in the entropy estimate of a large sample of arbitrary distributions, varies as a function of the sample size.  The x-axis represents the \"effective\" sample size in more intuitive terms of years.  \n",
    "\n",
    "The motivation is to understand how the length of record is reflected in downstream analysis when comparing pairs of arbitrary distributions.\n",
    "\n",
    "What is needed is to figure out how to convey this in terms that relate to the ultimate discriminant, or the ability to distinguish between potential models for their reduction in uncertainty.  Stated otherwise, to describe some kind of threshold where we can say that one proxy model is better than another, that is provides a more accurate estimate of the 'ground truth' distribution.  Such a model would enable the large scale comparison of potential future observation network expansion locations on the basis of how much they reduce the **aggregate uncertainty** of the unmonitored space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd343ddc-f0ca-4014-ab0c-cc69a4afe705",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e265d6e-2868-4660-bd80-0f2b55d882a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "def neg_log_likelihood(params):\n",
    "    h, k, loc, scale = params\n",
    "    if scale <= 0:\n",
    "        return np.inf  # Enforce positive scale\n",
    "    try:\n",
    "        logpdf = kappa4.logpdf(data, h, k, loc=loc, scale=scale)\n",
    "        if np.any(np.isnan(logpdf) | np.isinf(logpdf)):\n",
    "            return np.inf\n",
    "        return -np.sum(logpdf)\n",
    "    except Exception:\n",
    "        return np.inf\n",
    "\n",
    "def fit_kappa4_mle(data):\n",
    "    if len(data) < 2 or np.std(data) <= 0:\n",
    "        # Handle edge cases where MLE is not feasible\n",
    "        return {'h': np.nan, 'k': np.nan, 'loc': np.nan, 'scale': np.nan}\n",
    "    \n",
    "    # Define the initial guesses for the parameters\n",
    "    initial_params = [0.0, 0.0, np.mean(data), np.std(data)]\n",
    "    \n",
    "    # Bounds for the parameters: h and k between -2 and 2, scale > 1e-5\n",
    "    bounds = [(-2, 2), (-2, 2), (None, None), (1e-5, None)]\n",
    "    \n",
    "    # Minimize the negative log-likelihood\n",
    "    result = minimize(neg_log_likelihood, initial_params, method='L-BFGS-B', bounds=bounds)\n",
    "    \n",
    "    if result.success:\n",
    "        h, k, loc, scale = result.x\n",
    "    else:\n",
    "        h, k, loc, scale = np.nan, np.nan, np.nan, np.nan\n",
    "        # Log or handle fitting failure if needed\n",
    "    \n",
    "    return {'h': h, 'k': k, 'loc': loc, 'scale': scale}\n",
    "\n",
    "\n",
    "def compute_pdf_cdf_kde_scipy(data, grid_points=1000, bandwidth=None):\n",
    "    \"\"\"\n",
    "    Computes the probability density function (pdf) and cumulative distribution function (cdf)\n",
    "    from an array of values using Kernel Density Estimation (KDE) with scipy.stats.\n",
    "\n",
    "    Parameters:\n",
    "    data (array-like): Input data array.\n",
    "    grid_points (int): Number of points in the grid where the pdf and cdf are evaluated.\n",
    "    bandwidth (float or str, optional): The bandwidth of the kernel. If None, Scott's Rule is used.\n",
    "                                         Can also be a string for methods like 'scott' or 'silverman'.\n",
    "\n",
    "    Returns:\n",
    "    x_grid (numpy.ndarray): Grid points where the pdf and cdf are evaluated.\n",
    "    pdf_values (numpy.ndarray): Estimated pdf values corresponding to x_grid.\n",
    "    cdf_values (numpy.ndarray): Estimated cdf values corresponding to x_grid.\n",
    "    \"\"\"\n",
    "    # Convert input data to a numpy array\n",
    "    data = np.asarray(data)\n",
    "    \n",
    "    # Create a Gaussian KDE object\n",
    "    kde = gaussian_kde(data, bw_method=bandwidth)\n",
    "    \n",
    "    # Create a grid over which to evaluate the KDE\n",
    "    x_min = data.min() - 1.0 * data.std()\n",
    "    x_max = data.max() + 1.0 * data.std()\n",
    "    x_grid = np.linspace(x_min, x_max, grid_points)\n",
    "    \n",
    "    # Evaluate the pdf over the grid\n",
    "    pdf_values = kde.evaluate(x_grid)\n",
    "    \n",
    "    # Compute the cdf by integrating the pdf\n",
    "    cdf_values = np.array([kde.integrate_box_1d(-np.inf, xi) for xi in x_grid])\n",
    "    \n",
    "    return x_grid, pdf_values, cdf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565faeb8-7e97-406f-a351-6889f3145ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_low_to_high_resolution(data, b1, b2, which_binning='log'):\n",
    "    \"\"\"\n",
    "    Vectorized distribution of low-resolution frequencies over high-resolution bins.\n",
    "    Expands each low-res bin's frequency uniformly across the corresponding high-res bins.\n",
    "    \"\"\"\n",
    "    b1_bin_dict = compute_discrete_distributions(data, b1)  # Low-res (Q)\n",
    "    b2_bin_dict = compute_discrete_distributions(data, b2)  # High-res (P)\n",
    "    b1_bins = b1_bin_dict[which_binning]['edges']\n",
    "    b1_freqs = b1_bin_dict[which_binning]['freqs']\n",
    "    b2_bins = b2_bin_dict[which_binning]['edges']\n",
    "    b2_freqs = b2_bin_dict[which_binning]['freqs']\n",
    "    # Normalize up-scaled low-res frequencies and convert to probabilities\n",
    "    b1_probs = b1_freqs / np.sum(b1_freqs)\n",
    "    # Determine the low-res bin for each high-res bin\n",
    "    bin_indices = np.digitize(b2_bins[:-1], b1_bins) - 1  # Map high-res bins to low-res bins\n",
    "    \n",
    "    # Compute counts of high-res bins falling into each low-res bin\n",
    "    counts_per_low_bin = np.bincount(bin_indices, minlength=len(b1_probs))\n",
    "\n",
    "    # Broadcast low-res frequencies to high-res bins, dividing by the count to distribute uniformly\n",
    "    high_res_probs = b1_probs[bin_indices] / counts_per_low_bin[bin_indices]\n",
    "    # Normalize to ensure the result sums to 1\n",
    "    high_res_probs /= np.sum(high_res_probs)\n",
    "    \n",
    "    return high_res_probs, b1_bin_dict, b2_bin_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb76306-ad25-4cb4-be96-d8e8093c5978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_discrete_distributions(data, b):\n",
    "    # 1. Determine the log-spaced bin edges\n",
    "    bin_dict = {}\n",
    "    total_count = len(data)\n",
    "    # 1. Compute the histogram using equal-width bins\n",
    "    counts, bin_edges = np.histogram(data, bins=2**b, density=False)\n",
    "    densities, _ = np.histogram(data, bins=2**b, density=True)\n",
    "    freqs = counts / len(data)\n",
    "    bin_widths = bin_edges[1:] - bin_edges[:-1] \n",
    "    bin_dict['equal'] = {'edges': bin_edges, 'freqs': freqs, 'densities': densities, 'widths': bin_widths}\n",
    "\n",
    "    # 2. Compute the histogram using log-spaced bins\n",
    "    minx, maxx = np.min(data), np.max(data)\n",
    "    log_edges = np.logspace(np.log10(minx), np.log10(maxx), 2**b + 1)\n",
    "    bin_widths = log_edges[1:] - log_edges[:-1]\n",
    "    log_densities, _ = np.histogram(data, bins=log_edges, density=True)\n",
    "    log_counts, _ = np.histogram(data, bins=log_edges, density=False)\n",
    "    log_freqs = log_counts / sum(log_counts)\n",
    "    assert abs(sum(log_freqs) - 1) < 0.001, sum(log_freqs)\n",
    "    bin_dict['log'] = {'edges': log_edges, 'freqs': log_freqs, 'densities': densities, 'widths': bin_widths}\n",
    "\n",
    "    # 3. Compute the histogram using uniform (probability) bins\n",
    "    quantiles = np.linspace(0, 1, 2**b + 1)\n",
    "    uniform_edges = np.quantile(data, quantiles)\n",
    "    bin_widths = uniform_edges[1:] - uniform_edges[:-1]\n",
    "    uniform_freqs, _ = np.histogram(data, bins=uniform_edges, density=True)\n",
    "    uniform_counts, _ = np.histogram(data, bins=uniform_edges, density=False)\n",
    "    uniform_freqs = uniform_counts / total_count\n",
    "    # assert abs(sum(uniform_freqs) - 1) < 0.001, sum(uniform_freqs)\n",
    "    bin_dict['uniform'] = {'edges': uniform_edges, 'freqs': uniform_freqs, 'densities': densities, 'widths': bin_widths}\n",
    "    return bin_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a9c7d2-7159-46a2-9bdf-e561930c9100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_MLE_fit_plot(b, df, stn):\n",
    "    test_fig = figure(title=None,\n",
    "                     width=600, height=500, x_axis_type='log')  \n",
    "\n",
    "    # plot empirical (discrete) distributions using linear and log binning\n",
    "    b=8\n",
    "    label = f'{b}_bit_log'\n",
    "    bin_edges, freqs, log_bin_edges, log_freqs = compute_discrete_distributions(df, b, label, stn)\n",
    "    test_fig.quad(left=bin_edges[:-1], right=bin_edges[1:], top=freqs, bottom=[0 for _ in freqs], \n",
    "                  legend_label=f'{b }bits linear bins', color=Sunset10[0], fill_alpha=0.4, line_color=None)\n",
    "    test_fig.quad(left=log_bin_edges[:-1], right=log_bin_edges[1:], top=log_freqs, \n",
    "                  bottom=[0 for _ in freqs], legend_label=f'{b} bits log bins', color=Sunset10[8], \n",
    "                  line_color=None, fill_alpha=0.6)\n",
    "\n",
    "    # fit and plot a lognormal distribution\n",
    "    ln_shape, ln_loc, ln_scale = lognorm.fit(df[stn], floc=0)  # Fixing location to 0\n",
    "    x = np.logspace(-2, 3, 1000)\n",
    "    ln_mle_pdf = lognorm.pdf(x, ln_shape, loc=0, scale=ln_scale)\n",
    "\n",
    "    # fit and plot an exponential distribution\n",
    "    ex_loc, ex_scale = expon.fit(df[stn], floc=0)\n",
    "    ex_mle_pdf = expon.pdf(x, loc=0, scale=ex_scale)\n",
    "\n",
    "    test_fig.line(x, ln_mle_pdf, color='black', legend_label='LN MLE pdf', line_width=2)\n",
    "    test_fig.line(x, ex_mle_pdf, color='grey', legend_label='EXP MLE pdf', line_width=2)\n",
    "    test_fig.legend.background_fill_alpha = 0.6\n",
    "    test_fig.legend.location = 'top_right'\n",
    "    test_fig.legend.click_policy='hide'\n",
    "    test_fig.xaxis.axis_label = r'$$\\text{Mean Daily Flow } [m^3/s]$$'\n",
    "    test_fig.yaxis.axis_label = r'$$P(X)$$'\n",
    "    test_fig = dpf.format_fig_fonts(test_fig)\n",
    "    return test_fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f07295-e57e-458c-a050-ecbdcd311a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_logspace_bins(data, num_bins):\n",
    "    \"\"\"Create linearly spaced bins in log space over the range of `data`.\"\"\"\n",
    "    min_val, max_val = np.min(data), np.max(data)\n",
    "    log_min, log_max = np.log10(min_val), np.log10(max_val)\n",
    "    bin_edges = np.logspace(log_min, log_max, num_bins + 1)\n",
    "    return bin_edges\n",
    "\n",
    "\n",
    "def calculate_probabilities(data, bin_edges):\n",
    "    \"\"\"Calculate the probabilities of data falling into each bin given `bin_edges`.\"\"\"\n",
    "    counts, _ = np.histogram(data, bins=bin_edges)\n",
    "    probabilities = counts / counts.sum()  # Normalize to get probabilities\n",
    "    return probabilities\n",
    "\n",
    "\n",
    "def aggregate_probabilities(high_res_probs, high_res_bins, low_res_bins):\n",
    "    \"\"\"\n",
    "    Aggregate high-resolution probabilities to match low-resolution bins.\n",
    "    This will sum the high_res_probs that fall within each low-res bin.\n",
    "    \"\"\"\n",
    "    low_res_probs = []\n",
    "    for i in range(len(low_res_bins) - 1):\n",
    "        # Find indices of high-resolution bins that fall within the current low-res bin\n",
    "        indices = np.where((high_res_bins[:-1] >= low_res_bins[i]) & (high_res_bins[:-1] < low_res_bins[i+1]))[0]\n",
    "        # Sum probabilities within the range\n",
    "        low_res_probs.append(np.sum(high_res_probs[indices]))\n",
    "    return np.array(low_res_probs)\n",
    "\n",
    "\n",
    "def compute_kl_divergence(p, q):\n",
    "    \"\"\"Compute KL divergence between two probability distributions `p` and `q`.\"\"\"\n",
    "    # Avoid division by zero and log of zero by adding a small epsilon\n",
    "    # epsilon = 1e-10\n",
    "    # p = np.maximum(p, epsilon)\n",
    "    # q = np.maximum(q, epsilon)\n",
    "    kl_divergence = np.sum(p * np.log(p / q))\n",
    "    return kl_divergence\n",
    "\n",
    "    \n",
    "def fit_continuous_distributions(data, bin_edges):\n",
    "    \"\"\"simulate the target using the parametric MLE parameters from the proxy\"\"\"\n",
    "    # fit and plot a lognormal distribution\n",
    "    ln_shape, ln_loc, ln_scale = lognorm.fit(data, floc=0)  # Fixing location to 0\n",
    "    ex_loc, ex_scale = expon.fit(data, floc=0)\n",
    "    kp = fit_kappa4_mle(data)\n",
    "    # kp = constrained_optimization(data)\n",
    "    kde = gaussian_kde(np.log10(data), bw_method='scott')\n",
    "\n",
    "    edges = list(bin_edges)# + [np.inf]\n",
    "    ln_cdf_vals = lognorm.cdf(edges, ln_shape, \n",
    "                              loc=ln_loc, scale=ln_scale)\n",
    "    expon_cdf_vals = expon.cdf(edges, loc=ex_loc, \n",
    "                               scale=ex_scale)\n",
    "    kappa4_cdf_vals = kappa4.cdf(edges, kp['h'], kp['k'],\n",
    "                                 loc=kp['loc'], scale=kp['scale'])\n",
    "    \n",
    "    # Compute the KDE-based CDF at the evaluation points\n",
    "    kde_cdf_vals = np.array([kde.integrate_box_1d(-np.inf, np.log10(xi)) for xi in edges])\n",
    "    \n",
    "    p_sim = pd.DataFrame()\n",
    "    p_sim[f'{stn}_LN'] = np.diff(ln_cdf_vals)\n",
    "    p_sim[f'{stn}_EXP'] = np.diff(expon_cdf_vals)\n",
    "    p_sim[f'{stn}_KP4'] = np.diff(kappa4_cdf_vals)\n",
    "    p_sim[f'{stn}_KDE'] = np.diff(kde_cdf_vals)\n",
    "    # normalize the distributions \n",
    "    p_sim /= p_sim.sum()\n",
    "    # make sure all distributions sum to 1\n",
    "    assert np.isclose(p_sim.sum(), 1, atol=0.0001).all(), p_sim.sum()\n",
    "    bin_midpoints = (np.array(bin_edges[:-1]) + np.array(bin_edges[1:])) / 2\n",
    "    # replace the last bin midpoint with half the previous bin's width\n",
    "    # because our right bin edge is np.inf\n",
    "    # right_bin_midpoint = edges[-2] + (edges[-2] - edges[-3]) / 2.0\n",
    "    # bin_midpoints[-1] = right_bin_midpoint\n",
    "    p_sim['bin_midpoints'] = bin_midpoints\n",
    "    return p_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3095a07c-39c0-40b4-a454-02419cabbe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_quantization_comparison(b1, b2, df, stn):\n",
    "    test_fig = figure(title=None,\n",
    "                     width=600, height=500, x_axis_type='log')  \n",
    "\n",
    "    # plot empirical (discrete) distributions using linear and log binning\n",
    "    colors = ['gold', 'green']\n",
    "    n = 0\n",
    "    for b in [b1, b2]:\n",
    "        clr = colors[n]\n",
    "        label = f'{b}_bit_log'\n",
    "        bin_edges, freqs, log_bin_edges, log_freqs = compute_discrete_distributions(df, b, label, stn)\n",
    "        test_fig.quad(left=log_bin_edges[:-1], right=log_bin_edges[1:], top=log_freqs, \n",
    "                      bottom=[0 for _ in freqs], legend_label=f'{b} bits log bins', color=clr, \n",
    "                      line_color=None, fill_alpha=0.6)\n",
    "        n += 1\n",
    "    # fit and plot a lognormal distribution\n",
    "    ln_shape, ln_loc, ln_scale = lognorm.fit(df[stn], floc=0)  # Fixing location to 0\n",
    "    x = np.logspace(-2, 3, 1000)\n",
    "    ln_mle_pdf = lognorm.pdf(x, ln_shape, loc=0, scale=ln_scale)\n",
    "\n",
    "    kld_test = kl_divergence_between_quantizations(df, b1, b2, stn)\n",
    "    print(f'KLd between {b1} and {b2} bit quantizations = {kld_test:.2f}')\n",
    "\n",
    "    # fit and plot an exponential distribution\n",
    "    ex_loc, ex_scale = expon.fit(df[stn], floc=0)\n",
    "    ex_mle_pdf = expon.pdf(x, loc=0, scale=ex_scale)\n",
    "    test_fig.line(x, ln_mle_pdf, color='black', legend_label='LN MLE pdf', line_width=2)\n",
    "    test_fig.line(x, ex_mle_pdf, color='grey', legend_label='EXP MLE pdf', line_width=2)\n",
    "    test_fig.legend.background_fill_alpha = 0.6\n",
    "    test_fig.legend.location = 'top_right'\n",
    "    test_fig.legend.click_policy='hide'\n",
    "    test_fig.xaxis.axis_label = r'$$\\text{Mean Daily Flow } [m^3/s]$$'\n",
    "    test_fig.yaxis.axis_label = r'$$P(X)$$'\n",
    "    test_fig = dpf.format_fig_fonts(test_fig)\n",
    "    return test_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b791acb-553f-4551-90fb-11695f06f32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qn_df = pd.DataFrame.from_dict(quantization_noise_results, orient='index', columns=test_bs)\n",
    "bounds = pd.DataFrame()\n",
    "qn_fig = figure(width=700, height=400)\n",
    "for p in [2.5, 25, 50, 75, 98.5]:\n",
    "    bounds[p] = [np.percentile(qn_df[c], p) for c in test_bs]\n",
    "bounds.index = test_bs\n",
    "qn_fig.varea(x=test_bs, y1=bounds[2.5], y2=bounds[98.5], alpha=0.4, color='grey', legend_label='95% CI')\n",
    "qn_fig.varea(x=test_bs, y1=bounds[25], y2=bounds[75], alpha=0.4, color='black', legend_label='IQR')\n",
    "qn_fig.line(x=test_bs, y=bounds[50], color='crimson', legend_label='Median', line_dash='dashed', line_width=3)\n",
    "qn_fig.yaxis.axis_label = r\"$$\\text{Noise [bits/sample]}$$\"\n",
    "qn_fig.xaxis.axis_label = r\"$$\\text{Dictionary Size (b)} [2^b = \\text{N symbols}]$$\"\n",
    "qn_fig = dpf.format_fig_fonts(qn_fig)\n",
    "show(qn_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89967d4-a66e-4a28-aa69-86265e361eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stn = filtered_stns[0]\n",
    "test_df = dpf.get_timeseries_data(stn)\n",
    "foo = plot_quantization_comparison(4, 5, test_df, stn)\n",
    "\n",
    "show(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ed28d0-13ef-4168-b906-e6f7f2dc9f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import export_png\n",
    "\n",
    "test_stn = filtered_stns\n",
    "for stn in filtered_stns:\n",
    "    output_folder = 'MLE_plots'\n",
    "    plot_fpath = os.path.join(output_folder, f\"{stn}_LN_and_expon_fits.png\")\n",
    "    if os.path.exists(plot_fpath):\n",
    "        continue\n",
    "    test_df = dpf.get_timeseries_data(stn)\n",
    "    test_df.dropna(subset=[stn], inplace=True)\n",
    "    minx, maxx = test_df[stn].min(), test_df[stn].max()\n",
    "    # print(f'X range is {minx:.1f} to {maxx:.1f} cms')\n",
    "    # print('')\n",
    "    \n",
    "    test_fig = create_MLE_fit_plot(8, test_df, stn)\n",
    "    \n",
    "    export_png(test_fig, filename=plot_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6b54f6-2e22-43d5-bdec-bbf76101782f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "for stn in filtered_stns:\n",
    "    df = dpf.get_timeseries_data(stn)\n",
    "    ln_shape, ln_loc, ln_scale = lognorm.fit(df[stn], floc=0)\n",
    "    expon_loc, expon_scale = expon.fit(df[stn], floc=0)\n",
    "    # kh, kk, k_loc, k_scale = fit_kappa4_mle(df[stn])\n",
    "    # kh, kk, k_loc, k_scale = constrained_optimization(df[stn])\n",
    "    attr_df.loc[attr_df['official_id'] == stn, ['ln_shape', 'ln_loc', 'ln_scale']] = (ln_shape, ln_loc, ln_scale)\n",
    "    attr_df.loc[attr_df['official_id'] == stn, ['expon_loc', 'expon_scale']] = (expon_loc, expon_scale)\n",
    "    # attr_df.loc[attr_df['official_id'] == stn, ['kappa_h', 'kappa_k', 'kappa_loc', 'kappa_scale']] = (kh, kk, k_loc, k_scale)\n",
    "    for b in test_bs:\n",
    "        attr_df.loc[attr_df['official_id'] == stn, f'{b}_quantization_noise'] = kl_divergence_between_quantizations(df, b, max(test_bs), stn) \n",
    "    n += 1\n",
    "    if n % 150 == 0:\n",
    "        print(f'    ...{n}/{len(filtered_stns)} completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280b1651-8d0b-4f44-8c4c-887fc0d034b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the MLE parameters to dicts for easier access\n",
    "ln_dict = (\n",
    "    attr_df\n",
    "    .set_index('official_id')[['ln_shape', 'ln_loc', 'ln_scale']]\n",
    "    .to_dict(orient='index')\n",
    ")\n",
    "expon_dict = (\n",
    "    attr_df\n",
    "    .set_index('official_id')[['expon_loc', 'expon_scale']]\n",
    "    .to_dict(orient='index')\n",
    ")\n",
    "\n",
    "# kappa_dict = (\n",
    "#     attr_df\n",
    "#     .set_index('official_id')[['kappa_a', 'kappa_b', 'kappa_c', 'kappa_d']]\n",
    "#     .to_dict(orient='index')\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2f9bb2-02f0-4789-88e3-3ba472b410ce",
   "metadata": {},
   "source": [
    "## Compute the \"noise\" added to a discriminant value due to assuming an error distribution as a prior\n",
    "\n",
    "Rating curve uncertainty is a hard problem in hydrology.  Instead of treating daily flow observations as discrete measurements with the fixed (often overzealous) precision that it is published by governing agencies, we can assume some kind of basic error model and test how much the error model distorts the information in the distribution.  In other words, how much noise/uncertainty is added for any model error.  \n",
    "\n",
    "Below we'll test a range of uniform error distributions as models for the observations.  We'll take an example streamflow record, and we'll quantize it to a range of dictionary sizes in two ways.  One way is to bin the observations as they are, we'll refer to this as the \"deterministic\" treatment.  The second way is to apply a series of error distribution models, calling it the \"stochastic treatment\", and bin the observations by counting the fraction of the observation distribution interval that lies in each bin.  In other words, we'll count partial observations in proportion to where they fall over the binning intervals as opposed to counting a whole observation based on the interval alone.\n",
    "\n",
    "The quantization will take in a bitrate $b$, and it will divide and log-transform the measured interval $(\\log(x_\\text{min}),\\log(x_\\text{max}))$ into $2^b$ log-spaced bins.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3d5182-e940-4556-a8a1-3706124bd8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_uniform_bins(df, stn, bitrate):\n",
    "    n_bins = 2**bitrate\n",
    "    min_log_val = np.log10(df[stn].min())\n",
    "    max_log_val = np.log10(df[stn].max())\n",
    "\n",
    "    # set the bin edges to be evenly spaced between the\n",
    "    # observed range of the proxy/donor series\n",
    "    # np.digitize will assign 0 for out-of-range values at left\n",
    "    # and n_bins + 1 for out-of-range values at right\n",
    "    log_bin_edges = np.linspace(\n",
    "        min_log_val,\n",
    "        max_log_val,\n",
    "        n_bins + 1,\n",
    "    ).flatten()\n",
    "\n",
    "    # convert back to linear space\n",
    "    bin_edges = [10**e for e in log_bin_edges]\n",
    "\n",
    "    # there should be n_bins edges which define n_bins - 1 bins\n",
    "    # this is to reserve 2 bin for out-of-range values to the right\n",
    "    assert len(bin_edges) == n_bins + 1\n",
    "    return bin_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c7cda9-e204-4bda-8566-f59e4215c010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_error_to_observations(df, stn, bitrate=None, error=0.1):\n",
    "    min_q, max_q = df[stn].min() - 1e-9, df[stn].max() + 1e-9\n",
    "    assert min_q > 0\n",
    "    # use equal width bins in log10 space\n",
    "    bin_edges = compute_log_uniform_bins(df, stn, bitrate)\n",
    "    # df[f'{bitrate}_bits_quantized'] = np.digitize(df[stn], bin_edges)\n",
    "    fractional_obs_counts = dpf.error_adjusted_fractional_bin_counts(\n",
    "        df[stn], np.array(bin_edges), bitrate, error_factor=error\n",
    "    )\n",
    "    label = f'{stn}_{int(100*error)}_error'\n",
    "    count_df = pd.DataFrame(index=range(2**bitrate))\n",
    "    count_df[label] = 0\n",
    "    count_df[label] += fractional_obs_counts\n",
    "    count_df.fillna(0, inplace=True)\n",
    "    n_obs = np.nansum(count_df[label])\n",
    "    # normalize p_obs and p_sim\n",
    "    return count_df[label].values / n_obs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b54815e-3e59-4712-a405-20c6d594fa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_unadjusted_counts(df, stn, bitrate):\n",
    "    bin_edges = compute_log_uniform_bins(df, stn, bitrate)\n",
    "    label = f'{stn}_simple_{bitrate}bits'\n",
    "    df[label] = np.digitize(df[stn], bin_edges)\n",
    "    # print(df[[stn, f'{stn}_quantized_{bitrate}bits']].head(4))\n",
    "    # count the occurrences of each quantized value\n",
    "    # the \"simulated\" series is the proxy/donor series\n",
    "    # and the \"observed\" series is the target location\n",
    "    obs_count_df = df.groupby(label).count()\n",
    "    count_df = pd.DataFrame(index=range(2**bitrate))\n",
    "    count_df[label] = 0\n",
    "    count_df[label] += obs_count_df[stn]\n",
    "    count_df.fillna(0, inplace=True)\n",
    "    adjusted_p = count_df / obs_count_df[stn].sum()\n",
    "    return adjusted_p.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bed0c3a-8f3d-4b9a-8289-3aca49a6188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distortion(inputs):\n",
    "    df, stn, b, err = inputs\n",
    "    simple_frequencies = compute_unadjusted_counts(df, stn, b)\n",
    "    error_adjusted_frequencies = apply_error_to_observations(df, stn, bitrate=b, error=err)\n",
    "    # compute KL divergence between the simple and adjusted frequencies\n",
    "    # this represents the distortion due to the error model\n",
    "    mask = (simple_frequencies > 0) & (error_adjusted_frequencies > 0)\n",
    "    distortion = np.zeros_like(simple_frequencies)\n",
    "    distortion[mask] = simple_frequencies[mask] * np.log2(simple_frequencies[mask] / error_adjusted_frequencies[mask])\n",
    "    kld = sum(distortion)\n",
    "    return stn, kld, b, err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0c41d0-cbf9-48a2-9130-99e946da40b5",
   "metadata": {},
   "source": [
    "## Pairwise Processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca80fbc4-dd73-4937-9ce7-263599d7d3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# generate all combinations of pairs of station ids\n",
    "id_pairs = list(itertools.combinations(filtered_stns, 2))\n",
    "print(f' There are {len(id_pairs)} unique pairings in the dataset')\n",
    "# shuffle the pairs to make testing smaller batches more robust\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(id_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7043f8a9-4541-447d-8f6f-31b02c95ab2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the attributes file with catchment geometries\n",
    "geom_file = 'BCUB_watershed_attributes_updated.geojson'\n",
    "bcub_gdf = gpd.read_file(os.path.join(os.getcwd(), 'data', geom_file))\n",
    "bcub_gdf.columns = [c.lower() for c in bcub_gdf.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46464524-4afd-466a-a354-e4e0ab0b4b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a revision date for the results output file\n",
    "revision_date = '20241112'\n",
    "\n",
    "# how many pairs to compute in each batch\n",
    "batch_size = 5000\n",
    "# batch_size = 10\n",
    "\n",
    "# # what percentage of 365 observations in a year counts as a \"complete\" year\n",
    "# completeness_threshold = 0.9\n",
    "# min_observations = 365 * 0.9\n",
    "\n",
    "# station pairs with less than min_years concurrent years of data are excluded (for concurrent analysis),\n",
    "# stations with less than min_years are excluded (for non-concurrent analysis),\n",
    "min_years = 1 #[2, 3, 4, 5, 10]\n",
    "\n",
    "# a prior is applied to q in the form of a uniform array of 10**c pseudo-counts \"c\"\n",
    "# this prior is used to test the effect of the choice of prior on the model\n",
    "pseudo_counts = [-5, -4, -3, -2, -1, -0.5, -0.2, -0.1, 0, 0.1, 0.2, 0.5, 1, 2, 3, 4, 5]\n",
    "\n",
    "# set the number of quantization levels to test, equal to 2^bitrate\n",
    "bitrates = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
    "\n",
    "# Preload all records into a dictionary for fast lookup\n",
    "records_dict = bcub_gdf.copy().set_index('official_id').to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34b5164-2fdc-4bfd-b092-9b240d1e4f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_batch_generator(df, id_pairs_filtered, bitrate, \n",
    "                          min_years, use_partial_counts):\n",
    "    batch_inputs = []\n",
    "    for proxy, target in id_pairs_filtered:\n",
    "        \n",
    "        proxy_dict = records_dict.get(proxy, {})\n",
    "        target_dict = records_dict.get(target, {})\n",
    "\n",
    "        proxy_dict['official_id'] = proxy\n",
    "        target_dict['official_id'] = target\n",
    "\n",
    "        assert 'geometry' in proxy_dict.keys(), proxy_dict.keys()\n",
    "        assert 'geometry' in target_dict.keys(), target_dict.keys()\n",
    "        \n",
    "        batch = [\n",
    "            proxy_dict, target_dict, bitrate, \n",
    "            min_years,\n",
    "        ]\n",
    "        batch_inputs.append(batch)\n",
    "    return batch_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8d2e6c-89f7-4d04-9d5b-54659a58d28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir = os.path.join(os.getcwd(), 'data/', 'temp')\n",
    "if not os.path.exists(temp_dir):\n",
    "    os.makedirs(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67401f9c-9169-4024-becc-6e723bd01cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulated_parametric_probabilities(bin_edges, proxy, target):\n",
    "    \"\"\"simulate the target using the parametric MLE parameters from the proxy\"\"\"\n",
    "    ln_params = ln_dict[proxy.id]\n",
    "    exp_params = expon_dict[proxy.id]\n",
    "    p_sim = pd.DataFrame()\n",
    "    bin_edges = [0] + bin_edges + [np.inf]\n",
    "\n",
    "    # ln_pdf_vals = lognorm.pdf(bin_midpoints, ln_params['ln_shape'], loc=ln_params['ln_loc'], scale=ln_params['ln_scale'])\n",
    "    # expon_pdf_vals = expon.pdf(bin_midpoints, loc=ln_params['ln_loc'], scale=ln_params['ln_scale'])\n",
    "    ln_cdf_vals = lognorm.cdf(bin_edges, ln_params['ln_shape'], \n",
    "                              loc=ln_params['ln_loc'], scale=ln_params['ln_scale'])\n",
    "    expon_cdf_vals = expon.cdf(bin_edges, loc=ln_params['ln_loc'], \n",
    "                               scale=ln_params['ln_scale'])\n",
    "    # p_sim[target.ln_pdf_label] = ln_pdf_vals\n",
    "    p_sim[target.ln_cdf_label] = np.diff(ln_cdf_vals)\n",
    "    # p_sim[target.expon_pdf_label] = expon_pdf_vals\n",
    "    p_sim[target.expon_cdf_label] = np.diff(expon_cdf_vals)\n",
    "\n",
    "    # normalize the distributions\n",
    "    p_sim /= p_sim.sum()\n",
    "    assert np.isclose(p_sim.sum(), 1, atol=0.0001).all(), p_sim.sum()\n",
    "\n",
    "    return p_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccdbf85-fe6e-403a-bbdc-edd935ae01d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_KL_divergence(p_obs, p_sim, bitrate, concurrent_data, epsilon=1e-12):\n",
    "    \"\"\"\n",
    "    Processes the Kullback-Leibler (KL) divergence between observed and simulated probability distributions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    p_obs : np.ndarray\n",
    "        The observed probability distribution.\n",
    "    p_sim : pd.DataFrame\n",
    "        A DataFrame containing the simulated probability distributions with different priors.\n",
    "    bitrate : int\n",
    "        The number of bits used for quantizing the observed series.\n",
    "    concurrent_data : bool\n",
    "        A flag indicating whether the data is concurrent.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        A series containing the sum of KL divergences for each simulated distribution.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    Exception\n",
    "        If any value in the simulated distribution is zero, which should not happen due to the addition of pseudo-counts.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The function computes the KL divergence for each simulated distribution in `p_sim`.\n",
    "    - It ensures that the probability distributions sum to 1 before computing the divergence.\n",
    "    - If the data is concurrent, the divergence labels are prefixed with 'dkl_concurrent_', otherwise 'dkl_nonconcurrent_'.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> p_obs = np.array([0.2, 0.3, 0.5])\n",
    "    >>> p_sim = pd.DataFrame({'q_post_0.1R': [0.1, 0.4, 0.5], 'q_post_0.5R': [0.2, 0.3, 0.5]})\n",
    "    >>> bitrate = 3\n",
    "    >>> concurrent_data = True\n",
    "    >>> sum_dkl = process_KL_divergence(p_obs, p_sim, bitrate, concurrent_data)\n",
    "    >>> print(sum_dkl)\n",
    "    \"\"\"\n",
    "    # dkl_df = uf.compute_kl_divergence(p_obs, p_sim, bitrate, concurrent_data)\n",
    "\n",
    "    # explicitly set data types before vectorization\n",
    "    p = np.array(p_obs, dtype=np.float64)\n",
    "    mask = p > 0\n",
    "    df = pd.DataFrame()\n",
    "    df[\"bin\"] = range(1, 2**bitrate + 1)\n",
    "    df.set_index(\"bin\", inplace=True)\n",
    "    small_val_flags = []\n",
    "    for c in p_sim.columns:\n",
    "        label = \"dkl_nonconcurrent_\" + \"_\".join(c.split(\"_\")[1:])\n",
    "        if concurrent_data is True:\n",
    "            label = \"dkl_concurrent_\" + \"_\".join(c.split(\"_\")[1:])\n",
    "        q = np.array(p_sim[c].values, dtype=np.float64) \n",
    "        small_values = q < epsilon\n",
    "        kld_array = np.zeros_like(p)\n",
    "        q = np.clip(q, epsilon, None)\n",
    "        kld_array[mask] = p[mask] * np.log2(p[mask] / q[mask])\n",
    "        df[label] = kld_array\n",
    "        if np.any(small_values):\n",
    "            small_val_flags.append(c)\n",
    "    n_flags = len(small_val_flags)\n",
    "    q_flag = None\n",
    "    if n_flags > 0:\n",
    "        q_flag = ','.join(small_val_flags)\n",
    "        # print(q_flag)\n",
    "    sum_dkl = df.sum()\n",
    "\n",
    "    if np.any(sum_dkl.values <= 0):\n",
    "        print(f\"negative or zero dkl\")\n",
    "        print(sum_dkl.values)\n",
    "        raise Exception('negative or zero dkl')\n",
    "\n",
    "    return sum_dkl, q_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cc875d-b803-4eb0-88c5-763e9258cf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_divergences(result, p_obs, p_sim, bin_edges, bitrate, concurrent_data):\n",
    "    dkl, q_flag = process_KL_divergence(p_obs, p_sim, bitrate, concurrent_data)\n",
    "\n",
    "    # p = p_obs\n",
    "    # q = p_sim[\"q_sim_no_prior\"].values\n",
    "    # q_uniform = p_sim[\"q_uniform\"].values\n",
    "    result.update(dkl.to_dict())\n",
    "    result['small_q_flag'] = q_flag\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad704a3c-8074-4dae-8987-5a2ed2b2519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_probabilities(\n",
    "    df, proxy, target, bitrate, concurrent_data, #pseudo_counts, p_errors\n",
    "):\n",
    "    target.obs_quantized_label = f\"obs_quantized_{target.id}_{bitrate}b\"\n",
    "    target.sim_quantized_label = f\"sim_quantized_{target.id}_{bitrate}b\"\n",
    "    # compute the bin edges based on equal width in log space\n",
    "    # binning should be done on the 'ground truth' observed range (the target)\n",
    "    bin_edges = dpf.uniform_log_bins(df, target, bitrate)\n",
    "\n",
    "    simple_count_df = dpf.compute_unadjusted_counts(\n",
    "        df, target, bin_edges, bitrate, concurrent_data\n",
    "    )\n",
    "    \n",
    "    p_obs = simple_count_df[target.obs_label].values / simple_count_df[target.obs_label].sum()\n",
    "\n",
    "    # add a uniformly distributed error to the observed data\n",
    "    # and compute probabilities from partial observation counts\n",
    "    # where counts are divided based on the proportion of the bin\n",
    "    # that the measurement error falls within\n",
    "    fractional_obs_counts = dpf.error_adjusted_fractional_bin_counts(\n",
    "        df[target.obs_label], np.array(bin_edges), bitrate, error_factor=0.1\n",
    "    )\n",
    "    fractional_sim_counts = dpf.error_adjusted_fractional_bin_counts(\n",
    "        df[target.sim_label], np.array(bin_edges), bitrate, error_factor=0.1\n",
    "    )\n",
    "\n",
    "    t1 = time()\n",
    "    # print(f' {t1-t0:.2f}s to process fractional bin counts')\n",
    "\n",
    "    partial_count_df = pd.DataFrame(index=range(2**bitrate))\n",
    "    partial_count_df[target.obs_label] = 0\n",
    "    partial_count_df[target.sim_label] = 0\n",
    "    partial_count_df[target.obs_label] += fractional_obs_counts\n",
    "    partial_count_df[target.sim_label] += fractional_sim_counts\n",
    "    partial_count_df.fillna(0, inplace=True)\n",
    "\n",
    "    partial_count_df /= partial_count_df.sum()\n",
    "    \n",
    "    # Check if the sums are close enough to 1 within a tolerance of 0.001\n",
    "    assert np.isclose(partial_count_df.sum(), 1, atol=0.0001).all(), partial_count_df.sum()\n",
    "\n",
    "    p_sim = simulated_parametric_probabilities(bin_edges, proxy, target)\n",
    "    \n",
    "    return p_obs, p_sim, bin_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1d723c-3ba8-49f9-bcd8-8c130bf1d9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(inputs):    \n",
    "    (\n",
    "        proxy,\n",
    "        target,\n",
    "        bitrate,\n",
    "        min_concurrent_years,\n",
    "    ) = inputs\n",
    "    \n",
    "    proxy_id, target_id = proxy['official_id'], target['official_id']\n",
    "    bitrate = int(bitrate)\n",
    "\n",
    "    # create a result dict object for tracking results of the batch comparison\n",
    "    result = {\n",
    "        \"proxy\": proxy_id,\n",
    "        \"target\": target_id,\n",
    "        \"bitrate\": bitrate,\n",
    "        \"min_concurrent_years\": min_concurrent_years,\n",
    "    }\n",
    "    station_info = {\"proxy\": proxy, \"target\": target}\n",
    "\n",
    "    # check if the polygons are nested\n",
    "    result[\"nested_catchments\"] = dpf.check_if_nested(\n",
    "        proxy, target\n",
    "    )\n",
    "\n",
    "    # for stn in pair:\n",
    "    proxy = dpf.Station(station_info[\"proxy\"])\n",
    "    target = dpf.Station(station_info[\"target\"])\n",
    "    target.ln_pdf_label = f'{target.id}_sim_lognorm_pdf'\n",
    "    target.ln_cdf_label = f'{target.id}_sim_lognorm_cdf'\n",
    "    target.expon_pdf_label = f'{target.id}_sim_expon_pdf'\n",
    "    target.expon_cdf_label = f'{target.id}_sim_expon_cdf'\n",
    "\n",
    "    # compute spatial distance\n",
    "    p1, p2 = (\n",
    "        station_info[\"proxy\"][\"geometry\"].centroid,\n",
    "        station_info[\"target\"][\"geometry\"].centroid,\n",
    "    )\n",
    "    # compute the distance between catchment centroids (km)\n",
    "    centroid_distance = p1.distance(p2) / 1000\n",
    "    result[\"centroid_distance\"] = round(centroid_distance, 2)\n",
    "    if centroid_distance > 1000:\n",
    "        return None\n",
    "\n",
    "    if np.isnan(target.drainage_area_km2):\n",
    "        raise ValueError(f\"No drainage area for {target_id}\")\n",
    "    if np.isnan(proxy.drainage_area_km2):\n",
    "        raise ValueError(f\"No drainage area for {proxy_id}\")\n",
    "\n",
    "    # Retrieve the data for both stations\n",
    "    # this is all data, including non-concurrent\n",
    "    adf = dpf.retrieve_nonconcurrent_data(proxy_id, target_id)\n",
    "\n",
    "    assert ~adf.empty, \"No data returned.\"\n",
    "\n",
    "    for stn in [proxy, target]:\n",
    "        adf = dpf.transform_and_jitter(adf, stn)\n",
    "\n",
    "    # simulate flow at the target based on equal unit area runoff scaling\n",
    "    adf[target.sim_label] = adf[proxy.id] * (\n",
    "        target.drainage_area_km2 / proxy.drainage_area_km2\n",
    "    )\n",
    "\n",
    "    # filter for the concurrent data\n",
    "    df = adf.copy().dropna(subset=[proxy_id, target_id], how=\"any\")\n",
    "    result[\"num_concurrent_obs\"] = len(df)\n",
    "    \n",
    "    if df.empty:\n",
    "        num_complete_concurrent_years = 0\n",
    "    else:\n",
    "        df.reset_index(inplace=True)\n",
    "        num_complete_concurrent_years = dpf.count_complete_years(df, 'time', proxy_id)\n",
    "        \n",
    "    counts = df[[proxy_id, target_id]].count(axis=0)\n",
    "    counts = adf.count(axis=0)\n",
    "    proxy.n_obs, target.n_obs = counts[proxy_id], counts[target_id]\n",
    "    result[f\"proxy_n_obs\"] = proxy.n_obs\n",
    "    result[f\"target_n_obs\"] = target.n_obs\n",
    "    result[f\"proxy_frac_concurrent\"] = len(df) / proxy.n_obs\n",
    "    result[f\"target_frac_concurrent\"] = len(df) / target.n_obs\n",
    "\n",
    "    if (counts[proxy_id] == 0) or (counts[target_id] == 0):\n",
    "        print(f\"   Zero observations.  Skipping.\")\n",
    "        return None\n",
    "\n",
    "    # process the PMFs and divergences for concurrent data\n",
    "    # using a range of uniform priors via pseudo counts\n",
    "    if num_complete_concurrent_years > min_concurrent_years:\n",
    "        # df is concurrent data, so the results\n",
    "        # are updating concurrent data here\n",
    "        # df, proxy, target, bitrate, concurrent_data, partial_counts, pseudo_counts\n",
    "        concurrent_data = True\n",
    "        p_obs, p_sim, bin_edges = process_probabilities(\n",
    "            df, proxy, target, bitrate, concurrent_data, #pseudo_counts, p_errors\n",
    "        )\n",
    "    if (target.n_obs > 365 * 0.9) & (proxy.n_obs > 365 * 0.9):\n",
    "        # adf is all data (includes non-concurrent), so the results\n",
    "        # are updated if both series meet the minimum length\n",
    "        concurrent_data = False\n",
    "        p_obs, p_sim, bin_edges = process_probabilities(\n",
    "            adf, proxy, target, bitrate, concurrent_data,# pseudo_counts, p_errors\n",
    "        )\n",
    "        \n",
    "    result = process_divergences(\n",
    "        result, p_obs, p_sim, bin_edges, bitrate, concurrent_data\n",
    "    )\n",
    "\n",
    "    noise = process_noise(result, p_obs, p_sim, bin_edges, bitrate, concurrent_data)\n",
    "    # result['underspecified_model_flag'] = underspecified_flag\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ffb95e-ccbb-45b5-b7c0-e1a82c00d57e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the 'process' variable is here so jupyter doesn't go computing \n",
    "# a million rows per iteration when the book is built for pushing to github pages.\n",
    "# reordered_bitrates = [4, 6, 8, 10, 12, 3, 5, 7, 9, 11]\n",
    "process = True\n",
    "partial_counts = False\n",
    "if process: \n",
    "    for bitrate in reordered_bitrates:\n",
    "        print(f'Processing pairs at {bitrate} bits quantization ')\n",
    "        results_fname = f'KL_parametric_fits_{bitrate}bits_{revision_date}.csv'\n",
    "\n",
    "        out_fpath = os.path.join('data/', 'parametric_divergence_test', results_fname)\n",
    "        if os.path.exists(out_fpath):\n",
    "            continue\n",
    "\n",
    "        n_batches = max(len(id_pairs) // batch_size, 1)\n",
    "        batches = np.array_split(np.array(id_pairs, dtype=object), n_batches)\n",
    "        n_pairs = len(id_pairs)\n",
    "        print(\n",
    "            f\"    Processing {n_pairs} pairs in {n_batches} batches at {bitrate} bits\"\n",
    "        )\n",
    "        batch_no = 1\n",
    "        batch_files = []\n",
    "        t0 = time()\n",
    "        # error_df = error_model_df[error_model_df['bitrate'] == bitrate].copy()\n",
    "        for batch_ids in batches:\n",
    "            print(f'Starting batch {batch_no}/{len(batches)} processing.')\n",
    "            batch_fname = results_fname.replace('.csv', f'_batch_{batch_no:03d}.csv')\n",
    "            batch_output_fpath = os.path.join(temp_dir, batch_fname)\n",
    "            if os.path.exists(batch_output_fpath):\n",
    "                batch_files.append(batch_output_fpath)\n",
    "                batch_no += 1\n",
    "                continue\n",
    "            \n",
    "            # define the input array for multiprocessing\n",
    "            inputs = input_batch_generator(bcub_gdf, batch_ids, bitrate,\n",
    "                     min_years, partial_counts)\n",
    "\n",
    "            with mp.Pool(20) as pool:\n",
    "                results = pool.map(process_batch, inputs)\n",
    "                results = [r for r in results if r is not None]\n",
    "\n",
    "            batch_result = pd.DataFrame(results)\n",
    "            if batch_result.empty:\n",
    "                print('Empty batch.  Skipping')\n",
    "            else:\n",
    "                batch_result.to_csv(batch_output_fpath, index=False)\n",
    "                print(f\"    Saved {len(batch_result)} new results to file.\")\n",
    "            \n",
    "            batch_files.append(batch_output_fpath)\n",
    "            t2 = time()\n",
    "            print(f'    Processed {len(batch_ids)} pairs at ({bitrate} bits) in {t2 - t0:.1f} seconds')\n",
    "            batch_no += 1\n",
    "            \n",
    "        print(f'    Concatenating {len(batch_files)} batch files.')\n",
    "        if len(batch_files) > 0:\n",
    "            all_results = pd.concat([pd.read_csv(f, engine='pyarrow') for f in batch_files], axis=0)\n",
    "            all_results.to_csv(out_fpath, index=False)\n",
    "            if os.path.exists(out_fpath):\n",
    "                for f in batch_files:\n",
    "                    os.remove(f)\n",
    "            print(f'    Wrote {len(all_results)} results to {out_fpath}')\n",
    "        else:\n",
    "            print('    No new results to write to file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c61f5c-5df5-41f5-908b-b81fac43ea1f",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f17218-aa35-420e-a9db-2a5360efdcaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
