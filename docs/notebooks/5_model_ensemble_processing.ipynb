{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a31275eb",
   "metadata": {},
   "source": [
    "# Multi-model ensembles\n",
    "\n",
    "The FDCs estimated by individual models are combined into multi-model ensembles in this notebook.  Here, the goal is to evaluate whether multi-model ensembles can exploit low rank correlation to improve performance overall.\n",
    "\n",
    "In this section:\n",
    "\n",
    "1. We organize and align the outputs from all FDC-estimation methods to ensure consistent comparison across catchments and performance metrics.\n",
    "\n",
    "2. We construct inter-model ensembles by combining estimates from parametric, kNN, and LSTM approaches, and define the rules used for averaging or aggregation.\n",
    "\n",
    "3. We evaluate ensemble performance against that of the individual models, examining when complementary information improves accuracy and when it does not.\n",
    "\n",
    "4. We summarize patterns in ensemble gains and identify where model disagreement limits the benefits of combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "558cd32f-b562-4502-ab63-826aede8f321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import json\n",
    "\n",
    "# from utils.kde_estimator import KDEEstimator\n",
    "from utils.fdc_estimator_context import FDCEstimationContext\n",
    "from utils.fdc_data import StationData\n",
    "from utils.evaluation_metrics import EvaluationMetrics\n",
    "\n",
    "import utils.data_processing_functions as dpf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f15a258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bitrate PMFs are we working with\n",
    "regularization_type = 'discrete'  # 'kde' or 'discrete'\n",
    "\n",
    "bitrate = 10 if regularization_type == 'discrete' else 10  # number of bins = 2**bitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e87c598e",
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_fpath = 'data/catchment_attributes_with_runoff_stats.csv'\n",
    "attr_df = pd.read_csv(attr_fpath, dtype={'Official_ID': str})\n",
    "station_ids = sorted(attr_df['official_id'].unique().tolist())\n",
    "\n",
    "# streamflow folder from (updated) HYSETS\n",
    "HYSETS_DIR = Path('/home/danbot/code/common_data/HYSETS')\n",
    "hs_df = pd.read_csv('data/HYSETS_watershed_properties.txt', sep=';')\n",
    "hs_df = hs_df[hs_df['Official_ID'].isin(station_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "870b0bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "watershed_id_dict = {row['Watershed_ID']: row['Official_ID'] for _, row in hs_df.iterrows()}\n",
    "# and the inverse\n",
    "official_id_dict = {row['Official_ID']: row['Watershed_ID'] for _, row in hs_df.iterrows()}\n",
    "# also for drainage areas\n",
    "da_dict = {row['Official_ID']: row['Drainage_Area_km2'] for _, row in hs_df.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39c08431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the baseline PMFs from the previous notebook\n",
    "baseline_distribution_folder = Path(os.getcwd()) / 'data' / 'baseline_distributions' \n",
    "\n",
    "baseline_pmf_path = baseline_distribution_folder / f'{bitrate:02d}_bits' /  f'pmf_obs.csv'\n",
    "if regularization_type == 'kde':\n",
    "    baseline_pmf_path = baseline_distribution_folder / f'{bitrate:02d}_bits' /  f'pmf_kde.csv'\n",
    "\n",
    "pmf_df = pd.read_csv(baseline_pmf_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1356ccc6-3d6d-4e08-bdbc-53a798187d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 712 monitored basins concurrent with LSTM ensemble results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve LSTM ensemble predictions\n",
    "LSTM_ensemble_result_folder = '/home/danbot/code/neuralhydrology/data/ensemble_results_20250514'# uses NSE mean as loss function\n",
    "# LSTM_ensemble_result_folder = '/home/danbot/code/neuralhydrology/data/ensemble_results_20250627'# uses NSE 95% as loss function\n",
    "lstm_result_files = os.listdir(LSTM_ensemble_result_folder)\n",
    "lstm_result_stns = [e.split('_')[0] for e in lstm_result_files]\n",
    "\n",
    "# filter for the common stations between BCUB region and LSTM-compatible (i.e. 1980-)\n",
    "daymet_concurrent_stations = np.intersect1d(np.intersect1d(station_ids, lstm_result_stns), pmf_df.columns)\n",
    "# assert '012414900' in daymet_concurrent_stations\n",
    "print(f'There are {len(daymet_concurrent_stations)} monitored basins concurrent with LSTM ensemble results.')\n",
    "'08JE027' in daymet_concurrent_stations\n",
    "'12392895' in daymet_concurrent_stations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedee171",
   "metadata": {},
   "source": [
    "Load the global mean PMF and resample to the higher resolution evaluation grid (12 bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8333e015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station 05BG002 has 4 complete hydrological years of data.\n",
      "Station 08JE005 has 4 complete hydrological years of data.\n",
      "Station 08ME015 has 4 complete hydrological years of data.\n",
      "Station 08NG004 has 4 complete hydrological years of data.\n",
      "Station 08PA001 has 4 complete hydrological years of data.\n",
      "Station 12073000 has 4 complete hydrological years of data.\n",
      "Station 12164000 has 4 complete hydrological years of data.\n",
      "Station 12392895 has 4 complete hydrological years of data.\n",
      "Station 12444490 has 4 complete hydrological years of data.\n",
      "Station 15081614 has 4 complete hydrological years of data.\n"
     ]
    }
   ],
   "source": [
    "# load the pre-computed dictionary of complete years of record for each station\n",
    "complete_year_stats_fpath = os.path.join('data', 'complete_year_stats.npy')\n",
    "complete_year_stats = np.load(complete_year_stats_fpath, allow_pickle=True).item()\n",
    "\n",
    "meet_min_hyd_years = []\n",
    "for stn in complete_year_stats.keys():\n",
    "    if len(complete_year_stats[stn]['hyd_years']) >= 5:\n",
    "        meet_min_hyd_years.append(stn)\n",
    "    else:\n",
    "        print(f'Station {stn} has {len(complete_year_stats[stn][\"hyd_years\"])} complete hydrological years of data.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fff5f866-6b19-4228-a9b7-5fe577060b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After exclusions, 712 stations remain for FDC estimation.\n"
     ]
    }
   ],
   "source": [
    "# see Notebook 1 for details on these exclusions\n",
    "exclude_stations = ['08FA009', '08GA037', '08NC003', '12052500', '12090480', '12107950', '12108450', '12119300', \n",
    "                    '12119450', '12200684', '12200762', '12203000', '12409500', '15056070', '15081510',\n",
    "                    '12323760', '12143700', '12143900', '12398000', '12058800', '12137800', '12100000']\n",
    "\n",
    "pmf_stations = np.intersect1d(meet_min_hyd_years, daymet_concurrent_stations)\n",
    "official_ids_to_include = [s for s in pmf_stations if s not in exclude_stations]\n",
    "print(f'After exclusions, {len(official_ids_to_include)} stations remain for FDC estimation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0bd3b3e-a654-4cb3-8a0d-85c322d2378f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the predicted parameter results\n",
    "parameter_prediction_results_folder = os.path.join('data', 'results', 'parameter_prediction_results', )\n",
    "predicted_params_fpath   = os.path.join(parameter_prediction_results_folder, 'OOS_parameter_predictions.csv')\n",
    "rdf = pd.read_csv(predicted_params_fpath, index_col=['official_id'], dtype={'official_id': str})\n",
    "predicted_param_dict = rdf.to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d716f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Using all stations in the catchment data with a baseline PMF (validated): 1007\n"
     ]
    }
   ],
   "source": [
    "LSTM_forcings_folder = '/home/danbot/neuralhydrology/data/BCUB_catchment_mean_met_forcings_20250320'\n",
    "# LSTM_ensemble_result_folder = '/home/danbot/code/neuralhydrology/data/ensemble_results'\n",
    "attr_df_fpath = os.path.join('data', f'catchment_attributes_with_runoff_stats.csv')\n",
    "\n",
    "methods = ('parametric', 'lstm', 'knn',)\n",
    "# methods = ('knn',)\n",
    "include_pre_1980_data = True  # use only stations with data 1980-present concurrent with Daymet\n",
    "daymet_start_date = '1980-01-01'  # default start date for Daymet data\n",
    "if include_pre_1980_data:\n",
    "    daymet_start_date = '1950-01-01'\n",
    "\n",
    "# load the predicted parameter results (Notebook 3)\n",
    "target_cols = [\n",
    "    'uar_mean_predicted', 'uar_std_predicted', 'uar_median_predicted', 'uar_mad_predicted',\n",
    "    'log_uar_mean_predicted', 'log_uar_std_predicted', 'log_uar_median_predicted', 'log_uar_mad_predicted',\n",
    "]\n",
    "\n",
    "global_min_uar = 5e-5   # see Notebook 1: data\n",
    "global_max_uar = 1e4    # see Notebook 1: data\n",
    "\n",
    "\n",
    "input_data = {\n",
    "        'attr_df_fpath': attr_df_fpath,\n",
    "        'LSTM_forcings_folder': LSTM_forcings_folder,\n",
    "        'LSTM_ensemble_result_folder': LSTM_ensemble_result_folder,\n",
    "        'include_pre_1980_data': include_pre_1980_data,  # use only stations with data 1980-present concurrent with Daymet\n",
    "        'predicted_param_dict': predicted_param_dict,\n",
    "        'eps': 1e-12,\n",
    "        'min_record_length': 5, # minimum record length (years)\n",
    "        'minimum_days_per_month': 20, # minimum number of days with valid data per month\n",
    "        'parametric_target_cols': target_cols,\n",
    "        'all_station_ids': daymet_concurrent_stations,\n",
    "        'baseline_distribution_folder': baseline_distribution_folder,\n",
    "        'delta': 0.001, # maximum uncertainty (by KL divergence) added to the predicted PMF by the uniform mixture ratio\n",
    "        'regularization_type': regularization_type, # use 'kde' or 'discrete'.  if discrete, bitrate must be specified\n",
    "        'bitrate': bitrate,\n",
    "        'complete_year_stats': complete_year_stats,\n",
    "        'year_type': 'hydrological',  # 'calendar' or 'hydrological'\n",
    "        'zero_flow_threshold': 1e-4,  # threshold below which flow is indistinguishable from zero\n",
    "        'global_min_uar': global_min_uar,\n",
    "        'global_max_uar': global_max_uar,\n",
    "    }\n",
    "\n",
    "fdc_context = FDCEstimationContext(**input_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b5553c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_multi_model_ensemble_pmf(stn, rev_date, which_models, result_folder=None):\n",
    "    # load the knn_result\n",
    "    knn_fpath = result_folder / 'knn' / f'{stn}_fdc_results.json'\n",
    "    knn_pmfs = {}\n",
    "    with open(knn_fpath, 'rb') as file:\n",
    "        knn_dict = json.load(file)\n",
    "        # retrieve the PMF for the 4_NN_0_minOverlapPct_attribute_dist_ID2\n",
    "        knn_models = list(knn_dict.keys())\n",
    "        knn_models = [k for k in knn_models if '_NN_attribute_dist_ID2_freqEnsemble' in k]\n",
    "        for m in sorted(knn_models):\n",
    "            knn_pmfs[m] = knn_dict[m]['pmf']\n",
    "            bias = knn_dict[m]['bias']\n",
    "        assert knn_models, f'No knn model found for {stn}'\n",
    "        # knn_pmf = knn_dict[knn_model[0]]['pmf']\n",
    "\n",
    "    lstm_fpath = result_folder / f'lstm_{rev_date}' / f'{stn}_fdc_results.json'\n",
    "    with open(lstm_fpath, 'rb') as file:\n",
    "        lstm_dict = json.load(file)\n",
    "        lstm_pmf = lstm_dict['frequency']['pmf']\n",
    "\n",
    "    param_fpath = result_folder / 'parametric' / f'{stn}_fdc_results.json'\n",
    "    with open(param_fpath, 'rb') as file:\n",
    "        param_dict = json.load(file)\n",
    "        # retrieve the PMF for the 'PredictedMOM' model\n",
    "        param_models = list(param_dict.keys())\n",
    "        param_model = [k for k in param_models if 'PredictedLog' in k]\n",
    "        assert param_model, f'No parametric model found for {stn}'\n",
    "        param_pmf = param_dict[param_model[0]]['pmf']\n",
    "        param_pmf /= np.sum(param_pmf)\n",
    "\n",
    "    # compute an ensemble PMF as the average of the knn and lstm PMFs\n",
    "    # compute the mean ensemble along the support evaluation grid\n",
    "    ensemble_pmfs = {}\n",
    "    # assert knn_pmfs[m].sum() and lstm_pmf.sum() == 1 so it's an equally weighted average\n",
    "    assert np.isclose(np.sum(lstm_pmf), 1.0), f'LSTM PMF does not sum to 1 for {stn}'\n",
    "    assert np.isclose(np.sum(param_pmf), 1.0), f'Parametric PMF does not sum to 1 for {stn}'\n",
    "\n",
    "    for m in knn_pmfs:\n",
    "        assert np.isclose(np.sum(knn_pmfs[m]), 1.0), f'KNN PMF does not sum to 1 for {stn} model {m}'\n",
    "\n",
    "    if which_models == 'knn-lstm':\n",
    "        for m in knn_pmfs:\n",
    "            ensemble_pmf = np.mean([knn_pmfs[m], lstm_pmf], axis=0)\n",
    "            ensemble_pmf /= np.sum(ensemble_pmf)\n",
    "            ensemble_pmfs[m] = ensemble_pmf\n",
    "    elif which_models == 'knn-lstm-parametric':\n",
    "        for m in knn_pmfs:\n",
    "            ensemble_pmf = np.mean([knn_pmfs[m], lstm_pmf, param_pmf], axis=0)\n",
    "            ensemble_pmf /= np.sum(ensemble_pmf)\n",
    "            ensemble_pmfs[m] = ensemble_pmf\n",
    "    elif which_models == 'knn-parametric':\n",
    "        for m in knn_pmfs:\n",
    "            ensemble_pmf = np.mean([knn_pmfs[m], param_pmf], axis=0)\n",
    "            ensemble_pmf /= np.sum(ensemble_pmf)\n",
    "            ensemble_pmfs[m] = ensemble_pmf\n",
    "    else:\n",
    "        raise ValueError(f'which_models {which_models} not recognized, must be one of knn-lstm, knn-lstm-parametric, knn-parametric')\n",
    "    return ensemble_pmfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f14530d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ensemble_divergence(stn, rev_date, pmf_obs_df, which_models, result_folder=None):\n",
    "    station = StationData(fdc_context, stn)\n",
    "    eval_object = EvaluationMetrics(data=station, bitrate=bitrate)\n",
    "    ensemble_pmfs = compute_multi_model_ensemble_pmf(stn, rev_date, which_models=which_models, result_folder=result_folder)\n",
    "    results = {}\n",
    "    for m in ensemble_pmfs:\n",
    "        results[m] = eval_object._evaluate_fdc_metrics_from_pmf(ensemble_pmfs[m], pmf_obs_df[stn].values)\n",
    "    return (results, ensemble_pmfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9561e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembles = {}\n",
    "ensemble_folder = f'data/results/ensemble_results/{bitrate:02d}_bits'\n",
    "results_folder = Path(f'data/results/fdc_estimation_results_{bitrate:02d}_bits')\n",
    "if regularization_type == 'kde':\n",
    "    ensemble_folder = f'data/results/ensemble_results/kde'\n",
    "    results_folder = Path(f'data/results/fdc_estimation_results_kde')\n",
    "assert os.path.exists(results_folder), f'Results folder {results_folder} does not exist'\n",
    "process_ensembles = True\n",
    "for ep in ['knn_lstm/', 'knn_lstm_lognorm/', 'knn_lognorm/']:\n",
    "    folder = Path(ensemble_folder) / ep\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    \n",
    "    model_ensemble = 'knn-lstm'\n",
    "    if ep == 'knn_lstm_lognorm/':\n",
    "        model_ensemble = 'knn-lstm-parametric'\n",
    "    elif ep == 'knn_lognorm/':\n",
    "        model_ensemble = 'knn-parametric'\n",
    "\n",
    "    rev_date = LSTM_ensemble_result_folder.split('_')[-1]\n",
    "    n = 0\n",
    "    if process_ensembles:\n",
    "        max_nae = 0\n",
    "        for stn in official_ids_to_include:\n",
    "            n += 1\n",
    "            ensemble_output_fpath = folder / f'{stn}-{model_ensemble}.csv'\n",
    "            if os.path.exists(ensemble_output_fpath):\n",
    "                # print(f'     {ensemble_output_fpath} already exists, skipping')\n",
    "                continue\n",
    "            try:\n",
    "                results, ensemble_pmfs = compute_ensemble_divergence(stn, rev_date, pmf_df, which_models=model_ensemble, result_folder=results_folder)\n",
    "            except Exception as e:\n",
    "                print(f'Error processing station {stn}: {e}')\n",
    "                raise Exception(f'Error processing station {stn}: {e}')\n",
    "                \n",
    "            results_df = pd.DataFrame(results)\n",
    "            results_df.columns = [e.split('_')[1] for e in results_df.columns]\n",
    "            results_df.index.name = 'metric'\n",
    "            results_df.to_csv(ensemble_output_fpath, index=True)\n",
    "            \n",
    "            if n % 50 == 0:\n",
    "                print(f'{n}/{len(daymet_concurrent_stations)} processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d137b44-8f57-4bd3-85fb-202d89216332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     data/results/ensemble_results/10_bits/knn-lstm_1NN.csv already exists, skipping\n",
      "     data/results/ensemble_results/10_bits/knn-lstm_2NN.csv already exists, skipping\n",
      "     data/results/ensemble_results/10_bits/knn-lstm_3NN.csv already exists, skipping\n",
      "     data/results/ensemble_results/10_bits/knn-lstm_4NN.csv already exists, skipping\n",
      "     data/results/ensemble_results/10_bits/knn-lstm_5NN.csv already exists, skipping\n",
      "     data/results/ensemble_results/10_bits/knn-lstm_6NN.csv already exists, skipping\n",
      "     data/results/ensemble_results/10_bits/knn-lstm_7NN.csv already exists, skipping\n",
      "     data/results/ensemble_results/10_bits/knn-lstm_8NN.csv already exists, skipping\n",
      "     data/results/ensemble_results/10_bits/knn-lstm_9NN.csv already exists, skipping\n",
      "     data/results/ensemble_results/10_bits/knn-lstm_10NN.csv already exists, skipping\n",
      "     data/results/ensemble_results/10_bits/knn-lstm-lognorm_1NN.csv already exists, skipping\n",
      "     data/results/ensemble_results/10_bits/knn-lstm-lognorm_2NN.csv already exists, skipping\n",
      "     data/results/ensemble_results/10_bits/knn-lstm-lognorm_3NN.csv already exists, skipping\n",
      "     data/results/ensemble_results/10_bits/knn-lstm-lognorm_4NN.csv already exists, skipping\n",
      "     data/results/ensemble_results/10_bits/knn-lstm-lognorm_5NN.csv already exists, skipping\n",
      "     data/results/ensemble_results/10_bits/knn-lstm-lognorm_6NN.csv already exists, skipping\n",
      "     data/results/ensemble_results/10_bits/knn-lstm-lognorm_7NN.csv already exists, skipping\n",
      "     data/results/ensemble_results/10_bits/knn-lstm-lognorm_8NN.csv already exists, skipping\n",
      "     data/results/ensemble_results/10_bits/knn-lstm-lognorm_9NN.csv already exists, skipping\n",
      "     data/results/ensemble_results/10_bits/knn-lstm-lognorm_10NN.csv already exists, skipping\n",
      "     data/results/ensemble_results/10_bits/knn-lognorm_1NN.csv already exists, skipping\n",
      "     data/results/ensemble_results/10_bits/knn-lognorm_2NN.csv already exists, skipping\n",
      "     data/results/ensemble_results/10_bits/knn-lognorm_3NN.csv already exists, skipping\n",
      "     data/results/ensemble_results/10_bits/knn-lognorm_4NN.csv already exists, skipping\n",
      "     data/results/ensemble_results/10_bits/knn-lognorm_5NN.csv already exists, skipping\n",
      "     data/results/ensemble_results/10_bits/knn-lognorm_6NN.csv already exists, skipping\n",
      "     data/results/ensemble_results/10_bits/knn-lognorm_7NN.csv already exists, skipping\n",
      "     data/results/ensemble_results/10_bits/knn-lognorm_8NN.csv already exists, skipping\n",
      "     data/results/ensemble_results/10_bits/knn-lognorm_9NN.csv already exists, skipping\n",
      "     data/results/ensemble_results/10_bits/knn-lognorm_10NN.csv already exists, skipping\n"
     ]
    }
   ],
   "source": [
    "for ep in ['knn_lstm/', 'knn_lstm_lognorm/', 'knn_lognorm/']:\n",
    "    folder = Path(ensemble_folder) / ep\n",
    "    if not process_ensembles:\n",
    "        break\n",
    "    which_ensemble = '-'.join(ep.split('/')[0].split('_'))\n",
    "    nn_ensemble_results = {which_ensemble: {}}    \n",
    "\n",
    "    for n in range(1, 11):\n",
    "        fname = f'{which_ensemble}_{n}NN.csv'\n",
    "        ensemble_path = os.path.join(ensemble_folder, fname)\n",
    "        if os.path.exists(ensemble_path):\n",
    "            print(f'     {ensemble_path} already exists, skipping')\n",
    "            continue\n",
    "        nn_results = []\n",
    "        for f in os.listdir(folder):\n",
    "            stn = f.split('.')[0]\n",
    "            df = pd.read_csv(os.path.join(folder, f))\n",
    "            if 'Unnamed: 0' in df.columns:\n",
    "                df.rename({'Unnamed: 0': 'metric'}, axis=1, inplace=True)\n",
    "            df.set_index('metric', inplace=True)\n",
    "            res = df[[str(n)]].to_dict()\n",
    "            res[str(n)]['stn_id'] = stn\n",
    "            nn_results.append(res[str(n)])\n",
    "\n",
    "        nn_df = pd.DataFrame(nn_results)\n",
    "        nn_df.set_index('stn_id', inplace=True)\n",
    "        nn_df.to_csv(ensemble_path, index=True)\n",
    "        print(f'    ...saved {len(nn_df)} results to {fname}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1267123b",
   "metadata": {},
   "source": [
    "### Concatenate all results into a single data structure for easier plotting and comparison\n",
    "\n",
    "Compute baseline values to represent the \"null\" models of using the global mean PMF, and the uniform distributions for all locations.  These are benchmarks to help understand how much value is added by using different models to predict FDCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b83a110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 712 completed stations in lstm_20250514 results folder.\n",
      "   Loading parametric results\n",
      "   Loading lstm results\n",
      "   Loading knn results\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "results_dfs = {}\n",
    "lstm_rev_date = LSTM_ensemble_result_folder.split('_')[-1]\n",
    "sub_folder = f'lstm_{lstm_rev_date}'\n",
    "method_results_folder = Path(os.path.join('data', 'results', f'additional_results'))\n",
    "completed_stns = [c.split('_')[0] for c in os.listdir(os.path.join(results_folder, 'knn'))]\n",
    "print(f'Found {len(set(completed_stns))} completed stations in {sub_folder} results folder.')\n",
    "\n",
    "for method in ['parametric', 'lstm', 'knn']:\n",
    "    print(f'   Loading {method} results')\n",
    "    method_results_fpath =  method_results_folder / f'{method}_all_results_{bitrate:02d}_bits.csv'\n",
    "    if regularization_type == 'kde':\n",
    "        method_results_fpath =  method_results_folder / f'{method}_all_results_kde.csv'\n",
    "    \n",
    "    if method == 'lstm':\n",
    "        rev_date = LSTM_ensemble_result_folder.split('_')[-1]\n",
    "        method_results_fpath = method_results_folder / f'{method}_all_results_{rev_date}_{bitrate:02d}_bits.csv'\n",
    "        \n",
    "    if os.path.exists(method_results_fpath):\n",
    "        results_dfs[method] = pd.read_csv(method_results_fpath, dtype={'Official_ID': str}).dropna(axis=1, how='all')\n",
    "    else:\n",
    "        print(f'   {method} results not found in {method_results_fpath}, loading from individual station files...')\n",
    "        res_folder = os.path.join(results_folder, method)\n",
    "        if method == 'lstm':\n",
    "            res_folder = os.path.join(results_folder, f'{method}_{rev_date}')\n",
    "        args = [(stn, res_folder, method) for stn in completed_stns]\n",
    "\n",
    "        with Pool() as pool:\n",
    "            results_list = pool.map(dpf.load_results, args)\n",
    "\n",
    "        merged = pd.concat(results_list, ignore_index=True)\n",
    "        \n",
    "        bad_dkl = merged[merged['KLD'].isna() | (merged['KLD'] < 0)].copy()\n",
    "        if not bad_dkl.empty:\n",
    "            print(f'Warning: {len(bad_dkl)} {method} rows with NaN or negative DKL values.')\n",
    "            bad_stns = bad_dkl['Official_ID'].values\n",
    "            raise Exception(f'Results have {len(bad_stns)} NaN or negative DKL values: {bad_stns}')\n",
    "        method_results = pd.concat(results_list, ignore_index=True)\n",
    "        results_dfs[method] = method_results\n",
    "        print(f'   Loaded {len(set(completed_stns))} station results for {method} results')\n",
    "        method_results.to_csv(method_results_fpath, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba05ce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the metrics to align score interpretation (zero better)\n",
    "for k, r in results_dfs.items():\n",
    "    # take exponential to express as geometric mean / average multiplicative deviation\n",
    "    results_dfs[k]['RMSE'] = 100 * (np.exp(results_dfs[k]['RMSE']) - 1) \n",
    "    # results_dfs[k]['PB'] = results_dfs[k]['PB'] # express as percentage\n",
    "    results_dfs[k]['NAE'] = 100 * (1 - results_dfs[k]['VE']) # express as %, 0 is perfect\n",
    "    results_dfs[k]['NSE'] = 1 - results_dfs[k]['NSE'] # express as 0 is perfect\n",
    "    results_dfs[k]['KGE'] = 1 - results_dfs[k]['KGE']   # express as 0 is perfect   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cde0467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_knn_label_col(df):\n",
    "    \"\"\"kNN results have a label column that needs to be split into multiple columns.\"\"\"\n",
    "    # Split the string column\n",
    "    # Determine format based on length\n",
    "    if 'MDB' in df.columns:\n",
    "        df.drop(labels=['MDB'], axis=1, inplace=True)\n",
    "    # df.rename({'TBV': 'PVB'}, inplace=True)\n",
    "    split_labels = df['Label'].str.split('_')\n",
    "    df['n_parts'] = split_labels.str.len()\n",
    "\n",
    "    assert len(set(df['n_parts'])) == 1, \"Not all labels have the same number of parts\"\n",
    "\n",
    "    # Define expected column structures\n",
    "    # format_a_cols = [\"Official_ID\", \"k\", \"NN\", 'concurrent', 'tree_type', 'dist', 'weighting', 'ensemble_method']\n",
    "    format_cols = [\"Official_ID\", \"k\", \"NN\", 'tree_type', 'dist', 'ensemble_weight', 'ensemble_method']\n",
    "\n",
    "    # Subset by format\n",
    "    df_a = df[df['n_parts'] == len(format_cols)].copy()\n",
    "\n",
    "    # Split and join with suffix to avoid conflicts\n",
    "    df_a_split = df_a['Label'].str.split('_', expand=True)\n",
    "    df_a_split.columns = format_cols\n",
    "    merged = pd.concat([df_a.reset_index(drop=True), df_a_split.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # Drop duplicates (if any) and update\n",
    "    merged.drop(columns=['NN', 'dist', 'n_parts', 'minYears', 'minOverlapPct'], errors='ignore', inplace=True)\n",
    "    merged = merged.loc[:, ~merged.columns.duplicated()]\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38217037",
   "metadata": {},
   "outputs": [],
   "source": [
    "parametric_targets = list(set(results_dfs['parametric']['Label'].values))\n",
    "results_dfs['knn'] = split_knn_label_col(results_dfs['knn'])\n",
    "knn_formatted_results = results_dfs['knn'].copy()\n",
    "knn_formatted_fpath = os.path.join('data', 'results', 'additional_results', f'knn_all_results_formatted_{bitrate:02d}_bits.csv')\n",
    "if regularization_type == 'kde':\n",
    "    knn_formatted_fpath = os.path.join('data', 'results', 'additional_results', f'knn_all_results_formatted_kde.csv')\n",
    "\n",
    "knn_formatted_results.to_csv(knn_formatted_fpath, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d83d47f",
   "metadata": {},
   "source": [
    "### Load the total sample mean PMF\n",
    "\n",
    "Here we want to pre-compute benchmark performance measures based on the \"global\" mean PMF and the uniform distribution.  These represent null models to provide context for comparing the value added by using different models to predict FDCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a20e8d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the mean global PMF\n",
    "mean_pmf_df_bits = pd.read_csv(f'data/results/sample_distribution_mixture/mean_distribution_{bitrate}bits.csv')\n",
    "mean_pmf_df_bits.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "# upsample to the 12 bits over the same range\n",
    "num_bins = np.log2(len(mean_pmf_df_bits))\n",
    "mean_pmf_df_bits['pmf'] /= mean_pmf_df_bits['pmf'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16507a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdc_df = pd.concat([results_dfs['parametric'].dropna(axis=1, how='all'), results_dfs['lstm'].dropna(axis=1, how='all')], axis=0)\n",
    "\n",
    "# load the reference PMFs\n",
    "pmf_obs_df = pd.read_csv(baseline_pmf_path)\n",
    "# log_edges = np.concatenate([mean_pmf_df['left_log_edges'].values[:1], mean_pmf_df['right_log_edges'].values])\n",
    "log_x_uar = mean_pmf_df_bits['log_x_uar'].values\n",
    "\n",
    "formatted_fdc_results_fpath = f'data/results/additional_results/formatted_results_by_performance_measure_{bitrate:02d}_bits.csv'\n",
    "if regularization_type == 'kde':\n",
    "    formatted_fdc_results_fpath = f'data/results/additional_results/formatted_results_by_performance_measure_kde.csv'\n",
    "    \n",
    "if not os.path.exists(formatted_fdc_results_fpath):\n",
    "\n",
    "    for stn in fdc_df['Official_ID'].unique():\n",
    "        # adjust log_x to adapt to station-specific zero flow threshold UAR equivalent\n",
    "        # set all values < zero_flow_uar to the adjusted minimum\n",
    "        log_zf = 1000.0 * fdc_context.zero_flow_threshold / da_dict[stn]  # convert to L/s/km2\n",
    "        zero_bin_index = int(np.searchsorted(log_x_uar, float(log_zf), side=\"right\")) - 1\n",
    "        min_measurable_log_uar = log_x_uar[zero_bin_index]\n",
    "        min_measurable_uar = np.exp(min_measurable_log_uar)\n",
    "        \n",
    "        eval_obj = EvaluationMetrics(bitrate=bitrate, log_x=log_x_uar, min_measurable_log_uar=min_measurable_log_uar)\n",
    "        \n",
    "        if stn not in official_ids_to_include:\n",
    "            continue\n",
    "\n",
    "        mean_pmf = mean_pmf_df_bits['pmf'].values\n",
    "        # get the sum of probabilities above the zero flow threshold\n",
    "        low_flow_prob = mean_pmf[:zero_bin_index].sum()\n",
    "        adjusted_mean_pmf = np.zeros_like(mean_pmf)\n",
    "        adjusted_mean_pmf[0] = low_flow_prob\n",
    "        adjusted_mean_pmf[zero_bin_index:] = mean_pmf[zero_bin_index:].copy()\n",
    "        # adjusted_mean_pmf /= adjusted_mean_pmf.sum()\n",
    "\n",
    "        assert np.isclose(mean_pmf.sum(), 1.0), 'Mean PMF does not sum to 1'\n",
    "        stn_data = StationData(fdc_context, stn)\n",
    "        prior_adjusted_pmf = stn_data._compute_adjusted_distribution_with_mixed_uniform(adjusted_mean_pmf)\n",
    "\n",
    "        u = np.ones_like(adjusted_mean_pmf) / len(mean_pmf)\n",
    "\n",
    "        assert np.isclose(u.sum(), 1.0), f'Prior adjusted PMF does not sum to 1 for station {stn}: {u.sum():.6f}'\n",
    "        assert np.isclose(prior_adjusted_pmf.sum(), 1.0), f'Adjusted mean PMF does not sum to 1 for station {stn}: {adjusted_mean_pmf.sum():.6f}'\n",
    "        \n",
    "        for q_est, label in zip([prior_adjusted_pmf, u], ['Mean_PMF', 'Uniform']):\n",
    "            new_eval = eval_obj._evaluate_fdc_metrics_from_pmf(q_est, pmf_obs_df[stn].values)\n",
    "            assert new_eval['kld'] > 0, f'Negative KLD for station {stn} with method {label}: {new_eval[\"kld\"]:.3f}'\n",
    "\n",
    "            # Prepare a new row with the results for this station\n",
    "            result_keys = ['kld', 'emd', 'rmse', 'pct_vol_bias', 'mean_abs_pct_error', 'nse', 'kge', 've', 'norm_abs_error']\n",
    "            df_labels = ['KLD', 'EMD', 'RMSE', 'PB', 'MAPE', 'NSE', 'KGE', 'VE', 'NAE']\n",
    "            new_row = {dl: new_eval[rk] for rk, dl in zip(result_keys, df_labels)}\n",
    "            new_row['Official_ID'] = stn\n",
    "            new_row['Label'] = label\n",
    "\n",
    "            fdc_mapper = {'RB': 'PB', 'MARE': 'MAPE',}\n",
    "\n",
    "            # Add missing columns as NaN if needed\n",
    "            for col in fdc_df.columns:\n",
    "                if col not in new_row:\n",
    "                    if col in fdc_mapper:\n",
    "                        mapped_col = fdc_mapper[col]\n",
    "                        new_row[col] = new_row[mapped_col]\n",
    "                        continue\n",
    "                    new_row[col] = np.nan\n",
    "                \n",
    "            # Append the new row to the dataframe\n",
    "            new_row['RMSE'] = 100 * (np.exp(new_row['RMSE']) - 1)\n",
    "            new_row['NSE'] = 1 - new_row['NSE']\n",
    "            new_row['NAE'] = 100 * (1 - new_row['VE'])\n",
    "            fdc_df = pd.concat([fdc_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    fdc_df.sort_values(by=['Official_ID'], inplace=True)\n",
    "    fdc_df.reset_index(drop=True, inplace=True)\n",
    "    fdc_df.to_csv(formatted_fdc_results_fpath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7adb10d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd59fef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
