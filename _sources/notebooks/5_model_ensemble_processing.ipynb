{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a31275eb",
   "metadata": {},
   "source": [
    "# Multi-model ensembles\n",
    "\n",
    "The FDCs estimated by individual models are combined into multi-model ensembles in this notebook.  The goal is to evaluate whether multi-model ensembles can exploit low rank correlation to improve performance overall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "558cd32f-b562-4502-ab63-826aede8f321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import multiprocessing as mp\n",
    "import json\n",
    "\n",
    "from utils.kde_estimator import KDEEstimator\n",
    "from utils.fdc_estimator_context import FDCEstimationContext\n",
    "from utils.fdc_data import StationData\n",
    "from utils.evaluation_metrics import EvaluationMetrics\n",
    "\n",
    "import utils.data_processing_functions as dpf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e87c598e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Watershed_ID</th>\n",
       "      <th>Source</th>\n",
       "      <th>Name</th>\n",
       "      <th>Official_ID</th>\n",
       "      <th>Centroid_Lat_deg_N</th>\n",
       "      <th>Centroid_Lon_deg_E</th>\n",
       "      <th>Drainage_Area_km2</th>\n",
       "      <th>Drainage_Area_GSIM_km2</th>\n",
       "      <th>Flag_GSIM_boundaries</th>\n",
       "      <th>Flag_Artificial_Boundaries</th>\n",
       "      <th>...</th>\n",
       "      <th>Land_Use_Wetland_frac</th>\n",
       "      <th>Land_Use_Water_frac</th>\n",
       "      <th>Land_Use_Urban_frac</th>\n",
       "      <th>Land_Use_Shrubs_frac</th>\n",
       "      <th>Land_Use_Crops_frac</th>\n",
       "      <th>Land_Use_Snow_Ice_frac</th>\n",
       "      <th>Flag_Land_Use_Extraction</th>\n",
       "      <th>Permeability_logk_m2</th>\n",
       "      <th>Porosity_frac</th>\n",
       "      <th>Flag_Subsoil_Extraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>847</td>\n",
       "      <td>HYDAT</td>\n",
       "      <td>CROWSNEST RIVER AT FRANK</td>\n",
       "      <td>05AA008</td>\n",
       "      <td>49.59732</td>\n",
       "      <td>-114.4106</td>\n",
       "      <td>402.6522</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>0.0785</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>1</td>\n",
       "      <td>-15.543306</td>\n",
       "      <td>0.170479</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>850</td>\n",
       "      <td>HYDAT</td>\n",
       "      <td>CASTLE RIVER NEAR BEAVER MINES</td>\n",
       "      <td>05AA022</td>\n",
       "      <td>49.48866</td>\n",
       "      <td>-114.1444</td>\n",
       "      <td>820.6510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.1156</td>\n",
       "      <td>0.0246</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>-15.929747</td>\n",
       "      <td>0.150196</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Watershed_ID Source                            Name Official_ID  \\\n",
       "846           847  HYDAT        CROWSNEST RIVER AT FRANK     05AA008   \n",
       "849           850  HYDAT  CASTLE RIVER NEAR BEAVER MINES     05AA022   \n",
       "\n",
       "     Centroid_Lat_deg_N  Centroid_Lon_deg_E  Drainage_Area_km2  \\\n",
       "846            49.59732           -114.4106           402.6522   \n",
       "849            49.48866           -114.1444           820.6510   \n",
       "\n",
       "     Drainage_Area_GSIM_km2  Flag_GSIM_boundaries  Flag_Artificial_Boundaries  \\\n",
       "846                     NaN                     0                           0   \n",
       "849                     NaN                     0                           0   \n",
       "\n",
       "     ...  Land_Use_Wetland_frac  Land_Use_Water_frac  Land_Use_Urban_frac  \\\n",
       "846  ...                 0.0103               0.0065               0.0328   \n",
       "849  ...                 0.0058               0.0023               0.0105   \n",
       "\n",
       "     Land_Use_Shrubs_frac  Land_Use_Crops_frac  Land_Use_Snow_Ice_frac  \\\n",
       "846                0.0785               0.0015                  0.0002   \n",
       "849                0.1156               0.0246                  0.0000   \n",
       "\n",
       "     Flag_Land_Use_Extraction  Permeability_logk_m2  Porosity_frac  \\\n",
       "846                         1            -15.543306       0.170479   \n",
       "849                         1            -15.929747       0.150196   \n",
       "\n",
       "     Flag_Subsoil_Extraction  \n",
       "846                        1  \n",
       "849                        1  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attr_fpath = 'data/catchment_attributes_with_runoff_stats.csv'\n",
    "attr_df = pd.read_csv(attr_fpath, dtype={'Official_ID': str})\n",
    "station_ids = sorted(attr_df['official_id'].unique().tolist())\n",
    "\n",
    "# streamflow folder from (updated) HYSETS\n",
    "HYSETS_DIR = Path('/home/danbot/code/common_data/HYSETS')\n",
    "hs_df = pd.read_csv('data/HYSETS_watershed_properties.txt', sep=';')\n",
    "hs_df = hs_df[hs_df['Official_ID'].isin(station_ids)]\n",
    "hs_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "870b0bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "watershed_id_dict = {row['Watershed_ID']: row['Official_ID'] for _, row in hs_df.iterrows()}\n",
    "# and the inverse\n",
    "official_id_dict = {row['Official_ID']: row['Watershed_ID'] for _, row in hs_df.iterrows()}\n",
    "# also for drainage areas\n",
    "da_dict = {row['Official_ID']: row['Drainage_Area_km2'] for _, row in hs_df.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1356ccc6-3d6d-4e08-bdbc-53a798187d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 715 monitored basins concurrent with LSTM ensemble results.\n"
     ]
    }
   ],
   "source": [
    "# retrieve LSTM ensemble predictions\n",
    "LSTM_ensemble_result_folder = '/home/danbot/code/neuralhydrology/data/ensemble_results_20250514'# uses NSE mean as loss function\n",
    "# LSTM_ensemble_result_folder = '/home/danbot/code/neuralhydrology/data/ensemble_results_20250627'# uses NSE 95% as loss function\n",
    "lstm_result_files = os.listdir(LSTM_ensemble_result_folder)\n",
    "lstm_result_stns = [e.split('_')[0] for e in lstm_result_files]\n",
    "\n",
    "# filter for the common stations between BCUB region and LSTM-compatible (i.e. 1980-)\n",
    "daymet_concurrent_stations = list(set(station_ids) & set(lstm_result_stns))\n",
    "# assert '012414900' in daymet_concurrent_stations\n",
    "print(f'There are {len(daymet_concurrent_stations)} monitored basins concurrent with LSTM ensemble results.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedee171",
   "metadata": {},
   "source": [
    "Load the global mean PMF and resample to the higher resolution evaluation grid (12 bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "345c4a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the baseline PMFs from the previous notebook\n",
    "pmf_path = Path(os.getcwd()) / 'data' / 'results' / 'baseline_distributions' / f'pmf_obs.csv'\n",
    "pmf_df = pd.read_csv(pmf_path, index_col=0)\n",
    "pmf_stations = pmf_df[[c for c in daymet_concurrent_stations if c in pmf_df.columns]].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fff5f866-6b19-4228-a9b7-5fe577060b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see Notebook 1 for details on these exclusions\n",
    "exclude_stations = ['08FA009', '08GA037', '08NC003', '12052500', '12090480', '12107950', '12108450', '12119300', \n",
    "                    '12119450', '12200684', '12200762', '12203000', '12409500', '15056070', '15081510',\n",
    "                    '12323760', '12143700', '12143900', '12398000', '12058800', '12137800', '12100000']\n",
    "\n",
    "official_ids_to_include = [s for s in pmf_stations if s not in exclude_stations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0bd3b3e-a654-4cb3-8a0d-85c322d2378f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['uar_mean_predicted', 'uar_mean_actual', 'uar_std_predicted', 'uar_std_actual', 'uar_median_predicted', 'uar_median_actual', 'uar_mad_predicted', 'uar_mad_actual', 'log_uar_mean_predicted', 'log_uar_mean_actual', 'log_uar_std_predicted', 'log_uar_std_actual', 'log_uar_median_predicted', 'log_uar_median_actual', 'log_uar_mad_predicted', 'log_uar_mad_actual'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the predicted parameter results\n",
    "parameter_prediction_results_folder = os.path.join('data', 'results', 'parameter_prediction_results', )\n",
    "predicted_params_fpath   = os.path.join(parameter_prediction_results_folder, 'OOS_parameter_predictions.csv')\n",
    "rdf = pd.read_csv(predicted_params_fpath, index_col=['official_id'], dtype={'official_id': str})\n",
    "predicted_param_dict = rdf.to_dict(orient='index')\n",
    "predicted_param_dict['0212414900'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d716f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Using all stations in the catchment data with a baseline PMF (validated): 715\n",
      "    ...overlap dict loaded from data/record_overlap_dict.json\n"
     ]
    }
   ],
   "source": [
    "LSTM_forcings_folder = '/home/danbot/neuralhydrology/data/BCUB_catchment_mean_met_forcings_20250320'\n",
    "# LSTM_ensemble_result_folder = '/home/danbot/code/neuralhydrology/data/ensemble_results'\n",
    "attr_df_fpath = os.path.join('data', f'catchment_attributes_with_runoff_stats.csv')\n",
    "baseline_distribution_folder = 'data/results/baseline_distributions'\n",
    "\n",
    "methods = ('parametric', 'lstm', 'knn',)\n",
    "# methods = ('knn',)\n",
    "include_pre_1980_data = True  # use only stations with data 1980-present concurrent with Daymet\n",
    "daymet_start_date = '1980-01-01'  # default start date for Daymet data\n",
    "if include_pre_1980_data:\n",
    "    daymet_start_date = '1950-01-01'\n",
    "\n",
    "# load the predicted parameter results (Notebook 3)\n",
    "target_cols = [\n",
    "    'uar_mean_predicted', 'uar_std_predicted', 'uar_median_predicted', 'uar_mad_predicted',\n",
    "    'log_uar_mean_predicted', 'log_uar_std_predicted', 'log_uar_median_predicted', 'log_uar_mad_predicted',\n",
    "]\n",
    "\n",
    "input_data = {\n",
    "    'attr_df_fpath': attr_df_fpath,\n",
    "    'LSTM_forcings_folder': LSTM_forcings_folder,\n",
    "    'LSTM_ensemble_result_folder': LSTM_ensemble_result_folder,\n",
    "    'include_pre_1980_data': include_pre_1980_data,  # use only stations with data 1980-present concurrent with Daymet\n",
    "    'predicted_param_dict': predicted_param_dict,\n",
    "    'divergence_measures': ['DKL', 'EMD'],\n",
    "    'eps': 1e-12,\n",
    "    'min_flow': 1e-4,\n",
    "    'n_grid_points': 2**12,\n",
    "    'min_record_length': 5,\n",
    "    'minimum_days_per_month': 20,\n",
    "    'parametric_target_cols': target_cols,\n",
    "    'all_station_ids': official_ids_to_include,\n",
    "    'daymet_concurrent_stations': daymet_concurrent_stations,\n",
    "    'baseline_distribution_folder': baseline_distribution_folder,\n",
    "    'delta': 0.01\n",
    "}\n",
    "\n",
    "fdc_context = FDCEstimationContext(**input_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b5553c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_multi_model_ensemble_pmf(stn, rev_date, which_models, result_folder=None):\n",
    "    # load the knn_result\n",
    "    knn_fpath = result_folder / 'knn' / f'{stn}_fdc_results.json'\n",
    "    knn_pmfs = {}\n",
    "    with open(knn_fpath, 'rb') as file:\n",
    "        knn_dict = json.load(file)\n",
    "        # retrieve the PMF for the 4_NN_0_minOverlapPct_attribute_dist_ID2\n",
    "        knn_models = list(knn_dict.keys())\n",
    "        knn_models = [k for k in knn_models if '_NN_attribute_dist_ID2_freqEnsemble' in k]\n",
    "        for m in sorted(knn_models):\n",
    "            knn_pmfs[m] = knn_dict[m]['pmf']\n",
    "            bias = knn_dict[m]['bias']\n",
    "        assert knn_models, f'No knn model found for {stn}'\n",
    "        # knn_pmf = knn_dict[knn_model[0]]['pmf']\n",
    "\n",
    "    lstm_fpath = result_folder / f'lstm_{rev_date}' / f'{stn}_fdc_results.json'\n",
    "    with open(lstm_fpath, 'rb') as file:\n",
    "        lstm_dict = json.load(file)\n",
    "        lstm_pmf = lstm_dict['frequency']['pmf']\n",
    "\n",
    "    param_fpath = result_folder / 'parametric' / f'{stn}_fdc_results.json'\n",
    "    with open(param_fpath, 'rb') as file:\n",
    "        param_dict = json.load(file)\n",
    "        # retrieve the PMF for the 'PredictedMOM' model\n",
    "        param_models = list(param_dict.keys())\n",
    "        param_model = [k for k in param_models if 'PredictedLog' in k]\n",
    "        assert param_model, f'No parametric model found for {stn}'\n",
    "        param_pmf = param_dict[param_model[0]]['pmf']\n",
    "\n",
    "    # compute an ensemble PMF as the average of the knn and lstm PMFs\n",
    "    # compute the mean ensemble along the support evaluation grid\n",
    "    ensemble_pmfs = {}\n",
    "    # assert knn_pmfs[m].sum() and lstm_pmf.sum() == 1 so it's an equally weighted average\n",
    "    assert np.isclose(np.sum(lstm_pmf), 1.0), f'LSTM PMF does not sum to 1 for {stn}'\n",
    "    assert np.isclose(np.sum(param_pmf), 1.0), f'Parametric PMF does not sum to 1 for {stn}'\n",
    "\n",
    "    for m in knn_pmfs:\n",
    "        assert np.isclose(np.sum(knn_pmfs[m]), 1.0), f'KNN PMF does not sum to 1 for {stn} model {m}'\n",
    "\n",
    "    if which_models == 'knn-lstm':\n",
    "        for m in knn_pmfs:\n",
    "            ensemble_pmf = np.mean([knn_pmfs[m], lstm_pmf], axis=0)\n",
    "            ensemble_pmf /= np.sum(ensemble_pmf)\n",
    "            ensemble_pmfs[m] = ensemble_pmf\n",
    "    elif which_models == 'knn-lstm-parametric':\n",
    "        for m in knn_pmfs:\n",
    "            ensemble_pmf = np.mean([knn_pmfs[m], lstm_pmf, param_pmf], axis=0)\n",
    "            ensemble_pmf /= np.sum(ensemble_pmf)\n",
    "            ensemble_pmfs[m] = ensemble_pmf\n",
    "    elif which_models == 'knn-parametric':\n",
    "        for m in knn_pmfs:\n",
    "            ensemble_pmf = np.mean([knn_pmfs[m], param_pmf], axis=0)\n",
    "            ensemble_pmf /= np.sum(ensemble_pmf)\n",
    "            ensemble_pmfs[m] = ensemble_pmf\n",
    "    else:\n",
    "        raise ValueError(f'which_models {which_models} not recognized, must be one of knn-lstm, knn-lstm-parametric, knn-parametric')\n",
    "    return ensemble_pmfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f14530d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ensemble_divergence(stn, rev_date, pmf_obs_df, which_models, result_folder=None):\n",
    "    station = StationData(fdc_context, stn)\n",
    "    eval_object = EvaluationMetrics(log_x=station.log_x, log_w=station.log_w)\n",
    "    ensemble_pmfs = compute_multi_model_ensemble_pmf(stn, rev_date, which_models=which_models, result_folder=result_folder)\n",
    "    results = {}\n",
    "    for m in ensemble_pmfs:\n",
    "        results[m] = eval_object._evaluate_fdc_metrics_from_pmf(ensemble_pmfs[m], pmf_obs_df[stn].values)\n",
    "    return (results, ensemble_pmfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9561e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembles = {}\n",
    "ensemble_folder = 'data/results/ensemble_results/'\n",
    "results_folder = Path('data/results/fdc_estimation_results/')\n",
    "assert os.path.exists(results_folder), f'Results folder {results_folder} does not exist'\n",
    "process_ensembles = True\n",
    "for ep in ['knn_lstm/', 'knn_lstm_lognorm/', 'knn_lognorm/']:\n",
    "    folder = Path(ensemble_folder) / ep\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    \n",
    "    model_ensemble = 'knn-lstm'\n",
    "    if ep == 'knn_lstm_lognorm/':\n",
    "        model_ensemble = 'knn-lstm-parametric'\n",
    "    elif ep == 'knn_lognorm/':\n",
    "        model_ensemble = 'knn-parametric'\n",
    "\n",
    "    rev_date = LSTM_ensemble_result_folder.split('_')[-1]\n",
    "    n = 0\n",
    "    if process_ensembles:\n",
    "        max_nae = 0\n",
    "        for stn in daymet_concurrent_stations:\n",
    "            n += 1\n",
    "            ensemble_output_fpath = folder / f'{stn}-{model_ensemble}.csv'\n",
    "            if os.path.exists(ensemble_output_fpath):\n",
    "                # print(f'     {ensemble_output_fpath} already exists, skipping')\n",
    "                continue\n",
    "            results, ensemble_pmfs = compute_ensemble_divergence(stn, rev_date, pmf_df, which_models=model_ensemble, result_folder=results_folder)\n",
    "            results_df = pd.DataFrame(results)\n",
    "            results_df.columns = [e.split('_')[1] for e in results_df.columns]\n",
    "            results_df.index.name = 'metric'\n",
    "            results_df.to_csv(ensemble_output_fpath, index=True)\n",
    "            \n",
    "            if n % 50 == 0:\n",
    "                print(f'{n}/{len(daymet_concurrent_stations)} processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d137b44-8f57-4bd3-85fb-202d89216332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ...saved 715 results to knn-lstm_1NN.csv\n",
      "    ...saved 715 results to knn-lstm_2NN.csv\n",
      "    ...saved 715 results to knn-lstm_3NN.csv\n",
      "    ...saved 715 results to knn-lstm_4NN.csv\n",
      "    ...saved 715 results to knn-lstm_5NN.csv\n",
      "    ...saved 715 results to knn-lstm_6NN.csv\n",
      "    ...saved 715 results to knn-lstm_7NN.csv\n",
      "    ...saved 715 results to knn-lstm_8NN.csv\n",
      "    ...saved 715 results to knn-lstm_9NN.csv\n",
      "    ...saved 715 results to knn-lstm_10NN.csv\n",
      "    ...saved 715 results to knn-lstm-lognorm_1NN.csv\n",
      "    ...saved 715 results to knn-lstm-lognorm_2NN.csv\n",
      "    ...saved 715 results to knn-lstm-lognorm_3NN.csv\n",
      "    ...saved 715 results to knn-lstm-lognorm_4NN.csv\n",
      "    ...saved 715 results to knn-lstm-lognorm_5NN.csv\n",
      "    ...saved 715 results to knn-lstm-lognorm_6NN.csv\n",
      "    ...saved 715 results to knn-lstm-lognorm_7NN.csv\n",
      "    ...saved 715 results to knn-lstm-lognorm_8NN.csv\n",
      "    ...saved 715 results to knn-lstm-lognorm_9NN.csv\n",
      "    ...saved 715 results to knn-lstm-lognorm_10NN.csv\n",
      "    ...saved 715 results to knn-lognorm_1NN.csv\n",
      "    ...saved 715 results to knn-lognorm_2NN.csv\n",
      "    ...saved 715 results to knn-lognorm_3NN.csv\n",
      "    ...saved 715 results to knn-lognorm_4NN.csv\n",
      "    ...saved 715 results to knn-lognorm_5NN.csv\n",
      "    ...saved 715 results to knn-lognorm_6NN.csv\n",
      "    ...saved 715 results to knn-lognorm_7NN.csv\n",
      "    ...saved 715 results to knn-lognorm_8NN.csv\n",
      "    ...saved 715 results to knn-lognorm_9NN.csv\n",
      "    ...saved 715 results to knn-lognorm_10NN.csv\n"
     ]
    }
   ],
   "source": [
    "for ep in ['knn_lstm/', 'knn_lstm_lognorm/', 'knn_lognorm/']:\n",
    "    folder = Path(ensemble_folder) / ep\n",
    "    if not process_ensembles:\n",
    "        break\n",
    "    which_ensemble = '-'.join(ep.split('/')[0].split('_'))\n",
    "    nn_ensemble_results = {which_ensemble: {}}\n",
    "\n",
    "    for n in range(1, 11):\n",
    "        nn_results = []\n",
    "        for f in os.listdir(folder):\n",
    "            stn = f.split('.')[0]\n",
    "            df = pd.read_csv(os.path.join(folder, f))\n",
    "            if 'Unnamed: 0' in df.columns:\n",
    "                df.rename({'Unnamed: 0': 'metric'}, axis=1, inplace=True)\n",
    "            df.set_index('metric', inplace=True)\n",
    "            res = df[[str(n)]].to_dict()\n",
    "            res[str(n)]['stn_id'] = stn\n",
    "            nn_results.append(res[str(n)])\n",
    "\n",
    "        nn_df = pd.DataFrame(nn_results)\n",
    "        nn_df.set_index('stn_id', inplace=True)\n",
    "        fname = f'{which_ensemble}_{n}NN.csv'\n",
    "        nn_df.to_csv(os.path.join(ensemble_folder, fname), index=True)\n",
    "        print(f'    ...saved {len(nn_df)} results to {fname}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1267123b",
   "metadata": {},
   "source": [
    "### Concatenate all results into a single data structure for easier plotting and comparison\n",
    "\n",
    "Compute baseline values to represent the \"null\" models of using the global mean PMF, and the uniform distributions for all locations.  These are benchmarks to help understand how much value is added by using different models to predict FDCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b83a110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 715 completed stations in lstm_20250514 results folder.\n",
      "   Loading parametric results\n",
      "   Loading lstm results\n",
      "   Loading knn results\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "results_dfs = {}\n",
    "lstm_rev_date = LSTM_ensemble_result_folder.split('_')[-1]\n",
    "sub_folder = f'lstm_{lstm_rev_date}'\n",
    "# results_folder = '/media/danbot/Samsung_T5/fdc_estimation_results/'\n",
    "results_folder = 'data/results/fdc_estimation_results/'\n",
    "completed_stns = [c.split('_')[0] for c in os.listdir(os.path.join(results_folder, 'knn'))]\n",
    "print(f'Found {len(set(completed_stns))} completed stations in {sub_folder} results folder.')\n",
    "\n",
    "for method in ['parametric', 'lstm', 'knn']:\n",
    "    print(f'   Loading {method} results')\n",
    "    method_results_fpath = os.path.join('data', 'results', f'{method}_all_results.csv')\n",
    "    if method == 'lstm':\n",
    "        rev_date = LSTM_ensemble_result_folder.split('_')[-1]\n",
    "        method_results_fpath = os.path.join('data', 'results', f'{method}_all_results_{rev_date}.csv')\n",
    "    if os.path.exists(method_results_fpath):\n",
    "        results_dfs[method] = pd.read_csv(method_results_fpath, dtype={'Official_ID': str})\n",
    "    else:\n",
    "        print(f'   {method} results not found in {method_results_fpath}, loading from individual station files...')\n",
    "        res_folder = os.path.join(results_folder, method)\n",
    "        if method == 'lstm':\n",
    "            res_folder = os.path.join(results_folder, f'{method}_{rev_date}')\n",
    "        args = [(stn, res_folder, method) for stn in completed_stns]\n",
    "\n",
    "        with Pool() as pool:\n",
    "            results_list = pool.map(dpf.load_results, args)\n",
    "\n",
    "        merged = pd.concat(results_list, ignore_index=True)\n",
    "        bad_dkl = merged[merged['KLD'].isna() | (merged['KLD'] < 0)].copy()\n",
    "        if not bad_dkl.empty:\n",
    "            print(f'Warning: {len(bad_dkl)} {method} rows with NaN or negative DKL values.')\n",
    "            bad_stns = bad_dkl['Official_ID'].values\n",
    "            raise Exception(f'Results have {len(bad_stns)} NaN or negative DKL values: {bad_stns}')\n",
    "        method_results = pd.concat(results_list, ignore_index=True)\n",
    "        results_dfs[method] = method_results\n",
    "        print(f'   Loaded {len(set(completed_stns))} station results for {method} results')\n",
    "        method_results.to_csv(method_results_fpath, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba05ce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the metrics to align score interpretation (zero better)\n",
    "for k, r in results_dfs.items():\n",
    "    # take exponential to express as geometric mean / average multiplicative deviation\n",
    "    results_dfs[k]['RMSE'] = 100 * (np.exp(results_dfs[k]['RMSE']) - 1) \n",
    "    results_dfs[k]['RB'] = 100 * results_dfs[k]['RB'] # express as percentage\n",
    "    results_dfs[k]['NAE'] = 100 * (1 - results_dfs[k]['VE']) # express as %, 0 is perfect\n",
    "    results_dfs[k]['NSE'] = 1 - results_dfs[k]['NSE'] # express as 0 is perfect\n",
    "    results_dfs[k]['KGE'] = 1 - results_dfs[k]['KGE']   # express as 0 is perfect   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cde0467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_knn_label_col(df):\n",
    "    \"\"\"kNN results have a label column that needs to be split into multiple columns.\"\"\"\n",
    "    # Split the string column\n",
    "    # Determine format based on length\n",
    "    if 'MDB' in df.columns:\n",
    "        df.drop(labels=['MDB'], axis=1, inplace=True)\n",
    "    # df.rename({'TBV': 'PVB'}, inplace=True)\n",
    "    split_labels = df['Label'].str.split('_')\n",
    "    df['n_parts'] = split_labels.str.len()\n",
    "\n",
    "    assert len(set(df['n_parts'])) == 1, \"Not all labels have the same number of parts\"\n",
    "\n",
    "    # Define expected column structures\n",
    "    # format_a_cols = [\"Official_ID\", \"k\", \"NN\", 'concurrent', 'tree_type', 'dist', 'weighting', 'ensemble_method']\n",
    "    format_cols = [\"Official_ID\", \"k\", \"NN\", 'tree_type', 'dist', 'ensemble_weight', 'ensemble_method']\n",
    "\n",
    "    # Subset by format\n",
    "    df_a = df[df['n_parts'] == len(format_cols)].copy()\n",
    "\n",
    "    # Split and join with suffix to avoid conflicts\n",
    "    df_a_split = df_a['Label'].str.split('_', expand=True)\n",
    "    df_a_split.columns = format_cols\n",
    "    merged = pd.concat([df_a.reset_index(drop=True), df_a_split.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # Drop duplicates (if any) and update\n",
    "    merged.drop(columns=['NN', 'dist', 'n_parts', 'minYears', 'minOverlapPct'], errors='ignore', inplace=True)\n",
    "    merged = merged.loc[:, ~merged.columns.duplicated()]\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38217037",
   "metadata": {},
   "outputs": [],
   "source": [
    "parametric_targets = list(set(results_dfs['parametric']['Label'].values))\n",
    "results_dfs['knn'] = split_knn_label_col(results_dfs['knn'])\n",
    "knn_formatted_results = results_dfs['knn'].copy()\n",
    "knn_formatted_fpath = os.path.join('data', 'results', f'knn_all_results_formatted.csv')\n",
    "knn_formatted_results.to_csv(knn_formatted_fpath, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d83d47f",
   "metadata": {},
   "source": [
    "### Load the total sample mean PMF\n",
    "\n",
    "Here we want to pre-compute benchmark performance measures based on the \"global\" mean PMF and the uniform distribution.  These represent null models to provide context for comparing the value added by using different models to predict FDCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a20e8d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the mean global PMF\n",
    "mean_pmf_df_bits = pd.read_csv('data/results/mean_distribution_8bits.csv')\n",
    "mean_pmf_df_bits.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "# upsample to the 12 bits over the same range\n",
    "a, b = mean_pmf_df_bits['left_log_edges'].min(), mean_pmf_df_bits['right_log_edges'].max()\n",
    "log_edges_10bit = np.linspace(a, b, 2**10 + 1)\n",
    "log_x_10bit = 0.5 * (log_edges_10bit[:-1] + log_edges_10bit[1:])\n",
    "# interpolate the mean_pmf_df to the 12-bit edges\n",
    "pmf_12bit_resampled = np.interp(\n",
    "    x=log_x_10bit,\n",
    "    xp=mean_pmf_df_bits['log_x'],\n",
    "    fp=mean_pmf_df_bits['pmf']\n",
    ")\n",
    "mean_pmf_df = pd.DataFrame({\n",
    "    'left_log_edges': log_edges_10bit[:-1],\n",
    "    'right_log_edges': log_edges_10bit[1:],\n",
    "    'log_x': log_x_10bit,\n",
    "    'pmf': pmf_12bit_resampled,\n",
    "})\n",
    "mean_pmf_df['pmf'] /= mean_pmf_df['pmf'].sum() # normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16507a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdc_df = pd.concat([results_dfs['parametric'], results_dfs['lstm']], axis=0)\n",
    "pmf_path = Path(os.getcwd()) / 'data' / 'results' / 'baseline_distributions' / f'pmf_obs.csv'\n",
    "pmf_obs_df = pd.read_csv(pmf_path)\n",
    "log_edges = np.concatenate([mean_pmf_df['left_log_edges'].values[:1], mean_pmf_df['right_log_edges'].values])\n",
    "log_w = np.diff(log_edges)\n",
    "eval_obj = EvaluationMetrics(log_x=mean_pmf_df['log_x'].values, log_w=log_w)\n",
    "\n",
    "formatted_fdc_results_fpath = 'data/results/formatted_results_by_performance_measure.csv'\n",
    "\n",
    "if not os.path.exists(formatted_fdc_results_fpath):\n",
    "\n",
    "    for stn in fdc_df['Official_ID'].unique():\n",
    "        pmf_obs_baseline = pmf_obs_df[stn].values\n",
    "        mean_pmf = mean_pmf_df['pmf'].values\n",
    "        stn_data = StationData(fdc_context, stn)\n",
    "        _, prior_adjusted_pmf = stn_data._compute_adjusted_distribution_with_mixed_uniform(mean_pmf)\n",
    "\n",
    "        u = np.ones_like(mean_pmf) / len(mean_pmf)\n",
    "        for new_dist, label in zip([mean_pmf, u], ['Mean_PMF', 'Uniform']):\n",
    "            new_eval = eval_obj._evaluate_fdc_metrics_from_pmf(new_dist, pmf_obs_baseline)\n",
    "            \n",
    "            # Prepare a new row with the results for this station\n",
    "            # result_keys = ['kld', 'emd', 'rmse', 'mean_error', 'pct_vol_bias', 'mean_abs_rel_error', 'nse', 'kge', 've', 'pb_50', 'vb_pmf', 'vb_fdc', 'mean_frac_diff']\n",
    "            # df_labels = ['KLD', 'EMD', 'RMSE', 'MB', 'RB', 'MARE', 'NSE', 'KGE', 'VE', 'PB_50', 'VB_PMF', 'VB_FDC', 'MEAN_FRAC_DIFF']\n",
    "            result_keys = ['kld', 'emd', 'rmse', 'mean_error', 'pct_vol_bias', 'mean_abs_rel_error', 'nse', 'kge', 've']\n",
    "            df_labels = ['KLD', 'EMD', 'RMSE', 'MB', 'RB', 'MARE', 'NSE', 'KGE', 'VE']\n",
    "            new_row = {dl: new_eval[rk] for rk, dl in zip(result_keys, df_labels)}\n",
    "            new_row['Official_ID'] = stn\n",
    "            new_row['Label'] = label\n",
    "\n",
    "            # Add missing columns as NaN if needed\n",
    "            for col in fdc_df.columns:\n",
    "                if col not in new_row:\n",
    "                    new_row[col] = np.nan\n",
    "            # Append the new row to the dataframe\n",
    "            new_row['RMSE'] = 100 * (np.exp(new_row['RMSE']) - 1)\n",
    "            new_row['RB'] = 100 * new_row['RB']\n",
    "            new_row['NSE'] = 1 - new_row['NSE']\n",
    "            new_row['NAE'] = 100 * (1 - new_row['VE'])\n",
    "            fdc_df = pd.concat([fdc_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    fdc_df.sort_values(by=['Official_ID'], inplace=True)\n",
    "    fdc_df.reset_index(drop=True, inplace=True)\n",
    "    fdc_df.to_csv(formatted_fdc_results_fpath, index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd965e74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47218c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
