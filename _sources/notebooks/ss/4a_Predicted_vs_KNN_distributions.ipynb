{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0bd3e2d-4130-4fd8-8d0e-aebdadd4d447",
   "metadata": {},
   "source": [
    "# Predicted vs. Empricial Distribution Fitting\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Question: What is the best general approach to estimate the PDF at an ungauged location?\n",
    "\n",
    "We test several methods of estimating flow duration curves for ungauged locations:\n",
    "\n",
    "1. k-Nearest neighbours, frequency and time averaging.  Equal weighting, inverse distance weighting, and catchment similarity weighting.\n",
    "\n",
    "We know that mean annual runoff can be predicted with reasonable accuracy from catchment attributes.  We also know that mean runoff and variance are correlated.  Runoff is generally best approximated by a lognormal distribution among parametric distributions, but the lognormal distribution parameters cannot be computed explicitly from the sample mean and variance.  However, the parameters can be estimated by method of moments, with some penalty over maximum likelihood estimation.  While the mean and variance (and even the log-mean) are predictable from catchment attributes, log-variance is not.  The question this experiment asks is whether the poor predictability of the log-variance results in worse estimation of runoff distribution than the method of moments approximation.\n",
    "\n",
    "2. Parametric estimation: predicting lognormal distribution parameters from catchment attributes by a) method of moments on predicted mean and variance, and b) predicting maximum likelihood parameters predicted from catchment attributes.\n",
    " \n",
    "\n",
    "We predicted mean and SD based on catchment attributes alone.  We then computed the KL divergence of the predicted (proxy) parametric distributions from KDE fits of target catchment distributions.  We then tested the predictability of these KL divergences from attributes, both individually and in pairs. Individually we tested catchment attributes for the predictability of divergence between the predicted parametric distributions and the (error model) adjusted KDE fits.  Pairwise we tested the predictability of divergence between a predicted parametric distribution of a potential proxy model and an (error model) adjusted KDE fit.\n",
    "\n",
    "Here we generate distribution estimates from k-nearest neighbours (KNN), where k = 1, ..., 10.  Finally, for each simulated location, we compare all methods of estimating distributions:\n",
    "\n",
    "1. **Parametric**: log-normal distributions estimated from predicting mean runoff from catchment attributes, plus the linear approximation between mean and standard deviation runoff.\n",
    "2. **KNN**: For each location we approximate a distribution using 1, ..., 10 nearest neighbours by:   \n",
    "    * equal weighting (EW)\n",
    "    * inverse-distance weighting (IDW)\n",
    "    * catchment similarity weighting (CSW)\n",
    "3. Find the most similar distribution in the monitoring network:\n",
    "    * by Kullback-Leibler, Wasserstein, TVD, Hellinger distance\n",
    "    * record the distance ranks of the neighbouring distributions\n",
    "    * think about how to address how stable the rankings are under different assumptions, i.e. priors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a42fc6-9ff0-4f8e-b904-d73810419b8a",
   "metadata": {},
   "source": [
    "## Data Import and Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c9864ab-af00-457b-bfe0-2e69ac18b2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"f1bf2b4c-d8d3-4cac-a17d-7b36ac27413f\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "'use strict';\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    function drop(id) {\n",
       "      const view = Bokeh.index.get_by_id(id)\n",
       "      if (view != null) {\n",
       "        view.model.document.clear()\n",
       "        Bokeh.index.delete(view)\n",
       "      }\n",
       "    }\n",
       "\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null) {\n",
       "      drop(id)\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim()\n",
       "            drop(id)\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded(error = null) {\n",
       "    const el = document.getElementById(\"f1bf2b4c-d8d3-4cac-a17d-7b36ac27413f\");\n",
       "    if (el != null) {\n",
       "      const html = (() => {\n",
       "        if (typeof root.Bokeh === \"undefined\") {\n",
       "          if (error == null) {\n",
       "            return \"BokehJS is loading ...\";\n",
       "          } else {\n",
       "            return \"BokehJS failed to load.\";\n",
       "          }\n",
       "        } else {\n",
       "          const prefix = `BokehJS ${root.Bokeh.version}`;\n",
       "          if (error == null) {\n",
       "            return `${prefix} successfully loaded.`;\n",
       "          } else {\n",
       "            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n",
       "          }\n",
       "        }\n",
       "      })();\n",
       "      el.innerHTML = html;\n",
       "\n",
       "      if (error != null) {\n",
       "        const wrapper = document.createElement(\"div\");\n",
       "        wrapper.style.overflow = \"auto\";\n",
       "        wrapper.style.height = \"5em\";\n",
       "        wrapper.style.resize = \"vertical\";\n",
       "        const content = document.createElement(\"div\");\n",
       "        content.style.fontFamily = \"monospace\";\n",
       "        content.style.whiteSpace = \"pre-wrap\";\n",
       "        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n",
       "        content.textContent = error.stack ?? error.toString();\n",
       "        wrapper.append(content);\n",
       "        el.append(wrapper);\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(() => display_loaded(error), 100);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.6.0.min.js\"];\n",
       "  const css_urls = [];\n",
       "\n",
       "  const inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      try {\n",
       "            for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "\n",
       "      } catch (error) {display_loaded(error);throw error;\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(\"f1bf2b4c-d8d3-4cac-a17d-7b36ac27413f\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"f1bf2b4c-d8d3-4cac-a17d-7b36ac27413f\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.6.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.6.0.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"f1bf2b4c-d8d3-4cac-a17d-7b36ac27413f\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import xyzservices.providers as xyz\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.layouts import gridplot, row, column\n",
    "from bokeh.transform import factor_cmap, linear_cmap\n",
    "from bokeh.models import ColumnDataSource, LinearAxis, Range1d\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.palettes import Sunset10, Vibrant7, Category20, Bokeh6, Bokeh7, Bokeh8, Greys256\n",
    "\n",
    "import xgboost as xgb\n",
    "xgb.config_context(verbosity=2)\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    root_mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    roc_auc_score,\n",
    "    roc_curve, auc,\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "from scipy.stats import linregress\n",
    "from scipy.stats import lognorm, norm, rdist\n",
    "from scipy.special import kl_div\n",
    "\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.scipy.stats import gaussian_kde as jkde\n",
    "from jax import config as jax_config\n",
    "jax_config.update(\"jax_enable_x64\", False)\n",
    "\n",
    "from jax import jit\n",
    "from jax import vmap\n",
    "\n",
    "from KDEpy import FFTKDE\n",
    "\n",
    "import data_processing_functions as dpf\n",
    "\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ef3f79c-03c7-4d25-aac0-3365836b1060",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "tiles = xyz['USGS']['USTopo']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36488ad5-32a9-4d0a-b8ae-64967236e839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1325 monitored basins in the attribute set.\n"
     ]
    }
   ],
   "source": [
    "# load the catchment characteristics\n",
    "fname = 'BCUB_watershed_attributes_updated.csv'\n",
    "attr_df = pd.read_csv(os.path.join('data', fname))\n",
    "attr_df['log_drainage_area_km2'] = np.log(attr_df['drainage_area_km2'])\n",
    "attr_df.columns = [c.lower() for c in attr_df.columns]\n",
    "attr_df['tmean'] = (attr_df['tmin'] + attr_df['tmax']) / 2.0\n",
    "station_ids = attr_df['official_id'].values\n",
    "print(f'There are {len(station_ids)} monitored basins in the attribute set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27b09707-ece8-4422-9978-8e86a3c32c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the lognormal fit parameter results\n",
    "ln_fit_fname = 'LN_fit_method_comparison_20250128.csv'\n",
    "ln_fit_fpath = os.path.join('data', 'results', ln_fit_fname)\n",
    "ln_df = pd.read_csv(ln_fit_fpath)\n",
    "\n",
    "ln_df = ln_df[ln_df['official_id'].isin(attr_df['official_id'].values)]\n",
    "ln_df['mean_runoff_mm_day'] = ln_df['mean_uar'] * 3.6 / 1000\n",
    "ln_df['sd_runoff_mm_day'] = ln_df['sd_uar'] * 3.6 / 1000\n",
    "\n",
    "target_columns = [c for c in ln_df.columns if c not in attr_df.columns]\n",
    "for tc in target_columns:\n",
    "    # create a dict of the\n",
    "    target_dict = ln_df[['official_id', tc]].copy().set_index('official_id').to_dict()[tc]\n",
    "    if tc not in attr_df.columns:\n",
    "        attr_df[tc] = attr_df['official_id'].apply(lambda x: target_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dee077b8-18d1-4d0d-979b-c59cb5518c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>official_id</th>\n",
       "      <th>obs_mean_mm_day</th>\n",
       "      <th>obs_std</th>\n",
       "      <th>pred_mean_mm_day</th>\n",
       "      <th>pred_sigma</th>\n",
       "      <th>KL_KDE_AKDE_2.5</th>\n",
       "      <th>KL_KDE_AKDE_50</th>\n",
       "      <th>KL_KDE_AKDE_97.5</th>\n",
       "      <th>KL_AKDE_LNobs_2.5</th>\n",
       "      <th>KL_AKDE_LNobs_50</th>\n",
       "      <th>KL_AKDE_LNobs_97.5</th>\n",
       "      <th>KL_AKDE_LNest_2.5</th>\n",
       "      <th>KL_AKDE_LNest_50</th>\n",
       "      <th>KL_AKDE_LNest_97.5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>05AA023</td>\n",
       "      <td>0.763394</td>\n",
       "      <td>1.286813</td>\n",
       "      <td>0.821331</td>\n",
       "      <td>1.201441</td>\n",
       "      <td>0.004435</td>\n",
       "      <td>0.006690</td>\n",
       "      <td>0.011342</td>\n",
       "      <td>1.026485</td>\n",
       "      <td>1.044057</td>\n",
       "      <td>1.065971</td>\n",
       "      <td>0.700195</td>\n",
       "      <td>0.715953</td>\n",
       "      <td>0.735906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>05AA035</td>\n",
       "      <td>0.772076</td>\n",
       "      <td>1.449857</td>\n",
       "      <td>0.662123</td>\n",
       "      <td>1.017802</td>\n",
       "      <td>0.021181</td>\n",
       "      <td>0.029060</td>\n",
       "      <td>0.043829</td>\n",
       "      <td>0.687753</td>\n",
       "      <td>0.749125</td>\n",
       "      <td>0.806108</td>\n",
       "      <td>0.678864</td>\n",
       "      <td>0.744101</td>\n",
       "      <td>0.802399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>05AD033</td>\n",
       "      <td>4.073865</td>\n",
       "      <td>5.079821</td>\n",
       "      <td>3.869882</td>\n",
       "      <td>4.717802</td>\n",
       "      <td>0.039788</td>\n",
       "      <td>0.053163</td>\n",
       "      <td>0.073777</td>\n",
       "      <td>0.698456</td>\n",
       "      <td>0.783050</td>\n",
       "      <td>0.865009</td>\n",
       "      <td>0.741019</td>\n",
       "      <td>0.827571</td>\n",
       "      <td>0.912939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>05BF017</td>\n",
       "      <td>1.162493</td>\n",
       "      <td>1.957323</td>\n",
       "      <td>1.481970</td>\n",
       "      <td>1.963457</td>\n",
       "      <td>2.081178</td>\n",
       "      <td>2.130511</td>\n",
       "      <td>2.189047</td>\n",
       "      <td>0.864798</td>\n",
       "      <td>0.900978</td>\n",
       "      <td>0.940367</td>\n",
       "      <td>0.476573</td>\n",
       "      <td>0.506467</td>\n",
       "      <td>0.536577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05BJ010</td>\n",
       "      <td>0.821697</td>\n",
       "      <td>1.169613</td>\n",
       "      <td>0.695347</td>\n",
       "      <td>1.056124</td>\n",
       "      <td>0.011753</td>\n",
       "      <td>0.021304</td>\n",
       "      <td>0.038843</td>\n",
       "      <td>1.333488</td>\n",
       "      <td>1.355958</td>\n",
       "      <td>1.382492</td>\n",
       "      <td>1.697818</td>\n",
       "      <td>1.721834</td>\n",
       "      <td>1.752463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  official_id  obs_mean_mm_day   obs_std  pred_mean_mm_day  pred_sigma  \\\n",
       "0     05AA023         0.763394  1.286813          0.821331    1.201441   \n",
       "1     05AA035         0.772076  1.449857          0.662123    1.017802   \n",
       "2     05AD033         4.073865  5.079821          3.869882    4.717802   \n",
       "3     05BF017         1.162493  1.957323          1.481970    1.963457   \n",
       "4     05BJ010         0.821697  1.169613          0.695347    1.056124   \n",
       "\n",
       "   KL_KDE_AKDE_2.5  KL_KDE_AKDE_50  KL_KDE_AKDE_97.5  KL_AKDE_LNobs_2.5  \\\n",
       "0         0.004435        0.006690          0.011342           1.026485   \n",
       "1         0.021181        0.029060          0.043829           0.687753   \n",
       "2         0.039788        0.053163          0.073777           0.698456   \n",
       "3         2.081178        2.130511          2.189047           0.864798   \n",
       "4         0.011753        0.021304          0.038843           1.333488   \n",
       "\n",
       "   KL_AKDE_LNobs_50  KL_AKDE_LNobs_97.5  KL_AKDE_LNest_2.5  KL_AKDE_LNest_50  \\\n",
       "0          1.044057            1.065971           0.700195          0.715953   \n",
       "1          0.749125            0.806108           0.678864          0.744101   \n",
       "2          0.783050            0.865009           0.741019          0.827571   \n",
       "3          0.900978            0.940367           0.476573          0.506467   \n",
       "4          1.355958            1.382492           1.697818          1.721834   \n",
       "\n",
       "   KL_AKDE_LNest_97.5  \n",
       "0            0.735906  \n",
       "1            0.802399  \n",
       "2            0.912939  \n",
       "3            0.536577  \n",
       "4            1.752463  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open an example pairwise results file\n",
    "input_folder = os.path.join(\n",
    "    BASE_DIR, \"data\", \"processed_divergence_inputs\",\n",
    ")\n",
    "pairs_files = os.listdir(input_folder)\n",
    "rev_date = '20250119'\n",
    "n_rows = None\n",
    "# parametric_df = pd.read_csv(os.path.join(input_folder, f'MEMBAKDE_results_{rev_date}.csv'), nrows=n_rows)\n",
    "# parametric_df.head()\n",
    "fname = 'Results_estimated_vs_observed_LN_fits_20250118.csv'\n",
    "bootstrap_result_fpath = os.path.join(os.getcwd(), 'data', 'parametric_fits', fname)\n",
    "param_df = pd.read_csv(bootstrap_result_fpath)\n",
    "param_df.drop('Unnamed: 0', inplace=True, axis=1)\n",
    "param_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c182acb-c487-4d90-9acd-d6a18ef20eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict of 'official_id': 'drainage area'\n",
    "da_dict = attr_df[['official_id', 'drainage_area_km2']].copy().set_index('official_id').to_dict()['drainage_area_km2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70d426ef-0c36-4523-ad01-eed87b26bc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = attr_df.apply(lambda row: Point(row['centroid_lon_deg_e'], row['centroid_lat_deg_n']), axis=1)\n",
    "attr_gdf = gpd.GeoDataFrame(attr_df, geometry=centroids, crs='EPSG:4326')\n",
    "attr_gdf.drop('unnamed: 0', inplace=True, axis=1)\n",
    "attr_gdf.reset_index(inplace=True)\n",
    "# convert to BC Albers for computing distances\n",
    "attr_gdf = attr_gdf.to_crs(3005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdec8b20-0f10-463f-9d05-c4cc4a259426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1323 unique proxies, 1324 unique targets\n"
     ]
    }
   ],
   "source": [
    "# preload the FFT KDE fit results (note these are not \"error adapted\"\n",
    "kde_file = 'KL_fft_kde_fits_20241226.csv'\n",
    "kde_fit_df = pd.read_csv(os.path.join('data', 'parametric_divergence_test', kde_file))\n",
    "unique_proxies = np.unique(kde_fit_df['proxy'].values)\n",
    "unique_targets = np.unique(kde_fit_df['target'].values)\n",
    "print(f'{len(unique_proxies)} unique proxies, {len(unique_targets)} unique targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b7f7469-4202-4ec6-8341-0fb6b487157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the predicted parameter results\n",
    "predict_result_folder = '/home/danbot2/code_5820/24/divergence_measures/docs/notebooks/data/prediction_results/runoff_prediction_results'\n",
    "best_result_files = [e for e in os.listdir(predict_result_folder) if e.startswith('best_')]\n",
    "predicted_params, best_result_dfs = [], []\n",
    "for f in best_result_files:\n",
    "    param = '_'.join(f.split('_')[4:-1])\n",
    "    rdf = pd.read_csv(os.path.join(predict_result_folder, f), index_col='official_id')\n",
    "    rdf = rdf[[c for c in rdf.columns if not c.startswith('Unnamed:')]]\n",
    "    rdf.columns = [f'{e}_{param}' for e in rdf.columns]\n",
    "    best_result_dfs.append(rdf)\n",
    "    predicted_params.append(param)\n",
    "    \n",
    "# predicted_params = ['LN_MMO_mu_hat', 'LN_MMO_sd_hat', 'mean_logx', 'sd_logx']\n",
    "predicted_param_df = pd.concat(best_result_dfs, join='inner', axis=1)\n",
    "predicted_param_dict = predicted_param_df.to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7798a03b-895c-45a3-82c6-346ba341d1b9",
   "metadata": {},
   "source": [
    "## Perform KNN distribution estimation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "183a47d9-3318-4b5f-9c24-bf299488953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "coords = np.array([[geom.x, geom.y] for geom in attr_gdf.geometry])\n",
    "stn_tree = cKDTree(coords)\n",
    "\n",
    "# Create mapping from official_id to index\n",
    "id_to_index = {oid: i for i, oid in enumerate(attr_gdf[\"official_id\"])}\n",
    "index_to_id = {i: oid for oid, i in id_to_index.items()}  # Reverse mapping\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# Extract values (excluding 'official_id' since it's categorical)\n",
    "attribute_columns = ['log_drainage_area_km2', 'elevation_m', 'prcp', 'tmean', 'swe', \n",
    "                     'land_use_forest_frac_2010', 'land_use_snow_ice_frac_2010', 'land_use_water_frac_2010', 'land_use_wetland_frac_2010']\n",
    "attr_gdf['log_drainage_area_km2'] = np.log(attr_gdf['drainage_area_km2'])\n",
    "attr_values = attr_gdf[attribute_columns].to_numpy()\n",
    "normalized_attr_values = scaler.fit_transform(attr_values)\n",
    "# Convert normalized distances back to original units\n",
    "std_devs_attrs = scaler.scale_  # Standard deviation of each feature\n",
    "\n",
    "attr_tree = cKDTree(normalized_attr_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37de1e1a-6275-459c-b8e1-6caa5f70ae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lognorm_pmf(log_x, mu, sigma, integral_tol=2e-3):\n",
    "    # Lognormal parameters\n",
    "    norm_pdf = norm.pdf(log_x, loc=mu, scale=sigma)\n",
    "    norm_check = np.trapz(norm_pdf, x=log_x)\n",
    "    lin_grid = np.exp(log_x)\n",
    "    norm_lin_check = np.trapz(norm_pdf / lin_grid, x=lin_grid) \n",
    "    nc1 = np.isclose(norm_check, 1, atol=integral_tol)#, norm_check\n",
    "    nc2 = np.isclose(norm_lin_check, 1, atol=integral_tol)#, norm_lin_check\n",
    "    # print(f'Norm integral: {norm_check:.4f}')\n",
    "    \n",
    "    norm_cdf = np.cumsum(norm_pdf)\n",
    "    norm_cdf /= norm_cdf[-1]\n",
    "    norm_pmf = np.diff(norm_cdf, prepend=0)\n",
    "    return norm_pmf, norm_pdf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e482253d-37ff-4c68-b475-99000aa1d4ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"ef627a79-7dc8-4082-a0b7-ce7fd849efee\" data-root-id=\"p1054\" style=\"display: contents;\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "  const docs_json = {\"21539a5d-d73b-4448-a203-4bc769536054\":{\"version\":\"3.6.0\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"GridPlot\",\"id\":\"p1054\",\"attributes\":{\"rows\":null,\"cols\":null,\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1053\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"ToolProxy\",\"id\":\"p1047\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1023\"}]}},{\"type\":\"object\",\"name\":\"ToolProxy\",\"id\":\"p1048\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1024\",\"attributes\":{\"renderers\":\"auto\"}}]}},{\"type\":\"object\",\"name\":\"ToolProxy\",\"id\":\"p1049\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1025\",\"attributes\":{\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1026\",\"attributes\":{\"syncable\":false,\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5,\"level\":\"overlay\",\"visible\":false,\"left\":{\"type\":\"number\",\"value\":\"nan\"},\"right\":{\"type\":\"number\",\"value\":\"nan\"},\"top\":{\"type\":\"number\",\"value\":\"nan\"},\"bottom\":{\"type\":\"number\",\"value\":\"nan\"},\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"handles\":{\"type\":\"object\",\"name\":\"BoxInteractionHandles\",\"id\":\"p1032\",\"attributes\":{\"all\":{\"type\":\"object\",\"name\":\"AreaVisuals\",\"id\":\"p1031\",\"attributes\":{\"fill_color\":\"white\",\"hover_fill_color\":\"lightgray\"}}}}}}}}]}},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1050\"},{\"type\":\"object\",\"name\":\"ToolProxy\",\"id\":\"p1051\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1034\"}]}},{\"type\":\"object\",\"name\":\"ToolProxy\",\"id\":\"p1052\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"HelpTool\",\"id\":\"p1035\"}]}}]}},\"children\":[[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1001\",\"attributes\":{\"width\":500,\"height\":350,\"x_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1002\"},\"y_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1003\"},\"x_scale\":{\"type\":\"object\",\"name\":\"LogScale\",\"id\":\"p1011\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1012\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1004\",\"attributes\":{\"text\":\"Estimated Measurement Error Model\"}},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1042\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1036\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1037\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1038\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"exSuR+F6hD+amZmZmZm5PwAAAAAAAPA/AAAAAAAAJEAAAAAAAABZQAAAAAAAQI9AAAAAAACIw0AAAAAAAGr4QAAAAACAhC5BAAAAANASY0E=\"},\"shape\":[10],\"dtype\":\"float64\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"AAAAAAAA8D8AAAAAAADgPwAAAAAAANA/MzMzMzMzwz+amZmZmZm5P5qZmZmZmbk/mpmZmZmZuT8zMzMzMzPDP5qZmZmZmck/AAAAAAAA0D8=\"},\"shape\":[10],\"dtype\":\"float64\",\"order\":\"little\"}]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1043\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1044\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1039\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"red\",\"line_width\":2}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1040\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"red\",\"line_alpha\":0.1,\"line_width\":2}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1041\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"red\",\"line_alpha\":0.2,\"line_width\":2}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1010\",\"attributes\":{\"tools\":[{\"id\":\"p1023\"},{\"id\":\"p1024\"},{\"id\":\"p1025\"},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1033\"},{\"id\":\"p1034\"},{\"id\":\"p1035\"}]}},\"toolbar_location\":null,\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1018\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1019\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1020\"},\"axis_label\":\"$$\\\\text{Error } [\\\\text{x}100\\\\%]$$\",\"axis_label_text_font\":\"Bitstream Charter\",\"axis_label_text_font_size\":\"12pt\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1021\"},\"major_label_text_font\":\"Bitstream Charter\",\"major_label_text_font_size\":\"10pt\"}}],\"below\":[{\"type\":\"object\",\"name\":\"LogAxis\",\"id\":\"p1013\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"LogTicker\",\"id\":\"p1014\",\"attributes\":{\"num_minor_ticks\":10,\"mantissas\":[1,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"LogTickFormatter\",\"id\":\"p1015\"},\"axis_label\":\"$$\\\\text{Unit Area Runoff }L s^{-1} \\\\text{km}^{-2}$$\",\"axis_label_text_font\":\"Bitstream Charter\",\"axis_label_text_font_size\":\"12pt\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1016\"},\"major_label_text_font\":\"Bitstream Charter\",\"major_label_text_font_size\":\"10pt\"}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1017\",\"attributes\":{\"axis\":{\"id\":\"p1013\"}}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1022\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1018\"}}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1045\",\"attributes\":{\"background_fill_alpha\":0.5,\"label_text_font\":\"Bitstream Charter\",\"label_text_font_size\":\"10pt\",\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1046\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Measurement Error Model\"},\"renderers\":[{\"id\":\"p1042\"}]}}]}}]}},0,0]]}}]}};\n",
       "  const render_items = [{\"docid\":\"21539a5d-d73b-4448-a203-4bc769536054\",\"roots\":{\"p1054\":\"ef627a79-7dc8-4082-a0b7-ce7fd849efee\"},\"root_ids\":[\"p1054\"]}];\n",
       "  void root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    let attempts = 0;\n",
       "    const timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "p1054"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the ranges and associated errors\n",
    "error_points = np.array([0.01, 0.1, 1.0, 10, 1e2, 1e3, 1e4, 1e5, 1e6, 1e7])  # Magnitude points in L/s/km^2\n",
    "error_values = np.array([1., 0.5, 0.25, 0.15, 0.1, 0.1, 0.1, 0.15, 0.2, 0.25])    # Associated errors (as proportions)\n",
    "\n",
    "efig = figure(title=\"Estimated Measurement Error Model\", width=600, height=400, x_axis_type='log')\n",
    "efig.line(error_points, error_values, line_color='red', line_width=2, legend_label='Measurement Error Model')\n",
    "efig.xaxis.axis_label = r'$$\\text{Unit Area Runoff }L s^{-1} \\text{km}^{-2}$$'\n",
    "efig.yaxis.axis_label = r'$$\\text{Error } [\\text{x}100\\%]$$'\n",
    "efig.legend.background_fill_alpha = 0.5\n",
    "efig = dpf.format_fig_fonts(efig, font_size=12)\n",
    "efig = dpf.format_fig_fonts(efig, font_size=12)\n",
    "\n",
    "layout = gridplot([efig], ncols=2, width=500, height=350)\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfefae08-f8a2-492f-a72c-0b4486de624c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def check_support_coverage(baseline_pmf, proxy_distribution):\n",
    "    # find the total mass of the KDE baseline distribution\n",
    "    # where the proxy distribution = 0\n",
    "    mask = np.where(proxy_distribution == 0)\n",
    "    unsupported_pmf = baseline_pmf[mask].sum()\n",
    "    return unsupported_pmf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34548593-38fd-4c1d-af74-0bcddd21c38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epanechnikov_kernel(u):\n",
    "    \"\"\" Epanechnikov kernel: finite support in [-1, 1] \"\"\"\n",
    "    return jnp.where(jnp.abs(u) <= 1, 0.75 * (1 - u**2), 0)\n",
    "\n",
    "def top_hat_kernel(u):\n",
    "    return jnp.where(jnp.abs(u) <= 1, 0.5, 0)\n",
    "\n",
    "def gaussian_kernel(u):\n",
    "    \"\"\" Gaussian kernel: smooth, infinite support \"\"\"\n",
    "    return (1 / jnp.sqrt(2 * jnp.pi)) * jnp.exp(-0.5 * u**2)\n",
    "\n",
    "def compute_silverman_approx(log_data):\n",
    "    q75, q25 = np.percentile(log_data, (75, 25))\n",
    "    stdev = np.std(log_data)\n",
    "    A = np.min([stdev, (q75 - q25) / 1.34])\n",
    "    return 1.06 * A / len(log_data)**0.2\n",
    "\n",
    "\n",
    "def measurement_error_bandwidth_function(x):\n",
    "    error_points = jnp.array([1e-4, 1e-3, 1e-2, 1e-1, 1., 1e1, 1e2, 1e3, 1e4, 1e5])  #  Reference flow points in m^3/s\n",
    "    error_values = jnp.array([1.0, 0.5, 0.2, 0.1, 0.1, 0.1, 0.1, 0.15, 0.2, 0.25])     # Associated errors (as proportions)\n",
    "    # scaling_factor = baseline_h / jnp.min(error_values) \n",
    "    return jnp.interp(x, error_points, error_values) \n",
    "\n",
    "  \n",
    "def compute_measurement_error_informed_adaptive_bandwidth(uar, da):\n",
    "    \"\"\" Compute midpoints in log-space for kernel support boundaries \"\"\"\n",
    "\n",
    "    # get the approximated measurement error associated with each unique FLOW value\n",
    "    flow_data = uar * da / 1000\n",
    "    unique_q = jnp.unique(flow_data)\n",
    "    error_model = measurement_error_bandwidth_function(unique_q)\n",
    "\n",
    "    # error widths must be in unit area runoff log space \n",
    "    # to align with the precision bandwidth correction\n",
    "    # since the KDE is fit to UAR in log space\n",
    "    unique_UAR = (1000 / da) * unique_q \n",
    "    upper_err_UAR = unique_UAR * (1 + error_model)\n",
    "    err_widths_UAR = np.log(upper_err_UAR) - jnp.log(unique_UAR)\n",
    "    \n",
    "    # define bounds for support based on neighbouring unique values\n",
    "    # Compute midpoints between adjacent log_values\n",
    "    log_midpoints = np.log((unique_UAR[:-1] + unique_UAR[1:]) / 2)  # Midpoints of internal values\n",
    "    # Compute left and right extensions by mirroring end widths\n",
    "    left_mirror = unique_UAR[0] - (log_midpoints[0] - unique_UAR[0])\n",
    "    right_mirror = unique_UAR[-1] + (unique_UAR[-1] - log_midpoints[-1])\n",
    "    \n",
    "    # Prepend and append mirrored values\n",
    "    log_midpoints = np.concatenate((np.array([left_mirror]), log_midpoints, np.array([right_mirror])))\n",
    "    # the error distance is half the interval, \n",
    "    # then divide by a z-score to represent the proportion of probability mass\n",
    "    # falling within the range of the midpoints between unique values\n",
    "    log_diffs = np.diff(log_midpoints) / 2 / 1.15\n",
    "    \n",
    "    # integrate where the precision gaps yield larger values than the assumed error\n",
    "    # bw_vals = np.where(log_diffs > err_widths, log_diffs, err_widths)\n",
    "    bw_vals = jnp.where(log_diffs > err_widths_UAR, log_diffs, err_widths_UAR)\n",
    "    # just for interest's sake, find where the precision dominates the error model\n",
    "    # foo = np.sum(np.where(log_diffs > err_widths, 1, 0))\n",
    "    # print(f'    ...precision width dominates: {foo}/{len(values)} cases')\n",
    "    # broadcast the adaptive bandwidth values to the input timeseries\n",
    "    idx = jnp.searchsorted(unique_UAR, uar)\n",
    "    return bw_vals[idx]\n",
    "\n",
    "\n",
    "def adaptive_kde(uar_data, log_grid, da, estimated_grid=None, pdf_error_tol=1e-3, min_allowable_bandwidth=1e-3):\n",
    "    \n",
    "    eval_grid = estimated_grid if estimated_grid is not None else log_grid\n",
    "    \n",
    "    # compute bandwidths according to a measurement error model\n",
    "    # incorporating a test for precision vestiges\n",
    "    bw_values = compute_measurement_error_informed_adaptive_bandwidth(uar_data, da)  # Compute adaptive bandwidth per point\n",
    "\n",
    "    # Expand dimensions: Tile data and bandwidths for matrix operation\n",
    "    n, m = len(uar_data), len(eval_grid)\n",
    "    log_data = jnp.log(uar_data)\n",
    "    X_grid = jnp.tile(eval_grid, (n, 1))  # Shape: (N, M)\n",
    "    X_data = jnp.tile(log_data[:, None], (1, m))  # Shape: (N, M)\n",
    "    H = jnp.tile(bw_values[:, None], (1, m))  # Shape: (N, M)\n",
    "\n",
    "    # Compute u matrix: (N, M)\n",
    "    U = (X_grid - X_data) / H\n",
    "\n",
    "    # Apply Epanechnikov kernel (element-wise)\n",
    "    # K = epanechnikov_kernel(U) / H  # Scale kernel contributions\n",
    "    # K = top_hat_kernel(U) / H\n",
    "    K = gaussian_kernel(U) / H\n",
    "\n",
    "    # Sum contributions across observations\n",
    "    pdf = K.sum(axis=0) / n  # Normalize by sample count\n",
    "    pdf_check = jnp.trapezoid(pdf, x=eval_grid)\n",
    "    pdf /= pdf_check\n",
    "    error = abs(pdf_check - 1)\n",
    "    assert abs(jnp.trapezoid(pdf, x=eval_grid) - 1) < pdf_error_tol, \"PDF does not integrate to 1 in adaptive_kde()\"\n",
    "        \n",
    "    cdf = jnp.cumsum(pdf)\n",
    "    cdf /= cdf[-1]\n",
    "    pmf = jnp.diff(cdf, prepend=0)\n",
    "\n",
    "    # If estimated_grid is used, interpolate PMF onto log_grid\n",
    "    if estimated_grid is not None:\n",
    "        pmf_interp = jnp.interp(log_grid, estimated_grid, pmf, left=0, right=0)#, kind='linear', bounds_error=False, fill_value=(0, 0))\n",
    "        # pmf_interp = interp_func(log_grid)\n",
    "        pmf_interp = np.where(pmf_interp > 0, pmf_interp, 0)\n",
    "        # Do not normalize here, leave unsupported probability mass out\n",
    "        return pmf_interp, pdf\n",
    "\n",
    "    pmf = np.where(pmf > 0, pmf, 0)\n",
    "    return pmf, pdf\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4f4a6ad-f440-44ed-a7b9-b38ee97003de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_kde_fit(log_data, log_grid):\n",
    "    kde_pdf = FFTKDE(bw='ISJ').fit(log_data).evaluate(log_grid)\n",
    "    # Extract the estimated bandwidth\n",
    "    pdf_check = np.trapz(kde_pdf, x=log_grid)\n",
    "    kde_pdf /= pdf_check\n",
    "    # check that the numerical integration over the KDE pdf is close to 1\n",
    "    assert np.isclose(pdf_check, 1, atol=1e-5), f'{pdf_check:.4f} {kde_pdf[:5]} - {kde_pdf[-5:]}'\n",
    "    # pdfs[:, i] = sample_pdf\n",
    "    kde_cdf = np.cumsum(kde_pdf)\n",
    "    kde_cdf /= kde_cdf[-1]\n",
    "    kde_pmf = np.diff(kde_cdf, prepend=0)\n",
    "    assert np.abs(np.sum(kde_pmf) - 1.0) <= 0.001\n",
    "    return kde_pmf, kde_pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "591327b9-f22f-4623-bf04-9cc1efa18e8d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_similarity_weights(stns, attributes=None):\n",
    "    cols = ['official_id', 'log_drainage_area_km2', 'elevation_m', 'prcp', 'tmean', 'swe']\n",
    "    if len(stns) == 1:\n",
    "        return [1.0]\n",
    "    attrs = attr_df[attr_df['official_id'].isin(stns['official_id'])][cols].copy()\n",
    "    attrs = attrs.set_index('official_id').loc[stns['official_id'].values].reset_index()\n",
    "    # make sure the official id order is maintained so the weights are properly assigned\n",
    "    assert np.array_equal(stns['official_id'].values, attrs['official_id'].values)\n",
    "    attrs = attrs[[c for c in attrs.columns if c != 'official_id']]\n",
    "    # normalize weights first within column, then equally along rows\n",
    "    attrs = 1.0 * (attrs - attrs.min()) / (attrs.max() - attrs.min())\n",
    "    normalized_similarity = attrs.mean(axis=1).values\n",
    "    return normalized_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e212a71e-9718-40ed-9168-5703307fa95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k_nearest_neighbors(target_index, tree_type, k=3):\n",
    "    # Query the k+1 nearest neighbors because the first neighbor is the target point itself\n",
    "    if tree_type in ['EW', 'IDW']:\n",
    "        distances, indices = stn_tree.query(coords[target_index], k=k+1)\n",
    "    elif tree_type == 'CAS':\n",
    "        # Example query: Find the 3 nearest neighbors for the first point\n",
    "        distances_norm, indices = attr_tree.query(normalized_attr_values[target_index], k=k+1)\n",
    "        distances = distances_norm * np.linalg.norm(std_devs_attrs)\n",
    "    else:\n",
    "        raise Exception('tree type not identified, must be one of EW, IDW, or CAS.')\n",
    "\n",
    "    # Remove the target itself from the results\n",
    "    neighbor_indices = indices[1:]\n",
    "    neighbor_distances = distances[1:]\n",
    "    return neighbor_indices, np.round(neighbor_distances / 1000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fbb9956-65ed-498a-b8ac-b8a1d5469fd4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def check_neighbours(stn, distance, data):\n",
    "    proxy_df = dpf.get_timeseries_data(stn)\n",
    "    # data.set_index('time', inplace=True)\n",
    "    proxy_df.set_index('time', inplace=True)\n",
    "    df = pd.concat([data, proxy_df], axis=1, join='inner')\n",
    "    pct_covered = len(df) / len(data)\n",
    "    return (stn, distance, proxy_df) if (pct_covered >= 0.75) & (len(df) > 350) else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14ba0631-aff7-4c62-9b2c-b92172ce2c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, output_file, save\n",
    "from bokeh.models import Div\n",
    "from bokeh.layouts import layout\n",
    "\n",
    "def output_figure(target_stn, eval_grid, log_data, predicted_param_dist, lognorm_dist, mom_ln_dist, kde_dist, epan_kde_dist, best_knn, fpath, result, support_dict, plot_type='pdf'):\n",
    "    print('    processing output figure')\n",
    "    fig = figure(title=f'{target_stn} PDF estimation', width=900, height=450, x_axis_type='log')\n",
    "    hist, log_edges = np.histogram(log_data, bins=2**6, density=True)\n",
    "    edges = np.exp(log_edges)\n",
    "    \n",
    "    # convert density to mass\n",
    "    cdf = np.cumsum(hist)\n",
    "    cdf /= cdf[-1]\n",
    "    pmf = np.diff(cdf, prepend=cdf[0])\n",
    "    pmf /= pmf.sum()\n",
    "    \n",
    "    if plot_type == 'pmf':\n",
    "        hist = pmf\n",
    "    \n",
    "    if not np.isclose(np.sum(pmf), 1, atol=1e-3):\n",
    "        print(f'Histogram sum != 1: {pmf.sum():.3f}')\n",
    "\n",
    "    yrs = len(log_data) / 365\n",
    "    fig.quad(bottom=0, left=edges[:-1], right=edges[1:], top=hist, color='lightgreen', fill_alpha=0.5, legend_label=f'Data (N={yrs:.1f} yrs)')\n",
    "\n",
    "    x = np.exp(eval_grid)\n",
    "    fig.line(x, predicted_param_dist, color='dodgerblue', line_dash='dotted', line_width=2, legend_label='Predicted LN Params')\n",
    "    fig.line(x, lognorm_dist, color='dodgerblue', line_dash='solid', line_width=2, legend_label='MLE LogNorm')\n",
    "    fig.line(x, mom_ln_dist, color='dodgerblue', line_dash='dashed', line_width=2, legend_label='MOM LogNorm')\n",
    "    fig.line(x, kde_dist, color='grey', line_dash='solid', line_width=2, legend_label='KDE')\n",
    "    fig.line(x, epan_kde_dist, color='black', line_dash='solid', line_width=2, legend_label='EpanKDE')\n",
    "    fig.line(x, best_knn['EW'][2], color='orange', line_dash='solid', line_width=2, legend_label=best_knn['EW'][0])\n",
    "    fig.line(x, best_knn['IDW'][2], color='orange', line_dash='dashed', line_width=2, legend_label=best_knn['IDW'][0])\n",
    "    fig.line(x, best_knn['CAS'][2], color='orange', line_dash='dotted', line_width=2, legend_label=best_knn['CAS'][0])\n",
    "    fig.line(x, best_knn['CASdist'][2], color='darkorange', line_dash='solid', line_width=2, legend_label=best_knn['CASdist'][0])\n",
    "    \n",
    "    fig.legend.click_policy='hide'\n",
    "    fig.add_layout(fig.legend[0], 'right')\n",
    "    fig.legend.background_fill_alpha = 0.5\n",
    "    fig.xaxis.axis_label = r'$$\\text{Runoff } [Ls^{-1}\\text{km}^{-2}]$$'\n",
    "    fig.yaxis.axis_label = r'$$\\text{Probability Density}$$'\n",
    "    fig = dpf.format_fig_fonts(fig, font_size=14)\n",
    "    res_table = pd.DataFrame(result, index=['DKL']).T    \n",
    "    res_table.index.name = 'Model'\n",
    "    res_table = res_table.sort_values(by='DKL')\n",
    "    res_table.reset_index(inplace=True)\n",
    "\n",
    "    min_res = res_table['DKL'].min()\n",
    "    res_table['pct_from_top'] = round(100 * (res_table['DKL'] - min_res) / min_res, 0)\n",
    "    suppt_table = pd.DataFrame(support_dict, index=['unsupported_mass']).T\n",
    "    suppt_table.index.name = 'Model'\n",
    "    suppt_table = suppt_table.sort_values(by='unsupported_mass')\n",
    "    suppt_table.reset_index(inplace=True)\n",
    "    top_div = Div(text=res_table.head(5).to_html(index=True, border=0))\n",
    "    divs, sup_tabs, param_tabs = [], [], []\n",
    "    np_models = ['EW_', 'IDW_', 'CAS_', 'CASdist_']\n",
    "    for tp in np_models:\n",
    "        res = res_table[res_table['Model'].str.contains(tp)].copy().round(3)\n",
    "        tab = Div(text=res.to_html(index=True, border=0))\n",
    "        divs.append(tab)\n",
    "        sup = suppt_table[suppt_table['Model'].str.contains(tp)].copy().round(2)\n",
    "        result2 = Div(text=sup.to_html(index=False, border=0))\n",
    "        sup_tabs.append(result2)\n",
    "    p_mods = [c for c in res_table['Model'].values if all(n not in c for n in np_models)]\n",
    "    res = res_table[res_table['Model'].isin(p_mods)].copy().round(3)\n",
    "    tab = Div(text=res.to_html(index=True, border=0))\n",
    "    param_tabs.append(tab)\n",
    "        \n",
    "    lt = layout([[fig], [top_div, param_tabs], divs, sup_tabs])\n",
    "    # show(fig)\n",
    "    output_file(filename=fpath, title=f\"{target_stn}\")\n",
    "    save(lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6af77eb5-f4b4-4f56-8cf4-44a7f8fdc4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(self, i, row, k_nearest=10, n_neighbours_to_check=150, n_grid_points=2**14, left_log=-3, right_log=3):\n",
    "        for k, v in row.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "        self.result, self.support_dict = {}, {}  \n",
    "        stn = self.official_id # target catchment\n",
    "        self.figure_fpath = os.path.join(BASE_DIR, 'data', 'knn_comparison_plots', f'{stn}_distribution_prediction.html')\n",
    "        self.target_stn = stn\n",
    "        # self.params = {k: v[0] for k, v in ln_df[ln_df['official_id'] == stn].copy().to_dict(orient='list').items()}\n",
    "        self.params = predicted_param_dict[stn] \n",
    "        \n",
    "        # import the streamflow data and do uar and log conversions\n",
    "        self.stn_df = dpf.get_timeseries_data(stn)\n",
    "        self.stn_df.set_index('time', inplace=True)\n",
    "        self.stn_df[f'{stn}_uar'] = 1000 * self.stn_df[stn] / self.drainage_area_km2\n",
    "        \n",
    "        self.uar = self.stn_df[f'{stn}_uar'].dropna().values\n",
    "        self.log_uar = np.log(self.uar.reshape(-1, 1))\n",
    "        self.n_obs = len(self.uar)\n",
    "        self.n_grid_points = n_grid_points\n",
    "        self.n_neighbours_to_check = n_neighbours_to_check\n",
    "        self.k_nearest = k_nearest\n",
    "        self.index_to_id = index_to_id\n",
    "        self.id_to_index = id_to_index\n",
    "        self.da_dict = da_dict  # Store drainage areas\n",
    "        self.target_da = da_dict[stn]\n",
    "        self.target_tree_index = i\n",
    "        self.initialize_nearest_neighbour_data()\n",
    "        self.left_log = left_log\n",
    "        self.right_log = right_log\n",
    "        self.set_grid()\n",
    "        print('    ...completed initialization.')\n",
    "\n",
    "    def set_nearest_nbr_data(self, tree_type):\n",
    "\n",
    "        neighbour_idxs, distances = find_k_nearest_neighbors(self.target_tree_index, tree_type, k=n_neighbours_to_check)        \n",
    "        neighbours = attr_gdf.iloc[neighbour_idxs]['official_id'].tolist()\n",
    "        # for each proxy, load the MLE and predicted mean and standard deviation from XGBoost model  \n",
    "        # checked_neighbours = Parallel(n_jobs=-1, backend=\"loky\")(delayed(check_neighbours)(stn, dist, df.copy()) \n",
    "                                              # for stn, dist in zip(neighbours, distances))\n",
    "        with ThreadPoolExecutor(max_workers=22) as executor:  # Adjust max workers\n",
    "            checked_neighbours = list(executor.map(check_neighbours, neighbours, distances, [self.stn_df.copy()]*len(neighbours)))\n",
    "    \n",
    "        good_nbrs = [e for e in checked_neighbours if e is not None]\n",
    "        if len(good_nbrs) == 0:\n",
    "            raise Exception('No suitable nearest neighbours found')\n",
    "        \n",
    "        good_nbrs = sorted(good_nbrs, key=lambda tup: tup[1])\n",
    "        nbr_data = pd.DataFrame([e[:2] for e in good_nbrs], columns=['official_id', 'distance'])\n",
    "        \n",
    "        nbr_df = pd.concat([e[2] for e in good_nbrs], join='inner', axis=1)\n",
    "        n_found = len(good_nbrs)\n",
    "        if n_found < self.k_nearest:\n",
    "            raise Exception(f'{n_found}/{self.k_nearest} suitable nearest neighbours found')\n",
    "        return nbr_df, nbr_data\n",
    "\n",
    "\n",
    "    def initialize_nearest_neighbour_data(self):\n",
    "        print(f'    ...searching for minimum {self.k_nearest} in {self.n_neighbours_to_check} nearest neighbours with minimum concurrent record.')\n",
    "        self.nbr_df, self.nbr_data = self.set_nearest_nbr_data('EW')\n",
    "        self.nbr_df_attr, self.nbr_data_attr = self.set_nearest_nbr_data('CAS')\n",
    "        # set spatial distance in attribute space dataframe\n",
    "        self.nbr_data_attr['spatial_dist'] = self.nbr_data_attr['official_id'].apply(lambda x: self.query_distance(stn_tree, x, self.target_stn)/1e3)\n",
    "        # set attribute distances in the geographic distance dataframe\n",
    "        self.nbr_data['attr_dist'] = self.nbr_data['official_id'].apply(lambda x: self.query_distance(attr_tree, x, self.target_stn))\n",
    "\n",
    "        self.nbr_data = self.normalize_dataframe(self.nbr_data)\n",
    "        self.nbr_data_attr = self.normalize_dataframe(self.nbr_data_attr)\n",
    "        \n",
    "\n",
    "    def set_grid(self):\n",
    "        epsilon = 1e-6 \n",
    "        minx, maxx = np.min(self.uar) - epsilon, np.max(self.uar) + epsilon\n",
    "        self.baseline_log_grid = np.linspace(np.log(minx) + self.left_log, np.log(maxx) + self.right_log, self.n_grid_points)\n",
    "        self.baseline_lin_grid = np.exp(self.baseline_log_grid)\n",
    "\n",
    "    \n",
    "    def query_distance(self, tree, id1, id2):\n",
    "        \"\"\"Query distance between two points in a tree using official_id.\"\"\"\n",
    "        if id1 not in self.id_to_index or id2 not in self.id_to_index:\n",
    "            raise ValueError(f\"One or both IDs ({id1}, {id2}) not found.\")\n",
    "    \n",
    "        # Get indices from ID mapping\n",
    "        index1, index2 = self.id_to_index[id1], self.id_to_index[id2]        \n",
    "        # Query the distance\n",
    "        distance = np.linalg.norm(tree.data[index1] - tree.data[index2])  # Euclidean distance\n",
    "        return distance\n",
    "\n",
    "    def compute_MLE_lognorm(self):\n",
    "        obs_mean, obs_std = self.params['actual_mean_logx'], self.params['actual_sd_logx']\n",
    "        self.lognorm_pmf, self.lognorm_pdf = compute_lognorm_pmf(self.baseline_log_grid, obs_mean, obs_std)\n",
    "        kld, support = self.compute_kld(self.baseline_pmf, self.lognorm_pmf)\n",
    "        self.result['LN_MLE_DKL'] = kld.item()\n",
    "        self.support_dict['LN_MLE_DKL'] = support\n",
    "        \n",
    "\n",
    "    def ensemble_distribution_estimates(self, knn_df, ensemble_label, distance_weights=None, epsilon=0.5):\n",
    "    \n",
    "        pdfs, pmfs = pd.DataFrame(), pd.DataFrame()\n",
    "        log_knn = np.log(knn_df)\n",
    "        \n",
    "        for c in knn_df.columns:\n",
    "            proxy_stn = c.split('_')[0]\n",
    "            # print('proxy stn: ', proxy_stn, da_dict[proxy_stn])\n",
    "            \n",
    "            est_min, est_max = knn_df[c].min(), knn_df[c].max()\n",
    "            est_grid = np.linspace(np.log(est_min) - self.left_log, np.log(est_max) + self.right_log, self.n_grid_points)\n",
    "            k_pmf, k_pdf = adaptive_kde(knn_df[c].values, self.baseline_log_grid, da_dict[proxy_stn], estimated_grid=est_grid)\n",
    "            pmfs[c], pdfs[c] = k_pmf, k_pdf\n",
    "        # print('distance weights: ', distance_weights)\n",
    "        # Normalize distance weights\n",
    "        if distance_weights is not None:\n",
    "            distance_weights /= np.sum(distance_weights)\n",
    "    \n",
    "        if distance_weights is not None:\n",
    "            distance_weights = np.array(distance_weights)  # Ensure 1D array\n",
    "            pdf_est = pdfs.to_numpy() @ distance_weights\n",
    "        else:\n",
    "            pdf_est = pdfs.mean(axis=1).to_numpy()\n",
    "        # Check integral before normalization\n",
    "        pdf_check = np.trapz(pdf_est, x=self.baseline_log_grid)#.reshape(-1, 1)\n",
    "        if not np.isclose(pdf_check, 1, atol=1e-3):\n",
    "            pdf_est /= pdf_check  # Only normalize if necessary\n",
    "    \n",
    "        # Compute CDF and PMF\n",
    "        cdf_est = np.cumsum(pdf_est)\n",
    "        cdf_est /= cdf_est[-1]\n",
    "        pmf_est = np.diff(cdf_est, prepend=0)\n",
    "        \n",
    "        return pmf_est, pdf_est\n",
    "\n",
    "    \n",
    "    def compute_weights(self, distances, power=2):\n",
    "        \"\"\"Compute normalized inverse distance weights.\"\"\"\n",
    "        distances = np.maximum(distances, 1e-4)  # Prevent division by zero\n",
    "        weights = 1 / (distances ** power)\n",
    "        return weights / weights.sum()  # Normalize to sum to 1\n",
    "\n",
    "    \n",
    "    def normalize_dataframe(self, df, exclude_col='official_id'):\n",
    "        \"\"\"Normalize all columns except `exclude_col` using min-max scaling.\"\"\"\n",
    "        cols = [c for c in df.columns if c != exclude_col]\n",
    "        df[cols] = (df[cols] - df[cols].min()) / (df[cols].max() - df[cols].min())\n",
    "        return df\n",
    "\n",
    "            \n",
    "    def format_knn_pmf_inputs(self, nbrs, knn_df):\n",
    "        # Compute Unit Area Runoff (UAR) for each neighbor\n",
    "        cols = []\n",
    "        for s in nbrs:\n",
    "            uar_col = f'{s}_uar'    \n",
    "            da = da_dict[s]\n",
    "            assert ~np.isnan(da)\n",
    "            knn_df[uar_col] = 1000 * knn_df[s] / da\n",
    "            cols.append(uar_col)\n",
    "        return knn_df, cols\n",
    "        \n",
    "    \n",
    "    def estimate_knn_pmf_pdf(self, df, cols, weights=None):\n",
    "        \"\"\"Estimate PMF and PDF using adaptive KDE with optional weights.\"\"\"\n",
    "        \n",
    "        assert ~df[cols].empty, 'dataframe is empty'\n",
    "        assert df[cols].notna().all().all(), \"NaN values found in df[cols] before processing\"\n",
    "        if weights is not None:\n",
    "            assert ~np.any(np.isnan(weights)), f'nan weight found: {weights}'\n",
    "            assert np.isclose(np.sum(weights), 1), f'weights do not sum to 1: {weights}'\n",
    "            assert (weights > 0).all(), f'not all weights > 0, {weights}'\n",
    "            estimate = df[cols].mul(weights, axis=1).sum(axis=1)\n",
    "        else:\n",
    "            estimate = df[cols].mean(axis=1)\n",
    "\n",
    "        est_min, est_max = df[cols].min().min(), df[cols].max().max()\n",
    "        est_grid = np.linspace(np.log(est_min) - self.left_log, np.log(est_max) + self.right_log, self.n_grid_points)\n",
    "    \n",
    "        assert (estimate >= 0).all(), f\"Estimate < 0 detected: {np.min(estimate)}\"\n",
    "        pmf, pdf = adaptive_kde(estimate.values, self.baseline_log_grid, self.target_da, estimated_grid=est_grid)\n",
    "        return pmf, pdf\n",
    "        \n",
    "\n",
    "    def knn_pmf_estimation(self):\n",
    "        \"\"\"\n",
    "        Generate PDF/PMF estimates for the target catchment using kNN.\n",
    "        \"\"\"\n",
    "        knn_pmfs, knn_pdfs = pd.DataFrame(), pd.DataFrame()\n",
    "        stn = self.official_id\n",
    "        print(f'    Processing kNN for {stn}')\n",
    "        \n",
    "        t0 = time()\n",
    "        for k in range(1, self.k_nearest+1):\n",
    "            # Get spatial kNN\n",
    "            k_nbrs = self.nbr_data.iloc[:k].copy()\n",
    "            nbr_stns = k_nbrs['official_id'].values\n",
    "            knn_df, knn_cols = self.format_knn_pmf_inputs(nbr_stns, self.nbr_df.copy()) \n",
    "\n",
    "            # Compute Equal Weighting (EW) PMF/PDF\n",
    "            label = f'{k}NN_EW_{stn}'\n",
    "            knn_pmfs[label], knn_pdfs[label] = self.estimate_knn_pmf_pdf(knn_df, knn_cols)\n",
    "\n",
    "            # Compute IDW-based PMF/PDF\n",
    "            k_nbrs['distance'] = np.maximum(k_nbrs['distance'].values, 1e-5)\n",
    "            distance_weights = self.compute_weights(k_nbrs['distance'].values)\n",
    "            \n",
    "            label = f'{k}NN_IDW_{stn}'\n",
    "            knn_pmfs[label], knn_pdfs[label] = self.estimate_knn_pmf_pdf(knn_df, knn_cols, distance_weights)\n",
    "                \n",
    "            # Get attribute-space kNN\n",
    "            k_nbrs_attr = self.nbr_data_attr.iloc[:k].copy()\n",
    "            nbr_stns_attr = k_nbrs_attr['official_id'].values\n",
    "            knn_df_attr, knn_attr_cols = self.format_knn_pmf_inputs(nbr_stns_attr, self.nbr_df_attr.copy())    \n",
    "    \n",
    "            # Compute CAS-based PMF/PDF\n",
    "            if k == 0:\n",
    "                cas_weights = np.array([1])\n",
    "            else:\n",
    "                k_nbrs_attr['distance'] = np.maximum(k_nbrs_attr['distance'].values, 1e-5)\n",
    "                cas_weights = self.compute_weights(k_nbrs_attr['distance'].values)\n",
    "                \n",
    "            label = f'{k}NN_CAS_{stn}'\n",
    "            knn_pmfs[label], knn_pdfs[label] = self.estimate_knn_pmf_pdf(knn_df_attr, knn_attr_cols, cas_weights)\n",
    "            # Compute CAS + Distance PMF/PDF\n",
    "            k_nbrs_dist_attr = self.nbr_data.iloc[:k].copy()\n",
    "            nbr_stns_dist_attr = k_nbrs_dist_attr['official_id'].values\n",
    "            knn_df_dist_attr, knn_dist_attr_cols = self.format_knn_pmf_inputs(nbr_stns_dist_attr, self.nbr_df.copy())  \n",
    "            \n",
    "            k_nbrs_dist_attr['sum'] = np.maximum(k_nbrs_dist_attr['distance'] + k_nbrs_dist_attr['attr_dist'], 1e-3)\n",
    "            cas_dist_weights = self.compute_weights(k_nbrs_dist_attr['sum'].values)\n",
    "    \n",
    "            label = f'{k}NN_CASdist_{stn}'\n",
    "            knn_pmfs[label], knn_pdfs[label] = self.estimate_knn_pmf_pdf(knn_df_dist_attr, knn_dist_attr_cols, cas_dist_weights)\n",
    "            # Compute ensemble estimates\n",
    "            knn_sets = [\n",
    "                (\"EW\", knn_df, knn_cols, None), \n",
    "                (\"IDW\", knn_df, knn_cols, distance_weights), \n",
    "                (\"CAS\", knn_df_attr, knn_attr_cols, cas_weights), \n",
    "                (\"CASdist\", knn_df_dist_attr, knn_dist_attr_cols, cas_dist_weights)\n",
    "            ]\n",
    "            for method, df, cols, weights in knn_sets:\n",
    "                ensemble_label = f'{k}NN_{method}_ensemble_{stn}'\n",
    "                pmf_est, pdf_est = self.ensemble_distribution_estimates(df[cols].copy(), ensemble_label, distance_weights=weights)\n",
    "                if pmf_est is None:\n",
    "                    return [], [], test_data\n",
    "                knn_pdfs[ensemble_label] = pdf_est\n",
    "                knn_pmfs[ensemble_label] = pmf_est\n",
    "    \n",
    "        return knn_pmfs, knn_pdfs\n",
    "        \n",
    "\n",
    "    def compute_pmf_from_predicted_params(self, mu_hat, sd_hat):\n",
    "               \n",
    "        norm_pdf = norm.pdf(self.baseline_log_grid, loc=self.params[mu_hat], scale=self.params[sd_hat])\n",
    "        pdf_check = np.trapz(norm_pdf, x=self.baseline_log_grid)\n",
    "        norm_pdf /= pdf_check\n",
    "    \n",
    "        if not np.isclose(np.trapz(norm_pdf, x=self.baseline_log_grid), 1, atol=1e-3):\n",
    "            if norm_pdf[0] > norm_pdf[-1]:\n",
    "                self.left_log -= 1\n",
    "            else:\n",
    "                self.right_log += 1\n",
    "            msg = f'   Predicted param pdf_check failed: {pdf_check:.5f} {norm_pdf[:5]}, {norm_pdf[-5:]} new log range {self.left_log}-{self.right_log}'\n",
    "            print(msg)\n",
    "            raise Exception(msg)\n",
    "            \n",
    "        norm_cdf = norm_pdf.cumsum()\n",
    "        norm_cdf /= norm_cdf[-1]\n",
    "        norm_pmf = np.diff(norm_cdf, prepend=0)\n",
    "        return norm_pmf, norm_pdf\n",
    "        \n",
    "\n",
    "    def process_best_knn_results(self, knn_pmfs, knn_pdfs):\n",
    "        print('    ...processing knn result')\n",
    "        knn_results_dict = {}\n",
    "        for knn_type in ['EW', 'IDW', 'CAS', 'CASdist']:\n",
    "            knn_cols = [c for c in knn_pmfs.columns if f'_{knn_type}_' in c]    \n",
    "            min_knn, best_knn = 1e9, None\n",
    "            for c in knn_cols: \n",
    "                q = knn_pmfs[c].values\n",
    "                q_pdf = knn_pdfs[c].values               \n",
    "                knn_kld, support = self.compute_kld(self.baseline_pmf, q)\n",
    "                self.support_dict[c] = support\n",
    "                prior_bias = support['bias']\n",
    "                if prior_bias > 0.1 * knn_kld.item():\n",
    "                    pct_bias = round(100 * (prior_bias / knn_kld), 1)\n",
    "                    self.support_dict[c]['pct_of_signal'] = pct_bias\n",
    "                    # print(f'    {c}: Prior bias {prior_bias:.3f} bits/sample bias {pct_bias:.1f}% of the KLD')\n",
    "                    \n",
    "                self.result[c] = knn_kld.item()\n",
    "                if knn_kld < min_knn:\n",
    "                    min_knn = knn_kld.item()\n",
    "                    best_knn = (c, q, q_pdf)\n",
    "            knn_results_dict[knn_type] = best_knn\n",
    "        return knn_results_dict\n",
    "\n",
    "        \n",
    "    def compute_kld(self, p, q, prior=1):\n",
    "        # Ensure q is at least 2D for consistent broadcasting\n",
    "        mask = (p > 0) #& (q > 0)\n",
    "        kld_array = jnp.zeros_like(p)\n",
    "        unsupported_mass = check_support_coverage(p, q)\n",
    "        prior_bias = 0\n",
    "        if not (q > 0).all():\n",
    "            q_mod = self.n_obs * q + [prior for _ in q]\n",
    "            q_mod /= q_mod.sum()\n",
    "            prior_bias = jnp.sum(jnp.where(mask, p * jnp.log2(p / q_mod), 0))\n",
    "            q = q_mod\n",
    "            \n",
    "        prior_bias_dict = {\n",
    "            'bias': prior_bias, \n",
    "            'prior': prior,\n",
    "            'unsupported_mass': round(100 * unsupported_mass, 1)\n",
    "        }\n",
    "        return jnp.sum(jnp.where(mask, p * jnp.log2(p / q), 0)), prior_bias_dict\n",
    "        \n",
    "\n",
    "    def process_target(self):\n",
    "        stn = self.official_id # target catchment\n",
    "        \n",
    "        self.kde_pmf, self.kde_pdf = single_kde_fit(self.log_uar, self.baseline_log_grid)\n",
    "        print('   ...processed single kde fit')\n",
    "        \n",
    "        self.baseline_pmf, self.baseline_pdf = adaptive_kde(self.uar, self.baseline_log_grid, self.drainage_area_km2)        \n",
    "        print('   ...adaptive (baseline) kde fit')\n",
    "        \n",
    "        # compute parametric PMFs (Lognorm MLE and predicted params) for the target\n",
    "        # compute the pmf from the lognorm parameters predicted from catchment attributes\n",
    "        mom_param_pmf, mom_param_pdf = self.compute_pmf_from_predicted_params('predicted_LN_MMO_mu_hat', 'predicted_LN_MMO_sd_hat')\n",
    "        kld, support = self.compute_kld(self.baseline_pmf, mom_param_pmf)\n",
    "        self.result['LN_MOM_DKL'] = kld.item()\n",
    "        self.support_dict['LN_MOM_DKL'] = support\n",
    "        print('   ...processed pmf from predicted MOM parameters')\n",
    "        \n",
    "        predicted_param_pmf, predicted_param_pdf = self.compute_pmf_from_predicted_params('predicted_mean_logx', 'predicted_sd_logx')\n",
    "        kld, support = self.compute_kld(self.baseline_pmf, predicted_param_pmf)\n",
    "        self.result['LN_predicted_params_DKL'] = kld.item()\n",
    "        self.support_dict['LN_predicted_params_DKL'] = support\n",
    "        print('   ...processed pmf from direct predicted LN parameters')\n",
    "        \n",
    "        tc = time()\n",
    "        knn_pmf_estimates, knn_pdf_estimates = self.knn_pmf_estimation()\n",
    "        td = time()\n",
    "        print(f'Time to complete knn: {td-tc:.1f}')\n",
    "        knn_results_dict = self.process_best_knn_results(knn_pmf_estimates, knn_pdf_estimates)\n",
    "        \n",
    "        # keep track of whether the lognorm yields incomplete support coverage\n",
    "        self.compute_MLE_lognorm()\n",
    "            \n",
    "        foo = pd.DataFrame(self.result, index=['DKL']).round(4)\n",
    "        foo.index.name = 'Model'\n",
    "        print(foo.T.sort_values(by='DKL').head(10))\n",
    "        \n",
    "        output_figure(stn, self.baseline_log_grid, self.log_uar, predicted_param_pdf, \n",
    "                      self.lognorm_pdf, mom_param_pdf, self.kde_pdf, self.baseline_pdf, \n",
    "                      knn_results_dict, self.figure_fpath, self.result, self.support_dict)\n",
    "        \n",
    "        return self.result, self.support_dict\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d04cf00e-cdd8-4b67-b7c9-38688c28ebe2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'attr_gdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m to_check \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m05AD003\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m05AD031\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m05AB022\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m05AB030\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m05AD031\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m12091050\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m n_processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mattr_gdf\u001b[49m\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     10\u001b[0m     stn \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mofficial_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     11\u001b[0m     result_fpath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_knn_result\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'attr_gdf' is not defined"
     ]
    }
   ],
   "source": [
    "display_problem_fig = True\n",
    "n_bootstrap_samples = 100\n",
    "n_grid_points = 2**12\n",
    "n_neighbours_to_check = 150\n",
    "k_nearest = 10\n",
    "to_check = ['05AD003','05AD031', '05AB022', '05AB030', '05AD031', '12091050']\n",
    "n_processed = 1\n",
    "\n",
    "for i, row in attr_gdf.iterrows():\n",
    "    stn = row['official_id']\n",
    "    result_fpath = os.path.join(BASE_DIR, 'data', 'temp', f'{stn}_knn_result')\n",
    "    support_fpath = os.path.join(BASE_DIR, 'data', 'temp', f'{stn}_knn_support_result')\n",
    "    # if not os.path.exists(fpath): \n",
    "    \n",
    "    experiment = Experiment(i, row, k_nearest=k_nearest, n_neighbours_to_check=n_neighbours_to_check, n_grid_points=n_grid_points)\n",
    "    left_log, right_log = -2, 2\n",
    "    result, support_dict = experiment.process_target()                \n",
    "    res = pd.DataFrame(result, index=list(range(len(result))))\n",
    "    support_res = pd.DataFrame(support_dict, index=list(range(len(result))))\n",
    "    res.to_csv(result_fpath)\n",
    "    support_res.to_csv(support_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6b1250-c0f9-4931-837f-b2cf4159507a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aae9069-28de-466e-be3b-6fd2d6c2965a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60c6a6c-dbed-43e1-91f0-30af4c8f21c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57957fb4-893c-488d-bb47-be3c63505a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
