{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "558cd32f-b562-4502-ab63-826aede8f321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from time import time\n",
    "import geopandas as gpd\n",
    "\n",
    "import xgboost as xgb\n",
    "xgb.config_context(verbosity=2)\n",
    "from scipy.stats import norm, laplace, genextreme\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from kde_estimator import KDEEstimator\n",
    "from fdc_estimator_context import FDCEstimationContext \n",
    "from fdc_data import StationData\n",
    "\n",
    "import data_processing_functions as dpf\n",
    "\n",
    "from pathlib import Path\n",
    "BASE_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1356ccc6-3d6d-4e08-bdbc-53a798187d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"fc12d38c-27dd-4d50-871b-82d87d4ac9e8\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"fc12d38c-27dd-4d50-871b-82d87d4ac9e8\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.7.3.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"fc12d38c-27dd-4d50-871b-82d87d4ac9e8\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.layouts import gridplot\n",
    "\n",
    "import xyzservices.providers as xyz\n",
    "tiles = xyz['USGS']['USTopo']\n",
    "output_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "894b3bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1024 monitored basins in the attribute set.\n"
     ]
    }
   ],
   "source": [
    "# load the catchment characteristics\n",
    "fname = f'catchment_attributes_with_runoff_stats.csv'\n",
    "attr_df = pd.read_csv(os.path.join('data', fname), dtype={'official_id': str, 'drainage_area_km2': float})\n",
    "attr_df.columns = [c.lower() for c in attr_df.columns]\n",
    "attr_df['log_drainage_area_km2'] = np.log(attr_df['drainage_area_km2'])\n",
    "# attr_df = attr_df[~attr_df['official_id'].isin(exclude)]\n",
    "# attr_df.columns = [c.lower() for c in attr_df.columns]\n",
    "attr_df['tmean'] = (attr_df['tmin'] + attr_df['tmax']) / 2.0\n",
    "station_ids = attr_df['official_id'].values\n",
    "# assert '12414900' in station_ids\n",
    "\n",
    "print(f'There are {len(station_ids)} monitored basins in the attribute set.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3b463650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Watershed_ID</th>\n",
       "      <th>Source</th>\n",
       "      <th>Name</th>\n",
       "      <th>Official_ID</th>\n",
       "      <th>Centroid_Lat_deg_N</th>\n",
       "      <th>Centroid_Lon_deg_E</th>\n",
       "      <th>Drainage_Area_km2</th>\n",
       "      <th>Drainage_Area_GSIM_km2</th>\n",
       "      <th>Flag_GSIM_boundaries</th>\n",
       "      <th>Flag_Artificial_Boundaries</th>\n",
       "      <th>...</th>\n",
       "      <th>Land_Use_Wetland_frac</th>\n",
       "      <th>Land_Use_Water_frac</th>\n",
       "      <th>Land_Use_Urban_frac</th>\n",
       "      <th>Land_Use_Shrubs_frac</th>\n",
       "      <th>Land_Use_Crops_frac</th>\n",
       "      <th>Land_Use_Snow_Ice_frac</th>\n",
       "      <th>Flag_Land_Use_Extraction</th>\n",
       "      <th>Permeability_logk_m2</th>\n",
       "      <th>Porosity_frac</th>\n",
       "      <th>Flag_Subsoil_Extraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>847</td>\n",
       "      <td>HYDAT</td>\n",
       "      <td>CROWSNEST RIVER AT FRANK</td>\n",
       "      <td>05AA008</td>\n",
       "      <td>49.59732</td>\n",
       "      <td>-114.4106</td>\n",
       "      <td>402.6522</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>0.0785</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>1</td>\n",
       "      <td>-15.543306</td>\n",
       "      <td>0.170479</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>850</td>\n",
       "      <td>HYDAT</td>\n",
       "      <td>CASTLE RIVER NEAR BEAVER MINES</td>\n",
       "      <td>05AA022</td>\n",
       "      <td>49.48866</td>\n",
       "      <td>-114.1444</td>\n",
       "      <td>820.6510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.1156</td>\n",
       "      <td>0.0246</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>-15.929747</td>\n",
       "      <td>0.150196</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Watershed_ID Source                            Name Official_ID  \\\n",
       "846           847  HYDAT        CROWSNEST RIVER AT FRANK     05AA008   \n",
       "849           850  HYDAT  CASTLE RIVER NEAR BEAVER MINES     05AA022   \n",
       "\n",
       "     Centroid_Lat_deg_N  Centroid_Lon_deg_E  Drainage_Area_km2  \\\n",
       "846            49.59732           -114.4106           402.6522   \n",
       "849            49.48866           -114.1444           820.6510   \n",
       "\n",
       "     Drainage_Area_GSIM_km2  Flag_GSIM_boundaries  Flag_Artificial_Boundaries  \\\n",
       "846                     NaN                     0                           0   \n",
       "849                     NaN                     0                           0   \n",
       "\n",
       "     ...  Land_Use_Wetland_frac  Land_Use_Water_frac  Land_Use_Urban_frac  \\\n",
       "846  ...                 0.0103               0.0065               0.0328   \n",
       "849  ...                 0.0058               0.0023               0.0105   \n",
       "\n",
       "     Land_Use_Shrubs_frac  Land_Use_Crops_frac  Land_Use_Snow_Ice_frac  \\\n",
       "846                0.0785               0.0015                  0.0002   \n",
       "849                0.1156               0.0246                  0.0000   \n",
       "\n",
       "     Flag_Land_Use_Extraction  Permeability_logk_m2  Porosity_frac  \\\n",
       "846                         1            -15.543306       0.170479   \n",
       "849                         1            -15.929747       0.150196   \n",
       "\n",
       "     Flag_Subsoil_Extraction  \n",
       "846                        1  \n",
       "849                        1  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# streamflow folder from (updated) HYSETS\n",
    "HYSETS_DIR = Path('/home/danbot/code/common_data/HYSETS')\n",
    "# STREAMFLOW_DIR = HYSETS_DIR / 'streamflow'\n",
    "\n",
    "hs_df = pd.read_csv('data/HYSETS_watershed_properties.txt', sep=';')\n",
    "hs_df = hs_df[hs_df['Official_ID'].isin(station_ids)]\n",
    "hs_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "54a492f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the baseline PMFs from the previous notebook\n",
    "pmf_path = Path(os.getcwd()) / 'data' / 'results' / 'baseline_distributions' / f'bcub_pmfs.csv'\n",
    "pmf_df = pd.read_csv(pmf_path, index_col=0)\n",
    "pmf_stations = pmf_df.columns\n",
    "assert np.all(np.isin(pmf_stations, station_ids)), \"Not all stations with a baseline PMF are in the attribute station set.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6751a465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 12414900 is in LSTM results but not in the station attributes.\n",
      "Warning: 15056030 is in LSTM results but not in the station attributes.\n",
      "Warning: 12102190 is in LSTM results but not in the station attributes.\n",
      "There are 722 monitored basins concurrent with LSTM ensemble results.\n",
      "There are 1024 monitored basins with baseline PMFs.\n"
     ]
    }
   ],
   "source": [
    "# retrieve LSTM ensemble predictions\n",
    "lstm_result_folder = '/home/danbot/code/neuralhydrology/data/ensemble_results_20250514'\n",
    "lstm_result_files = os.listdir(lstm_result_folder)\n",
    "lstm_result_stns = [e.split('_')[0] for e in lstm_result_files]\n",
    "assert '12414900' in lstm_result_stns\n",
    "\n",
    "# find any non-matching station ids in the lstm result files\n",
    "for stn in lstm_result_stns:\n",
    "    if stn not in station_ids:\n",
    "        # try adding a leading zero\n",
    "        ending_in = [e for e in station_ids if e.endswith(stn)]\n",
    "        if len(ending_in) > 0:\n",
    "            print(stn, 'matches', ending_in)\n",
    "        modified_stn = stn.zfill(8)\n",
    "        if modified_stn in station_ids:\n",
    "            print(f'Found modified station id: {modified_stn} for {stn}')\n",
    "        else:\n",
    "            print(f'Warning: {stn} is in LSTM results but not in the station attributes.')\n",
    "\n",
    "# filter for the common stations between BCUB region and LSTM-compatible (i.e. 1980-)\n",
    "daymet_concurrent_stations = list(set(station_ids) & set(lstm_result_stns) & set(pmf_stations))\n",
    "# assert '12414900' in daymet_concurrent_stations\n",
    "print(f'There are {len(daymet_concurrent_stations)} monitored basins concurrent with LSTM ensemble results.')\n",
    "print(f'There are {len(pmf_stations)} monitored basins with baseline PMFs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4782d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import updated catchment polygons\n",
    "# poly_fpath = os.path.join(os.path.join('data', f'BCUB_watershed_attributes_updated_{rev_date}.csv'))\n",
    "# catchment_gdf = pd.read_csv(poly_fpath)\n",
    "# catchment\n",
    "# catchment_gdf = catchment_gdf[catchment_gdf['Official_ID'].isin(station_ids)]\n",
    "# print(len(catchment_gdf), 'catchments in the polygon set')\n",
    "\n",
    "# import the license water extraction points\n",
    "# dam_gdf = gpd.read_file('data/Dam_Points_20240103.gpkg')\n",
    "# assert dam_gdf.crs == catchment_gdf.crs, \"Catchment and dam geometries must have the same CRS\"\n",
    "# joined = gpd.sjoin(catchment_gdf, dam_gdf, how=\"inner\", predicate=\"contains\")\n",
    "# Create a new boolean column 'contains_dam' in catchment_gdf.\n",
    "# If a polygon's index appears in the joined result, it means it contains at least one point.\n",
    "# regulated = joined['Official_ID'].values\n",
    "# catchment_gdf[\"contains_dam\"] = catchment_gdf['Official_ID'].apply(lambda x: x in regulated)\n",
    "# n_regulated = catchment_gdf['contains_dam'].sum()\n",
    "# print(f'{n_regulated}/{len(catchment_gdf)} catchments contain withdrawal licenses')\n",
    "\n",
    "# # create dicts for easier access to 'official_id': 'drainage area', geometry, regulation status\n",
    "# da_dict = attr_df[['official_id', 'drainage_area_km2']].copy().set_index('official_id').to_dict()['drainage_area_km2']\n",
    "# dam_dict = catchment_gdf[['Official_ID', 'contains_dam']].copy().set_index('Official_ID').to_dict()['contains_dam']\n",
    "# polygon_dict = catchment_gdf[['Official_ID', 'geometry']].copy().set_index('Official_ID').to_dict()['geometry']\n",
    "\n",
    "# # add the centroid point geometry to the attributes dataframe\n",
    "# attr_df = attr_df[attr_df['official_id'].isin(catchment_gdf['Official_ID'].values)].copy()\n",
    "# centroids = attr_df.apply(lambda x: polygon_dict[x['official_id']].centroid, axis=1)\n",
    "# attr_gdf = gpd.GeoDataFrame(attr_df, geometry=centroids, crs=catchment_gdf.crs)\n",
    "# attr_gdf[\"contains_dam\"] = attr_gdf['official_id'].apply(lambda x: dam_dict[x] if x in dam_dict else False)\n",
    "# add the concurrency status as a boolean column\n",
    "# attr_df['LSTM_concurrent'] = attr_df['official_id'].apply(lambda x: x in daymet_concurrent_stations)\n",
    "# attr_df.reset_index(inplace=True, drop=True)\n",
    "# print(f'N network stations={len(attr_df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6f0e6e-e089-4951-a047-e0b49b6410c1",
   "metadata": {},
   "source": [
    "## Non-Parametric Simulation\n",
    "\n",
    "### Time-based ensemble\n",
    "\n",
    "A probability distribution $\\hat p = f(\\tilde x(t))$ is estimated for a target (ungauged location) by a weighted mean of runoff time-series from k nearest neighbour stations, $\\tilde x(t) = \\textbf{X}(t)\\cdot w$ where $X(t) \\in \\mathbb{R}^{N \\times k}$ and $w \\in \\mathbb{R}^{k\\times 1}$ is a vector of k weights.  So $\\hat p = f(\\textbf{X}(t) \\cdot w )$  Weights $w$ are computed in three ways, described in the next subsection, and k-nearest neighbours are selected using the criteria defined below.  Each gauged station in the monitoring network is treated as an ungauged location to generate a large sample of simulations across hydrologically diverse catchments, or rather as many catchments as can be tested.\n",
    "\n",
    "### Frequency-based ensembles\n",
    "\n",
    "A simulated probability density function is estimated from observations of k nearest neighbour stations.  First, k simulated series are generated by equal unit area runoff , $\\hat p = \\hat P \\cdot w$ where $\\hat P = [\\hat p_1, \\hat p_2, \\cdots, \\hat p_k]$ and each $\\hat p_i = f(X_i(t))$.\n",
    "\n",
    "In both cases, the function $f \\rightarrow \\hat p(x)$ represents kernel density estimation, which defines the probability density as $$\\hat p(x) = \\frac{1}{n \\cdot h(x)} \\sum_{i=1}^{n}K\\left( \\frac{x-x_i}{h(x)}\\right)$$ \n",
    "\n",
    "Where $h(x)$ reflects an adaptive kernel bandwidth that addresses vestiges of precision in the observed data to reflect the nature of streamflow as a continuous variable, and additionally incorporates piecewise linear model to represent overall measurement uncertainty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7d4695",
   "metadata": {},
   "source": [
    "## Notes on k-nearest neighbours\n",
    "\n",
    "Time series streamflow records vary widely in their temporal coverage, and finding k-nearest neighbours presents a tradeoff between selecting nearest neighbours and maximizing the number of observations concurrent with the target.  From the literature, concurrency is assured by pre-selecting a subset of stations with continuous records over a common period of record, or by infilling gaps with k-nearest neighbours simulation.  Some kind of tradeoff must be made, and we aim to use a method that maximizes information content while minimizing the number of assumptions.  The following notes are intended to clarify the implications of using k-nearest neighbours to fill gaps in the time series.\n",
    "\n",
    "1. **Infilled-by-kNN != Independent Proxy**: If a gap in an observation record is inferred from neighbors, it becomes redundant in the ensemble and increases the weight of the other (k minus n) neighbours.  So at that time step, its influence is non-unique, and including it in the ensemble is functionally equivalent to using the same set of other proxies directly, or just reducing the ensemble size.\n",
    "\n",
    "2. **Inflated Ensemble Size**: Filling gaps by \"nested\" k-nearest neighbours inflates the expresed number of independent neighbors.  Comparing the effectiveness of ensemble simulations as a function of k is then misleading because the effective number of independent proxies is *at most* k. \n",
    "\n",
    "3. **Information leakage risk**: If you repeatedly use kNN to fill missing data from within the same pool, especially when simulating extreme values, you risk suppressing variability by biasing toward the central tendency of the ensemble.  This defeats one of the core motivations for kNN: to preserve structure and variability from observations at neighboring stations.\n",
    "\n",
    "To address the nuance above, we propose three time-based methods for selecting k-nearest neighbours beyond strictly nodes in the network.  The problem is related to the set-cover problem where the goal is to select a subset of stations that maximizes the intersection of their data availability over a specified time period.  The following sections outline the three methods for selecting k-nearest neighbours based on availability of concurrent data.\n",
    "\n",
    "### Summary: Set-Theoretic Foundations of Strict k-NN Concurrency Selection\n",
    "\n",
    "This problem is closely related to classic combinatorial and set-theoretic optimization problems.\n",
    "\n",
    "#### Set-Theoretic Definition\n",
    "\n",
    "Let each column $( S_i \\subseteq T )$ represent the set of timestamps where station $( i )$ has valid (non-NaN) data.  \n",
    "Let $( \\mathcal{S} = \\{ S_1, S_2, \\dots, S_n \\} )$ be the collection of all such subsets, sorted by proximity (e.g., distance or attribute similarity).  \n",
    "The goal is to select a subset $( \\mathcal{K} \\subset \\mathcal{S} )$ such that:\n",
    "- $( |\\mathcal{K}| = k )$\n",
    "- $( \\bigcap_{S \\in \\mathcal{K}} S )$ satisfies a temporal completeness constraint (e.g., ≥5 years with ≥10 observations in each of 12 months)\n",
    "\n",
    "This is a constrained subset selection problem on the intersection of sets.\n",
    "\n",
    "#### Related Concepts\n",
    "\n",
    "| Concept                                 | Description |\n",
    "|----------------------------------------|-------------|\n",
    "| Set Intersection Selection             | Select \\( k \\) sets whose intersection satisfies a completeness constraint. |\n",
    "| Maximum Coverage under Cardinality Constraint | Choose \\( k \\) sets to maximize the coverage (or completeness) of their intersection. |\n",
    "| Recursive k-Subset Validation          | If the initial \\( k \\) sets fail, iteratively add more candidates and evaluate all \\( \\binom{k+1}{k} \\) combinations, and so on. |\n",
    "| NP-Hard Nature                         | This problem is computationally hard and shares structure with the Set Cover and Maximum Coverage problems. |\n",
    "\n",
    "#### Practical Implication\n",
    "\n",
    "This formulation justifies using greedy or approximate subset selection strategies when exhaustively testing all combinations becomes computationally infeasible.\n",
    "## Define a universal parametric prior\n",
    "\n",
    "In order to fairly test how parametric and non-parametric pdf estimation methods compare to each other, we need a consistent way to deal with indeterminate cases where the simulated distribution does not provide support coverage of the \"ground truth\" observations.  I feel two ways about this: the KL divergence is the culprit here, and the problem could be avoided by choosing another divergence measure.  However the definintion of KL divergence in information theoretic terms of compression make it seem more foundational than other measures, but ultimately is this true?  Should we look to math statistics to make more direct links between f-divergences and what we use as a discriminant for a particular application?  Should we be more concerned about \"Bayesian consistency\" of the discriminant (or surrogate loss function) with the choice of divergence measure?\n",
    "\n",
    "\n",
    "1.  **Quantify the distribution of unsupported mass across all models**.  It is important to describe the extent of the problem across the sample **and** across various methods.  i.e. discrete distributions have the issue of support coverage, but so do all methods!\n",
    "2.  Even in kNN / ensemble simulation approaches, the problem of incomplete support coverage necessitates assuming some prior probability.  The issue is that setting a uniform prior over the observed range takes advantage of information about the observed range.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008336e7-cca2-440e-99da-e3919f351793",
   "metadata": {},
   "source": [
    "### Global Uniform Prior\n",
    "\n",
    "$$f(x) = \\frac{1}{b-a}, \\quad x\\in (a, b) \\text{ and } f(x) = 0 \\text{ otherwise.}$$\n",
    "$$\\int_a^b f(x)\\text{dx} = 1$$\n",
    "\n",
    "Given the target range is a sub interval $(c, d) \\subseteq (a, b)$, then the **total** prior probability mass over (c, d) is:\n",
    "\n",
    "$$M_\\text{target} = \\int_c^d \\frac{1}{b-a}\\text{dx} = \\frac{d-c}{b-a}$$\n",
    "\n",
    "Over the set of intervals $\\Delta x_i$ covering the **target range**, the probability mass associated with each interval (bin) is given by: \n",
    "\n",
    "$$\\Delta x_i \\frac{d-c}{b-a}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2c621f-81ff-4b85-a15f-f3b9565ebf90",
   "metadata": {},
   "source": [
    "A desirable property of the prior is that it reflects the strength of belief in the model (data), where a smaller prior reflects stronger belief in the data/model and vice versa.  Dividing by the number of observations has such an effect, however it also makes for very small priors.  The consequence of very small priors is they have negligible effect on models that provide complete support coverage, and they severely penalize models that do not, resulting in a form of instability.  The very small prior creates a heavy tail in the distribution of a large sample of KL divergences, with further downstream effects in optimization.  \n",
    "\n",
    "A method that uses a prior with negligible effect on a model with complete support coverage and a very big effect on one without can be interpreted in a few ways:  \n",
    "\n",
    "1.  Incomplete support coverage, or underspecification, is very heavily penalized.  The method does not tolerate a model that cannot predict the full observed range.\n",
    "2.  A **proper** probability distribution sums (discrete) or integrates (continuous) to 1.  Very small probabilities are in a sense associated with a high degree of certainty since they reflect the expectation of the system being observed in a particular state.\n",
    "3.  The penalty of underestimating a state frequency is that storing and transmitting information about the state requires (the log ratio) more bandwidth/disk space because it is assigned a longer bit string than the actual frequency calls for under optimal encoding.\n",
    "4.  Assigning a very small probability to a state ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "71a48368-9dda-4442-a09c-c506db94ba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the predicted parameter results\n",
    "parameter_prediction_results_folder = os.path.join('data', 'results', 'parameter_prediction_results', )\n",
    "predicted_params_fpath   = os.path.join(parameter_prediction_results_folder, 'mean_parameter_predictions.csv')\n",
    "rdf = pd.read_csv(predicted_params_fpath, index_col=['official_id'], dtype={'official_id': str})\n",
    "predicted_param_dict = rdf.to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "646ea56f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"eed23fd4-cbb8-4230-9d22-f4992d621ae2\" data-root-id=\"p1530\" style=\"display: contents;\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function embed_document(root) {\n  const docs_json = {\"3eb5289a-b699-4982-a464-ccb0c389e905\":{\"version\":\"3.7.3\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"GridPlot\",\"id\":\"p1530\",\"attributes\":{\"rows\":null,\"cols\":null,\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1529\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"ToolProxy\",\"id\":\"p1523\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1453\"},{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1499\"}]}},{\"type\":\"object\",\"name\":\"ToolProxy\",\"id\":\"p1524\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1454\",\"attributes\":{\"renderers\":\"auto\"}},{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1500\",\"attributes\":{\"renderers\":\"auto\"}}]}},{\"type\":\"object\",\"name\":\"ToolProxy\",\"id\":\"p1525\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1455\",\"attributes\":{\"dimensions\":\"both\",\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1456\",\"attributes\":{\"syncable\":false,\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5,\"level\":\"overlay\",\"visible\":false,\"left\":{\"type\":\"number\",\"value\":\"nan\"},\"right\":{\"type\":\"number\",\"value\":\"nan\"},\"top\":{\"type\":\"number\",\"value\":\"nan\"},\"bottom\":{\"type\":\"number\",\"value\":\"nan\"},\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"handles\":{\"type\":\"object\",\"name\":\"BoxInteractionHandles\",\"id\":\"p1462\",\"attributes\":{\"all\":{\"type\":\"object\",\"name\":\"AreaVisuals\",\"id\":\"p1461\",\"attributes\":{\"fill_color\":\"white\",\"hover_fill_color\":\"lightgray\"}}}}}}}},{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1501\",\"attributes\":{\"dimensions\":\"both\",\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1502\",\"attributes\":{\"syncable\":false,\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5,\"level\":\"overlay\",\"visible\":false,\"left\":{\"type\":\"number\",\"value\":\"nan\"},\"right\":{\"type\":\"number\",\"value\":\"nan\"},\"top\":{\"type\":\"number\",\"value\":\"nan\"},\"bottom\":{\"type\":\"number\",\"value\":\"nan\"},\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"handles\":{\"type\":\"object\",\"name\":\"BoxInteractionHandles\",\"id\":\"p1508\",\"attributes\":{\"all\":{\"type\":\"object\",\"name\":\"AreaVisuals\",\"id\":\"p1507\",\"attributes\":{\"fill_color\":\"white\",\"hover_fill_color\":\"lightgray\"}}}}}}}}]}},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1526\"},{\"type\":\"object\",\"name\":\"ToolProxy\",\"id\":\"p1527\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1464\"},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1510\"}]}},{\"type\":\"object\",\"name\":\"ToolProxy\",\"id\":\"p1528\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"HelpTool\",\"id\":\"p1465\"},{\"type\":\"object\",\"name\":\"HelpTool\",\"id\":\"p1511\"}]}}]}},\"children\":[[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1431\",\"attributes\":{\"width\":400,\"height\":400,\"x_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1432\"},\"y_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1433\"},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1441\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1442\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1434\",\"attributes\":{\"text\":\"Predicted mean_logx_mean_predicted\"}},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1472\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1466\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1467\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1468\"},\"data\":{\"type\":\"map\",\"entries\":[[\"top\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"yc0YMJDygT+uNCVI2OuKPwAAAAAAAAAArjQlSNjrij/JzRgwkPKBP8PNGDCQ8oE/OwEfPDRvlj/JzRgwkPKhP2dOKE4qKq0/H2grVHxorz+fWhozuRGzPzQBHzw0b7Y/yc0YMJDykT/0GiJChq2oP4vBJksBC7w/0acjRa/MuT/NOp66n9/FP6rHn73I/sY/X3QdOQtQxT8b+6XJbHvLP8+92nODWdM/yc0YMJDy0T8Enl77aSfWPysBHzw0b9Y/SwEfPDRv1j8J+6XJbHvbP8HdVuyci9A/5IenzJWazD+Kepar0kPQP706nrqf38U/6xSpz765zT8sbqTGQ1zKPyqUmbEkgtI/OJSZsSSC0j8J+6XJbHvLP67nGzbiMLQ/vM0YMJDyoT+bNCVI2OuKP7zNGDCQ8nE/1s0YMJDycT8=\"},\"shape\":[40],\"dtype\":\"float64\",\"order\":\"little\"}],[\"left\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"vtMJDWk5/r9Yz2KMvG77v/LKuwsQpPi/i8YUi2PZ9b8lwm0Ktw7zv7+9xokKRPC/sHI/Erzy6r/kafEQY13lvzDCRh8UkN+/mLCqHGJl1L8APh00YHXCv8Ap14geAJ8/cAhT1mc1yj/QlcXtZUXYP7TTMPgLuOE/gNx++WRN5z9M5cz6veLsPwx3DX4LPPE/cnu0/rcG9D/Yf1t/ZNH2Pz6EAgARnPk/poipgL1m/D8MjVABajH/P7nI+0AL/gBA7UpPgWFjAkAfzaLBt8gDQFNP9gEOLgVAhdFJQmSTBkC5U52CuvgHQOvV8MIQXglAH1hEA2fDCkBR2pdDvSgMQIVc64MTjg1Aud4+xGnzDkB2MEkCYCwQQJDxciIL3xBAqLKcQraREUDCc8ZiYUQSQNw08IIM9xJA9vUZo7epE0A=\"},\"shape\":[40],\"dtype\":\"float64\",\"order\":\"little\"}],[\"right\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"WM9ijLxu+7/yyrsLEKT4v4vGFItj2fW/JcJtCrcO87+/vcaJCkTwv7ByPxK88uq/5GnxEGNd5b8wwkYfFJDfv5iwqhxiZdS/AD4dNGB1wr/AKdeIHgCfP3AIU9ZnNco/0JXF7WVF2D+00zD4C7jhP4DcfvlkTec/TOXM+r3i7D8Mdw1+CzzxP3J7tP63BvQ/2H9bf2TR9j8+hAIAEZz5P6aIqYC9Zvw/DI1QAWox/z+5yPtAC/4AQO1KT4FhYwJAH82iwbfIA0BTT/YBDi4FQIXRSUJkkwZAuVOdgrr4B0Dr1fDCEF4JQB9YRANnwwpAUdqXQ70oDECFXOuDE44NQLnePsRp8w5AdjBJAmAsEECQ8XIiC98QQKiynEK2kRFAwnPGYmFEEkDcNPCCDPcSQPb1GaO3qRNAD7dDw2JcFEA=\"},\"shape\":[40],\"dtype\":\"float64\",\"order\":\"little\"}]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1473\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1474\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Quad\",\"id\":\"p1469\",\"attributes\":{\"left\":{\"type\":\"field\",\"field\":\"left\"},\"right\":{\"type\":\"field\",\"field\":\"right\"},\"bottom\":{\"type\":\"value\",\"value\":0},\"top\":{\"type\":\"field\",\"field\":\"top\"},\"fill_color\":{\"type\":\"value\",\"value\":\"lightblue\"}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Quad\",\"id\":\"p1470\",\"attributes\":{\"left\":{\"type\":\"field\",\"field\":\"left\"},\"right\":{\"type\":\"field\",\"field\":\"right\"},\"bottom\":{\"type\":\"value\",\"value\":0},\"top\":{\"type\":\"field\",\"field\":\"top\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_color\":{\"type\":\"value\",\"value\":\"lightblue\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Quad\",\"id\":\"p1471\",\"attributes\":{\"left\":{\"type\":\"field\",\"field\":\"left\"},\"right\":{\"type\":\"field\",\"field\":\"right\"},\"bottom\":{\"type\":\"value\",\"value\":0},\"top\":{\"type\":\"field\",\"field\":\"top\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"value\",\"value\":\"lightblue\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1440\",\"attributes\":{\"tools\":[{\"id\":\"p1453\"},{\"id\":\"p1454\"},{\"id\":\"p1455\"},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1463\"},{\"id\":\"p1464\"},{\"id\":\"p1465\"}]}},\"toolbar_location\":null,\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1448\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1449\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1450\"},\"axis_label\":\"$$P(x)$$\",\"axis_label_text_font\":\"Bitstream Charter\",\"axis_label_text_font_size\":\"14pt\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1451\"},\"major_label_text_font\":\"Bitstream Charter\",\"major_label_text_font_size\":\"12pt\"}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1443\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1444\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1445\"},\"axis_label\":\"$$\\\\text{Log Mean UAR }(L/s/\\\\text{km}^2)$$\",\"axis_label_text_font\":\"Bitstream Charter\",\"axis_label_text_font_size\":\"14pt\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1446\"},\"major_label_text_font\":\"Bitstream Charter\",\"major_label_text_font_size\":\"12pt\"}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1447\",\"attributes\":{\"axis\":{\"id\":\"p1443\"}}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1452\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1448\"}}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1475\",\"attributes\":{\"label_text_font\":\"Bitstream Charter\",\"label_text_font_size\":\"12pt\",\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1476\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"\"},\"renderers\":[{\"id\":\"p1472\"}]}}]}}]}},0,0],[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1477\",\"attributes\":{\"width\":400,\"height\":400,\"x_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1478\"},\"y_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1479\"},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1487\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1488\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1480\",\"attributes\":{\"text\":\"Predicted sd_logx_mean_predicted\"}},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1518\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1512\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1513\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1514\"},\"data\":{\"type\":\"map\",\"entries\":[[\"top\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"bLJV6wmkyz/1jXNFI/PaP5Yzjx2UYuQ/0UWAcAe75D8wjaLOccnvP1f7GTfXBf0/UhSGZQ8nAEC78TjSxET/P/jp+VoV1/c/nmjAKIsY7z/jRYBwB7vkP+HWN5HwVNw/gGmRnzxC2j/l/LskOlnTP6jXCAiifsc/lCDNU2/gyD+UIM1Tb+C4P0NE3oKkZ74/vY5EvNQctj+EslXrCaS7P6mORLzUHJY//mozjZ+VsD8cazONn5WgP9CORLzUHIY/qY5EvNQchj8cazONn5WgPxxrM42flbA//mozjZ+VoD/+ajONn5WgPwAAAAAAAAAAAAAAAAAAAADQjkS81ByWP9CORLzUHJY/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANCORLzUHIY/qY5EvNQchj8=\"},\"shape\":[40],\"dtype\":\"float64\",\"order\":\"little\"}],[\"left\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"NJC635m34z9ibTPlqPvlP5BKrOq3P+g/vicl8MaD6j/sBJ711cfsPxriFvvkC+8/pN9HAPqn8D86TgSDAcrxP9K8wAUJ7PI/aCt9iBAO9D8AmjkLGDD1P5YI9o0fUvY/LXeyECd09z/E5W6TLpb4P1tUKxY2uPk/8sLnmD3a+j+JMaQbRfz7PyCgYJ5MHv0/tw4dIVRA/j9OfdmjW2L/P/L1SpMxQgBAPi2pVDXTAECKZAcWOWQBQNWbZdc89QFAINPDmECGAkBsCiJaRBcDQLdBgBtIqANAAnne3Es5BEBOsDyeT8oEQJrnml9TWwVA5R75IFfsBUAxVlfiWn0GQHyNtaNeDgdAx8QTZWKfB0AT/HEmZjAIQF4z0OdpwQhAqmouqW1SCUD1oYxqceMJQEHZ6it1dApAjBBJ7XgFC0A=\"},\"shape\":[40],\"dtype\":\"float64\",\"order\":\"little\"}],[\"right\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"Ym0z5aj75T+QSqzqtz/oP74nJfDGg+o/7ASe9dXH7D8a4hb75AvvP6TfRwD6p/A/Ok4EgwHK8T/SvMAFCezyP2grfYgQDvQ/AJo5Cxgw9T+WCPaNH1L2Py13shAndPc/xOVuky6W+D9bVCsWNrj5P/LC55g92vo/iTGkG0X8+z8goGCeTB79P7cOHSFUQP4/Tn3Zo1ti/z/y9UqTMUIAQD4tqVQ10wBAimQHFjlkAUDVm2XXPPUBQCDTw5hAhgJAbAoiWkQXA0C3QYAbSKgDQAJ53txLOQRATrA8nk/KBECa55pfU1sFQOUe+SBX7AVAMVZX4lp9BkB8jbWjXg4HQMfEE2VinwdAE/xxJmYwCEBeM9DnacEIQKpqLqltUglA9aGManHjCUBB2eordXQKQIwQSe14BQtA2EenrnyWC0A=\"},\"shape\":[40],\"dtype\":\"float64\",\"order\":\"little\"}]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1519\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1520\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Quad\",\"id\":\"p1515\",\"attributes\":{\"left\":{\"type\":\"field\",\"field\":\"left\"},\"right\":{\"type\":\"field\",\"field\":\"right\"},\"bottom\":{\"type\":\"value\",\"value\":0},\"top\":{\"type\":\"field\",\"field\":\"top\"},\"fill_color\":{\"type\":\"value\",\"value\":\"lightblue\"}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Quad\",\"id\":\"p1516\",\"attributes\":{\"left\":{\"type\":\"field\",\"field\":\"left\"},\"right\":{\"type\":\"field\",\"field\":\"right\"},\"bottom\":{\"type\":\"value\",\"value\":0},\"top\":{\"type\":\"field\",\"field\":\"top\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_color\":{\"type\":\"value\",\"value\":\"lightblue\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Quad\",\"id\":\"p1517\",\"attributes\":{\"left\":{\"type\":\"field\",\"field\":\"left\"},\"right\":{\"type\":\"field\",\"field\":\"right\"},\"bottom\":{\"type\":\"value\",\"value\":0},\"top\":{\"type\":\"field\",\"field\":\"top\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"value\",\"value\":\"lightblue\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1486\",\"attributes\":{\"tools\":[{\"id\":\"p1499\"},{\"id\":\"p1500\"},{\"id\":\"p1501\"},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1509\"},{\"id\":\"p1510\"},{\"id\":\"p1511\"}]}},\"toolbar_location\":null,\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1494\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1495\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1496\"},\"axis_label\":\"$$P(x)$$\",\"axis_label_text_font\":\"Bitstream Charter\",\"axis_label_text_font_size\":\"14pt\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1497\"},\"major_label_text_font\":\"Bitstream Charter\",\"major_label_text_font_size\":\"12pt\"}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1489\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1490\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1491\"},\"axis_label\":\"$$\\\\text{Log SD UAR }(L/s/\\\\text{km}^2)$$\",\"axis_label_text_font\":\"Bitstream Charter\",\"axis_label_text_font_size\":\"14pt\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1492\"},\"major_label_text_font\":\"Bitstream Charter\",\"major_label_text_font_size\":\"12pt\"}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1493\",\"attributes\":{\"axis\":{\"id\":\"p1489\"}}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1498\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1494\"}}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1521\",\"attributes\":{\"label_text_font\":\"Bitstream Charter\",\"label_text_font_size\":\"12pt\",\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1522\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"\"},\"renderers\":[{\"id\":\"p1518\"}]}}]}}]}},0,1]]}}]}};\n  const render_items = [{\"docid\":\"3eb5289a-b699-4982-a464-ccb0c389e905\",\"roots\":{\"p1530\":\"eed23fd4-cbb8-4230-9d22-f4992d621ae2\"},\"root_ids\":[\"p1530\"]}];\n  void root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n  }\n  if (root.Bokeh !== undefined) {\n    embed_document(root);\n  } else {\n    let attempts = 0;\n    const timer = setInterval(function(root) {\n      if (root.Bokeh !== undefined) {\n        clearInterval(timer);\n        embed_document(root);\n      } else {\n        attempts++;\n        if (attempts > 100) {\n          clearInterval(timer);\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n        }\n      }\n    }, 10, root)\n  }\n})(window);",
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "p1530"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plots = []\n",
    "predicted_param_sample = {}\n",
    "for l, al in zip(['mean_logx_mean_predicted', 'sd_logx_mean_predicted'], [r'$$\\text{Log Mean UAR }(L/s/\\text{km}^2)$$', r'$$\\text{Log SD UAR }(L/s/\\text{km}^2)$$']):\n",
    "    vals = [d[l] for _, d in predicted_param_dict.items()]\n",
    "    predicted_param_sample[l] = vals\n",
    "    # plot the histogram of the mean_uar values\n",
    "    hist, edges = np.histogram(vals, bins=40, density=True)\n",
    "    # create a scatter plot of the predicted parameter vs the target parameter\n",
    "    f = figure(title=f'Predicted {l}', width=600, height=400)\n",
    "    f.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:], fill_color='lightblue', line_color='black', legend_label='')\n",
    "    f.xaxis.axis_label = al\n",
    "    f.yaxis.axis_label = r'$$P(x)$$'\n",
    "    f = dpf.format_fig_fonts(f, font_size=14)\n",
    "    plots.append(f)\n",
    "# retrieve all the mean_uar values \n",
    "\n",
    "lt = gridplot(plots, ncols=2, width=400, height=400)\n",
    "show(lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "46b4bbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FDCEstimatorRunner:\n",
    "    def __init__(self, stn_id, ctx, methods, k_nearest, parametric_target_cols, estimator_classes, **kwargs):\n",
    "        self.stn_id = stn_id\n",
    "        self.ctx = ctx\n",
    "        self.methods = methods\n",
    "        self.k_nearest = k_nearest\n",
    "        self.parametric_target_cols = parametric_target_cols\n",
    "        # self._check_min_overlap()\n",
    "        self._create_results_folders()\n",
    "        self._create_readme()\n",
    "        self.ESTIMATOR_CLASSES = estimator_classes\n",
    "\n",
    "\n",
    "    def _create_results_folders(self):\n",
    "        # create a results foder for each method if it doesn't exist\n",
    "        self.results_folder = os.path.join('data', 'results', f'fdc_estimation_results',)\n",
    "        for method in self.methods:\n",
    "            method_folder = os.path.join(self.results_folder, method)\n",
    "            if not os.path.exists(method_folder):\n",
    "                os.makedirs(method_folder)\n",
    "\n",
    "    \n",
    "    def _create_readme(self):\n",
    "        # create a readme file in the results folder to list constraints\n",
    "        readme_file = os.path.join(self.results_folder, 'README.txt')\n",
    "        \n",
    "        with open(readme_file, 'w') as file:\n",
    "            file.write(\"This folder contains the results of the FDC estimation.\\n\")\n",
    "            file.write(f\"Methods evaluated: {', '.join(self.methods)}\\n\")\n",
    "            # add the concurrency constraint and number of stations represented in the network\n",
    "            N = len(self.ctx.official_ids)\n",
    "            if self.ctx.LSTM_concurrent_network == True:\n",
    "                file.write(f'Uses only stations within Daymet input period of record / LSTM results: N={N} stations in the network.\\n')\n",
    "                file.write(f'Global start date on streamflow data: {self.ctx.global_start_date}\\n')\n",
    "            else:\n",
    "                file.write(f'Uses all available network stations in the BCUB region (1950-2024): N={N} stationsin the network.')\n",
    "                \n",
    "\n",
    "    # def _process_ground_truth(self):\n",
    "        # self.kde = KDEEstimator(self.data.baseline_log_grid, self.data.log_dx)\n",
    "        # self.baseline_pmf, self.baseline_pdf = self.kde.compute(\n",
    "        #     self.data.stn_df[self.data.uar_label].values, self.data.target_da\n",
    "        # )\n",
    "        # self.ctx.baseline_pmf = self.baseline_pmf\n",
    "    def _load_reference_distributions(self):\n",
    "        self.kde = KDEEstimator(self.data.baseline_log_grid, self.data.log_dx)\n",
    "        self.baseline_pmf, self.baseline_pdf = self.data.baseline_pmf, self.data.baseline_pdf\n",
    "        self.ctx.baseline_pmf = self.baseline_pmf\n",
    "\n",
    "\n",
    "    def _save_result(self, result):\n",
    "        with open(self.result_file, 'w') as file:\n",
    "            json.dump(result, file, indent=4)\n",
    "\n",
    " \n",
    "    def run_selected(self):\n",
    "        # check the minimum number of years of overlap for all stations in self.ctx.overlap_dict\n",
    "        \n",
    "        for method in self.methods:\n",
    "            self.result_file = os.path.join(self.results_folder, method, f'{self.stn_id}_fdc_results.json')\n",
    "            if os.path.exists(self.result_file):\n",
    "                continue\n",
    "            else:\n",
    "                self.data = StationData(self.ctx, self.stn_id)\n",
    "                self.data.k_nearest = self.k_nearest\n",
    "                self.data.parametric_target_cols = self.parametric_target_cols\n",
    "                self._load_reference_distributions()\n",
    "            try:\n",
    "                EstimatorClass = self.ESTIMATOR_CLASSES[method]\n",
    "                estimator = EstimatorClass(\n",
    "                    self.ctx, self.stn_id, self.data\n",
    "                )\n",
    "                result = estimator.run_estimators(\n",
    "                    divergence_measures=self.ctx.divergence_measures, \n",
    "                    eps=self.ctx.eps,\n",
    "                    baseline_pmf=self.baseline_pmf,\n",
    "                    )\n",
    "                self._save_result(result)\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"  {method} estimator failed for {self.stn_id}: {str(e)}\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5b680881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParametricFDCEstimator:\n",
    "    def __init__(self, ctx, target_stn, data, *args, **kwargs):\n",
    "        # super().__init__(*args, **kwargs)\n",
    "        self.ctx = ctx\n",
    "        self.target_stn = target_stn\n",
    "        self.data = data\n",
    "        # self.data = data\n",
    "        self.predicted_param_dict = self.ctx.predicted_param_dict\n",
    "        self.predicted_param_df = pd.DataFrame(self.predicted_param_dict).T\n",
    "\n",
    "\n",
    "    def _compute_lognorm_pmf(self, mu, sigma, eps=1e-12):\n",
    "        pdf = norm.pdf(self.data.baseline_log_grid, loc=mu, scale=sigma)\n",
    "        pdf /= jnp.trapezoid(pdf, x=self.data.baseline_log_grid)\n",
    "        raw_pmf = pdf * self.data.log_dx\n",
    "        pmf = raw_pmf + eps\n",
    "        pmf /= pmf.sum()\n",
    "        return raw_pmf, pmf, pdf\n",
    "    \n",
    "\n",
    "    def _compute_GEV_pmf(self, xi, mu, sigma, eps=1e-12):\n",
    "        # assert values are within the valid range for GEV\n",
    "        xi = max(xi, -0.5 + eps)  # clip xi to avoid numerical issues\n",
    "        sigma = max(sigma, eps)  # ensure sigma is positive\n",
    "        pdf = genextreme.pdf(self.data.baseline_log_grid, xi, loc=mu, scale=sigma)\n",
    "        pdf /= jnp.trapezoid(pdf, x=self.data.baseline_log_grid)\n",
    "        raw_pmf = pdf * self.data.log_dx\n",
    "        raw_pmf /= raw_pmf.sum()  # normalize raw PMF\n",
    "        pmf = raw_pmf + eps\n",
    "        pmf /= pmf.sum()\n",
    "        return raw_pmf, pmf, pdf\n",
    "\n",
    "\n",
    "    def _estimate_from_mle(self, eps=1e-12):\n",
    "        mu, sigma = self.predicted_param_dict[self.target_stn]['mean_logx_actual'], self.predicted_param_dict[self.target_stn]['sd_logx_actual']\n",
    "        return self._compute_lognorm_pmf(mu, sigma, eps=eps)\n",
    "    \n",
    "\n",
    "    # def _estimate_from_observed_lmoments_gev(self, eps=1e-12):\n",
    "    #     # compute the GEV parameters from the L-moments\n",
    "    #     xi = self.data.LN_param_dict['logx_lmom_xi'][self.target_stn]['actual']\n",
    "    #     loc = self.data.LN_param_dict['logx_lmom_loc'][self.target_stn]['actual']\n",
    "    #     scale = self.data.LN_param_dict['logx_lmom_scale'][self.target_stn]['actual']\n",
    "    #     return self._compute_GEV_pmf(xi, loc, scale, eps=eps)\n",
    "    \n",
    "\n",
    "    def _estimate_from_predicted_log_params(self, eps=1e-12):\n",
    "        mu, sigma = self.predicted_param_dict[self.target_stn]['mean_logx_mean_predicted'], self.predicted_param_dict[self.target_stn]['sd_logx_mean_predicted']\n",
    "        return self._compute_lognorm_pmf(mu, sigma, eps=eps)\n",
    "        \n",
    "    \n",
    "    def _estimate_from_predicted_linear_mom(self, eps=1e-12):\n",
    "        mean_x = self.predicted_param_dict[self.target_stn]['mean_uar_mean_predicted']\n",
    "        sd_x = self.predicted_param_dict[self.target_stn]['sd_uar_mean_predicted']\n",
    "        v = np.log(1 + (sd_x / mean_x) ** 2)\n",
    "        mu = np.log(mean_x) - 0.5 * v\n",
    "        # compute the lognormal pmf\n",
    "        return self._compute_lognorm_pmf(mu, np.sqrt(v), eps=eps)\n",
    "    \n",
    "\n",
    "    def _estimate_LN_from_randomly_drawn_params(self, eps=1e-12):\n",
    "        # randomly draw from the predicted parameters\n",
    "        random_idx = np.random.choice(len(self.predicted_param_df))\n",
    "        random_stn_idx = self.predicted_param_df.index[random_idx]\n",
    "        mu_random =self.predicted_param_dict[random_stn_idx]['mean_logx_mean_predicted']\n",
    "        sigma_random = self.predicted_param_dict[random_stn_idx]['sd_logx_mean_predicted']\n",
    "        return self._compute_lognorm_pmf(mu_random, sigma_random, eps=eps)\n",
    "\n",
    "    # def _estimate_LMOM_gev_from_randomly_drawn_params(self, eps=1e-12):\n",
    "    #     # randomly draw from the predicted parameters\n",
    "    #     random_idx = np.random.choice(len(self.predicted_parameter_sample['mean_uar_lmom_xi']))\n",
    "    #     xi = self.predicted_parameter_sample['mean_uar_lmom_xi'][random_idx]\n",
    "    #     loc = self.predicted_parameter_sample['mean_uar_lmom_loc'][random_idx]\n",
    "    #     scale = self.predicted_parameter_sample['mean_uar_lmom_scale'][random_idx]\n",
    "    #     xi = max(xi, -0.5 + eps)  # clip xi to avoid numerical issues\n",
    "    #     scale = max(scale, 0.01)  # ensure scale is positive\n",
    "    #     # compute the GEV PMF\n",
    "    #     return self._compute_GEV_pmf(xi, loc, scale, eps=eps)    \n",
    "\n",
    "    # def _estimate_from_predicted_lmoments_gev(self, eps=1e-12):\n",
    "    #     # compute the GEV parameters from the L-moments\n",
    "    #     xi = self.data.LN_param_dict['mean_uar_lmom_xi'][self.target_stn]['predicted']\n",
    "    #     loc = self.data.LN_param_dict['logx_lmom_loc'][self.target_stn]['predicted']\n",
    "    #     scale = self.data.LN_param_dict['logx_lmom_scale'][self.target_stn]['predicted']\n",
    "    #     return self._compute_GEV_pmf(xi, loc, scale, eps=eps)\n",
    "\n",
    "\n",
    "    def run_estimators(self, divergence_measures, eps, baseline_pmf):\n",
    "        results = {}\n",
    "        fns = [\n",
    "            self._estimate_from_mle, \n",
    "            self._estimate_from_predicted_log_params,\n",
    "            self._estimate_from_predicted_linear_mom, \n",
    "            self._estimate_LN_from_randomly_drawn_params,\n",
    "            # self._estimate_from_observed_lmoments_gev,\n",
    "            # self._estimate_from_predicted_lmoments_gev, \n",
    "            # self._estimate_LMOM_gev_from_randomly_drawn_params\n",
    "            ]\n",
    "        labels = ['MLE', 'PredictedLog', 'PredictedMOM', 'RandomDraw', \n",
    "                  #'ObsLMomentsGEV', 'PredictedLMomentsGEV', 'LMomentsGEVRandomDraw',\n",
    "                  ]\n",
    "        for fn, label in zip(fns, labels):\n",
    "            raw_pmf, pmf, pdf = fn(eps=eps)\n",
    "            if 'Moments' in label:\n",
    "                # assert no nan values in the pmf\n",
    "                assert not np.any(np.isnan(pmf)), f'PMF contains NaN values for {label}: {pmf[:100]}'\n",
    "            results[label] = {'raw_pmf': raw_pmf.tolist(), 'pmf': pmf.tolist(), 'pdf': pdf.tolist()}\n",
    "            for measure in divergence_measures:\n",
    "                d = self.data.divergence_functions[measure](baseline_pmf, pmf)\n",
    "                bias = self.data._compute_bias_from_eps(raw_pmf, measure, eps=eps)\n",
    "                if bias < 0 or bias == -0.0:\n",
    "                    assert np.isclose(bias, 0, atol=1e-6), f'sum P = {np.sum(pmf)}'\n",
    "                    bias = 0\n",
    "                bias_pct = 100 * bias / d\n",
    "                if bias_pct > 5:\n",
    "                    raise Warning(f'Warning: bias is > 5% of measure: {bias_pct:.1f}%')\n",
    "                results[label][measure] = {'value': d, 'bias': bias, 'bias_pct': bias_pct}\n",
    "                # if measure == 'DKL':\n",
    "                #     print(f'     {label} {measure} = {d:.3f} bits/sample')\n",
    "        # compute the bias from the eps\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a1e0d15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class kNNFDCEstimator:\n",
    "    def __init__(self, ctx, target_stn, data, *args, **kwargs):\n",
    "        self.ctx = ctx\n",
    "        self.target_stn = target_stn\n",
    "        self.data = data\n",
    "        self.k_nearest = data.k_nearest\n",
    "        # self.max_to_check_start = data.max_to_check\n",
    "        # self.max_to_check = data.max_to_check\n",
    "        self.weight_schemes = [1, 2] #inverse distance and inverse square distance\n",
    "        self.knn_simulation_data = {}\n",
    "        self.knn_pdfs = pd.DataFrame()\n",
    "        self.knn_pmfs = pd.DataFrame()\n",
    "\n",
    "\n",
    "    def _find_k_nearest_neighbors(self, tree_type, max_to_check):\n",
    "        # Query the k+1 nearest neighbors because the first neighbor is the target point itself\n",
    "        target_idx = self.ctx.id_to_idx[self.target_stn]\n",
    "        if tree_type == 'spatial_dist':\n",
    "            distances, indices = self.ctx.spatial_tree.query(self.ctx.coords[target_idx], k=max_to_check)\n",
    "            distances *= 1 / 1000\n",
    "        elif tree_type == 'attribute_dist':\n",
    "            # Example query: Find the nearest neighbors for the first point\n",
    "            distances, indices = self.ctx.attribute_tree.query(self.ctx.normalized_attr_values[target_idx], k=max_to_check)\n",
    "        else:\n",
    "            raise Exception('tree type not identified, must be one of spatial_dist, or attribute_dist.')\n",
    "        \n",
    "        # Remove target (self) from the results\n",
    "        self_index = target_idx\n",
    "        keep = indices != self_index\n",
    "        indices = indices[keep]\n",
    "        distances = distances[keep]\n",
    "\n",
    "        return indices, np.round(distances, 3)\n",
    "    \n",
    "\n",
    "    def _compute_effective_k(self, df, max_k=None):\n",
    "        arr = df.to_numpy()\n",
    "        T, K = arr.shape\n",
    "        max_k = max_k or K\n",
    "\n",
    "        nan_mask = np.isnan(arr)\n",
    "        sorted_idx = np.argsort(nan_mask, axis=1)\n",
    "        row_idx = np.arange(T)[:, None]\n",
    "\n",
    "        ks = np.arange(1, max_k + 1)\n",
    "        effective_k = []\n",
    "        mean_furthest = []\n",
    "\n",
    "        for k in ks:\n",
    "            idx = sorted_idx[:, :k]\n",
    "            valid = ~nan_mask[row_idx, idx]\n",
    "            valid_count = valid.sum(axis=1)\n",
    "\n",
    "            effective_k.append(valid_count.mean())\n",
    "            furthest_idx = np.where(valid, idx, -1).max(axis=1)\n",
    "            mean_furthest.append(furthest_idx[valid_count >= k].mean() if np.any(valid_count >= k) else np.nan)\n",
    "\n",
    "        return pd.DataFrame({\n",
    "            'effective_k': np.round(effective_k, 1),\n",
    "            'mean_furthest_idx': np.round(mean_furthest, 2)\n",
    "        }, index=ks)\n",
    "    \n",
    "\n",
    "    def _find_complete_years(self, df):\n",
    "        \"\"\"\n",
    "        Count the number of complete years in df where each of the 12 months\n",
    "        has at least `min_days_per_month` valid observations.\n",
    "        \"\"\"\n",
    "        if df.empty:\n",
    "            return 0\n",
    "\n",
    "        dates = df.dropna().index\n",
    "        years = np.array([d.year for d in dates])\n",
    "        months = np.array([d.month for d in dates])\n",
    "\n",
    "        # Build count array: (year, month) → count\n",
    "        ym_counts = defaultdict(int)\n",
    "\n",
    "        for y, m in zip(years, months):\n",
    "            ym_counts[(y, m)] += 1\n",
    "\n",
    "        # Group into full years\n",
    "        year_to_month_counts = defaultdict(list)\n",
    "        for (y, m), count in ym_counts.items():\n",
    "            year_to_month_counts[y].append(count)\n",
    "\n",
    "        # A year is complete if it has all 12 months with enough days\n",
    "        return [\n",
    "            int(y) for y, counts in year_to_month_counts.items()\n",
    "            if len(counts) == 12 and all(c >= self.ctx.minimum_days_per_month for c in counts)\n",
    "        ]\n",
    "\n",
    "\n",
    "    def _query_distance(self, tree, id1, id2):\n",
    "        \"\"\"Query distance between two points in a tree using official_id.\"\"\"\n",
    "        if id1 not in self.ctx.id_to_idx or id2 not in self.ctx.id_to_idx:\n",
    "            raise ValueError(f\"One or both IDs ({id1}, {id2}) not found.\")\n",
    "    \n",
    "        # Get indices from ID mapping\n",
    "        index1, index2 = self.ctx.id_to_idx[id1], self.ctx.id_to_idx[id2]\n",
    "        # Query the distance\n",
    "        distance = np.linalg.norm(tree.data[index1] - tree.data[index2])  # Euclidean distance\n",
    "        return distance\n",
    "    \n",
    "\n",
    "    def _retrieve_nearest_nbr_data(self, tree_type):\n",
    "        MAX_CHECK = 700\n",
    "        REQUIRED_GOOD = 10\n",
    "        # Get the index of the target station\n",
    "        \n",
    "        # Query once for all potential neighbors\n",
    "        nbr_idxs, dists = self._find_k_nearest_neighbors(tree_type, MAX_CHECK)\n",
    "        nbr_ids = [self.ctx.idx_to_id[i] for i in nbr_idxs if self.ctx.idx_to_id[i] != self.target_stn]\n",
    "        distances = [d for i, d in zip(nbr_idxs, dists) if self.ctx.idx_to_id[i] != self.target_stn]\n",
    "\n",
    "        good_nbrs = []\n",
    "        sorted_nbrs = sorted(zip(nbr_ids, distances), key=lambda x: x[1])\n",
    "        for (nbr_id, dist) in sorted_nbrs:\n",
    "            df = self.data.retrieve_timeseries_discharge(nbr_id)\n",
    "            if not isinstance(df, pd.DataFrame) or df.empty:\n",
    "                continue  # Skip bad or empty DataFrames\n",
    "            col = f'{nbr_id}_uar'\n",
    "            if col not in df.columns:\n",
    "                continue  # Skip if expected column is missing\n",
    "            proxy_df = df[[col]]\n",
    "            complete_years = list(self._find_complete_years(proxy_df))\n",
    "            n_years = len(complete_years)\n",
    "            good_nbrs.append((nbr_id, dist, n_years, proxy_df))\n",
    "            if len(good_nbrs) == REQUIRED_GOOD:\n",
    "                print(f\"Found {len(good_nbrs)} good neighbors for {self.target_stn} by {tree_type}.\")\n",
    "                break\n",
    "\n",
    "        # Concatenate the timeseries\n",
    "        nbr_df = pd.concat([r[3] for r in good_nbrs], axis=1)\n",
    "\n",
    "        # Build metadata DataFrame\n",
    "        complement_type = 'attribute_dist' if tree_type == 'spatial_dist' else 'spatial_dist'\n",
    "        complement_tree = getattr(self.ctx, f\"{complement_type.split('_')[0]}_tree\")\n",
    "        scale = 1 / 1000 if complement_type == 'spatial_dist' else 1\n",
    "\n",
    "        nbr_data = pd.DataFrame(\n",
    "            [r[:3] for r in good_nbrs],\n",
    "            columns=['official_id', 'distance', 'n_years']\n",
    "        )\n",
    "        nbr_data[complement_type] = nbr_data['official_id'].apply(\n",
    "            lambda x: round(scale * self._query_distance(complement_tree, x, self.target_stn), 3)\n",
    "        )\n",
    "\n",
    "        return nbr_df, nbr_data\n",
    "\n",
    "\n",
    "    def _initialize_nearest_neighbour_data(self):\n",
    "        \"\"\"\n",
    "        Generate nearest neighbours for spatial and attribute selected k-nearest neighbours for both concurrent and asynchronous records.\n",
    "        \"\"\"\n",
    "        print(f'    ...initializing nearest neighbours with minimum concurrent record.')\n",
    "        self.nbr_dfs = defaultdict(lambda: defaultdict(dict))\n",
    "        \n",
    "        for tree_type in ['spatial_dist', 'attribute_dist']:\n",
    "            nbr_df, nbr_data = self._retrieve_nearest_nbr_data(tree_type)\n",
    "            effective_k = self._compute_effective_k(nbr_df, max_k=self.k_nearest)\n",
    "            self.nbr_dfs[tree_type] = {\n",
    "                'nbr_df': nbr_df,\n",
    "                'nbr_data': nbr_data,\n",
    "                'effective_k': effective_k,\n",
    "            }\n",
    "    \n",
    "\n",
    "    def _compute_weights(self, m, k, distances, epsilon=1e-3):\n",
    "        \"\"\"Compute normalized inverse (square) distance weights to a given power.\"\"\"\n",
    "\n",
    "        distances = jnp.where(distances == 0, epsilon, distances)\n",
    "\n",
    "        if k == 1:\n",
    "            return jnp.array([1])\n",
    "        else:\n",
    "            inv_weights = 1 / (jnp.abs(distances) ** m)\n",
    "            return inv_weights / jnp.sum(inv_weights)\n",
    "    \n",
    "    \n",
    "    def _compute_prior_from_laplace_fit(self, predicted_uar, n_cols=1, min_prior=1e-10, scale_factor=1.05, recursion_depth=0, max_depth=100):\n",
    "        \"\"\"\n",
    "        Fit a Laplace distribution to the simulation and define a \n",
    "        pdf across a pre-determined \"global\" range to avoid data\n",
    "        leakage.  \"Normalize\" by setting the total prior mass to\n",
    "        integrate to a factor related to the number of observations.\n",
    "        \"\"\"\n",
    "        # assert no nan values\n",
    "        assert np.isfinite(predicted_uar).all(), f'NaN values in predicted_uar: {predicted_uar}'\n",
    "        # assert all positive values\n",
    "        # assert np.all(predicted_uar > 0), f'Negative values in predicted_uar: {np.min(predicted_uar)}'\n",
    "        # replace anything <= 0 with 1e-4 scaled by the drainage area\n",
    "        predicted_uar = np.where(predicted_uar <= 0, 1000 * 1e-4 / self.data.target_da, predicted_uar)\n",
    "        assert np.all(predicted_uar > 0), f'Negative values in predicted_uar: {np.min(predicted_uar)}'\n",
    "        # print('min/max: ', np.min(predicted_uar), np.max(predicted_uar))\n",
    "        loc, scale = laplace.fit(np.log(predicted_uar))       \n",
    "\n",
    "        # Apply scale factor in case of recursion\n",
    "        if scale <= 0:\n",
    "            original_scale = scale\n",
    "            scale = scale_factor ** recursion_depth\n",
    "            print(f'   Adjusting scale from {original_scale:.3f} to {scale:.3f} for recursion depth {recursion_depth}')\n",
    "\n",
    "        prior_pdf = laplace.pdf(self.data.baseline_log_grid, loc=loc, scale=scale)\n",
    "        prior_check = jnp.trapezoid(prior_pdf, x=self.data.baseline_log_grid)\n",
    "        prior_pdf /= prior_check\n",
    "\n",
    "        # Check for zeros\n",
    "        if np.any(prior_pdf == 0) | np.any(np.isnan(prior_pdf)):\n",
    "            # Prevent scale from being too small\n",
    "            if recursion_depth >= max_depth:\n",
    "                # set a very small prior\n",
    "                prior_pdf = np.ones_like(self.data.baseline_log_grid)\n",
    "                err_msg = f\"Recursion limit reached. Scale={scale}, setting default prior to 1 pseudo-count uniform distribution.\"\n",
    "                print(err_msg)\n",
    "                return prior_pdf\n",
    "                # raise ValueError(err_msg)\n",
    "            # print(f\"Recursion {recursion_depth}: Zero values detected. Increasing scale to {scale:.6f}\")\n",
    "            return self._compute_prior_from_laplace_fit(predicted_uar, n_cols=n_cols, recursion_depth=recursion_depth + 1)\n",
    "        \n",
    "        second_check = jnp.trapezoid(prior_pdf, x=self.data.baseline_log_grid)\n",
    "        assert np.isclose(second_check, 1, atol=2e-4), f'prior check != 1, {second_check:.6f} N={len(predicted_uar)} {predicted_uar}'\n",
    "        assert np.min(prior_pdf) > 0, f'min prior == 0, scale={scale:.5f}'\n",
    "\n",
    "        # convert prior PDF to PMF (pseudo-count mass function)\n",
    "        prior_pmf = prior_pdf * self.data.log_dx\n",
    "\n",
    "        # scale the number of pseudo-counts based on years of record  (365 / n_observations)\n",
    "        # and number of models in the ensemble (given by n_cols)\n",
    "        prior_pseudo_counts = prior_pmf * (365 / (len(predicted_uar) * n_cols))\n",
    "        \n",
    "        # return weighted_prior_pdf\n",
    "        return prior_pseudo_counts\n",
    "    \n",
    "\n",
    "    def _compute_frequency_ensemble_mean(self, pdfs, weights):\n",
    "        \"\"\"\n",
    "        This function computes the weighted ensemble distribution estimates.\n",
    "        \"\"\"\n",
    "        # Normalize distance weights\n",
    "        if weights is not None:\n",
    "            weights /= jnp.sum(weights).astype(float)\n",
    "            weights = jnp.array(weights)  # Ensure 1D array\n",
    "            pdf_est = jnp.asarray(pdfs.to_numpy() @ weights)\n",
    "        else:\n",
    "            pdf_est = jnp.asarray(pdfs.mean(axis=1).to_numpy())\n",
    "\n",
    "\n",
    "        # Check integral before normalization\n",
    "        pdf_check = jnp.trapezoid(pdf_est, x=self.data.baseline_log_grid)\n",
    "        normalized_pdf = pdf_est / pdf_check\n",
    "        assert jnp.isclose(jnp.trapezoid(normalized_pdf, x=self.data.baseline_log_grid), 1), f'ensemble pdf does not integrate to 1: {pdf_check:.4f}'\n",
    "                \n",
    "        # Compute PMF\n",
    "        pmf_est = normalized_pdf * self.data.log_dx\n",
    "        pmf_est /= jnp.sum(pmf_est)\n",
    "\n",
    "        return pmf_est, pdf_est\n",
    "    \n",
    "\n",
    "    def _compute_ensemble_member_distribution_estimates(self, df):\n",
    "        \"\"\"\n",
    "        Compute the ensemble distribution estimates based on the KNN dataframe.\n",
    "        \"\"\"    \n",
    "        pdfs, prior_biases = pd.DataFrame(), {}\n",
    "        # initialize a kde estimator object\n",
    "        kde = KDEEstimator(self.data.baseline_log_grid, self.data.log_dx)\n",
    "        for c in df.columns: \n",
    "            # evaluate the laplace on the prediction as a prior\n",
    "            # drop the nan values\n",
    "            values = df[c].dropna().values\n",
    "            obs_count = len(values)\n",
    "            assert len(values) > 0, f'0 values for {c}'\n",
    "\n",
    "            # compute the pdf and pmf using kde\n",
    "            assert sum(np.isnan(values)) == 0, f'NaN values in {c} {values[:5]}'\n",
    "\n",
    "            kde_pmf, _ = kde.compute(\n",
    "                values, self.data.target_da\n",
    "            )\n",
    "\n",
    "            prior = self._compute_prior_from_laplace_fit(values, n_cols=1) # priors are expressed in pseudo-counts\n",
    "            # convert the pdf to counts and apply the prior\n",
    "            counts = kde_pmf * obs_count + prior\n",
    "\n",
    "            # re-normalize the pmf\n",
    "            pmf = counts / jnp.sum(counts)\n",
    "            pdf = pmf / self.data.log_dx\n",
    "\n",
    "            pdf_check = jnp.trapezoid(pdf, x=self.data.baseline_log_grid)\n",
    "            pdf /= pdf_check\n",
    "            # pdf /= pdf_check\n",
    "            assert jnp.isclose(jnp.trapezoid(pdf, x=self.data.baseline_log_grid), 1.0, atol=0.001), f'pdf does not integrate to 1 in compute_ensemble_member_distribution_estimates: {pdf_check:.4f}'\n",
    "            pdfs[c] = pdf\n",
    "\n",
    "            # convert the pdf to pmf\n",
    "            pmf = pdf * self.data.log_dx\n",
    "            pmf /= jnp.sum(pmf)\n",
    "            # assert np.isclose(np.sum(pmf), 1, atol=1e-4), f'pmf does not sum to 1 in compute_ensemble_member_distribution_estimates: {np.sum(pmf):.5f}'\n",
    "            \n",
    "            # compute the bias added by the prior\n",
    "            prior_biases[c.split('_')[0]] = {'DKL': self.data._compute_kld(kde_pmf, pmf), 'EMD': self.data._compute_emd(kde_pmf, pmf)}\n",
    "        return pdfs, prior_biases\n",
    "    \n",
    "    \n",
    "    def _compute_frequency_ensemble_distributions(self, nbr_df, nbr_data, distance_type):\n",
    "        \"\"\"\n",
    "        For asynchronous comparisons, we estimate pdfs for ensemble members, then compute the mean in the time domain\n",
    "        to represent the FDC simulation.  We do not do temporal averaging in this case.\n",
    "        \"\"\"\n",
    "        # distances_all = knn_data_all['distance'].values[:self.k_nearest]\n",
    "        # nbr_ids_all = knn_data_all['official_id'].values[:self.k_nearest]\n",
    "        knn_df_all = nbr_df.iloc[:, :self.k_nearest].copy()\n",
    "        knn_data_all = nbr_data.iloc[:, :self.k_nearest].copy()\n",
    "        frequency_ensemble_pdfs, _ = self._compute_ensemble_member_distribution_estimates(knn_df_all)\n",
    "        \n",
    "        # distances = jnp.array(nbr_data['distance'].astype(float).values)\n",
    "        labels, pdfs, pmfs = [], [], []\n",
    "        all_distances = knn_data_all['distance'].values\n",
    "        all_ids = knn_data_all['official_id'].values\n",
    "        # prior_bias_df = pd.DataFrame(prior_bias_dict)\n",
    "        for wm in self.weight_schemes:\n",
    "            for k in range(1, self.k_nearest + 1):\n",
    "                distances = all_distances[:k]\n",
    "                nbr_ids = all_ids[:k]\n",
    "                knn_pdfs = frequency_ensemble_pdfs.iloc[:, :k].copy()\n",
    "\n",
    "                label = f'{self.target_stn}_{k}_NN_{distance_type}_ID{wm}_freqEnsemble'\n",
    "                weights = self._compute_weights(wm, k, distances)\n",
    "                pmf_est, pdf_est = self._compute_frequency_ensemble_mean(knn_pdfs, weights)\n",
    "                assert pmf_est is not None, f'pmf_est is None for {label}'\n",
    "            \n",
    "                # compute the mean number of observations (non-nan values) per row\n",
    "                mean_obs_per_timestep = knn_df_all.iloc[:, :k].notna().sum(axis=1).mean()\n",
    "                mean_obs_per_proxy = knn_df_all.iloc[:, :k].notna().sum(axis=0).mean()\n",
    "      \n",
    "                # compute the frequency-based ensemble pdf estimate\n",
    "                self.knn_simulation_data[label] = {'k': k, 'n_obs': mean_obs_per_proxy,\n",
    "                                                'mean_obs_per_timestep': mean_obs_per_timestep,\n",
    "                                                'nbrs': ','.join(nbr_ids)}\n",
    "\n",
    "                self.knn_simulation_data[label]['DKL'] = self.data._compute_kld(self.ctx.baseline_pmf, pmf_est)\n",
    "                self.knn_simulation_data[label]['EMD'] = self.data._compute_emd(self.ctx.baseline_pmf, pmf_est)\n",
    "                \n",
    "                pdfs.append(np.asarray(pdf_est))\n",
    "                pmfs.append(np.asarray(pmf_est))\n",
    "                labels.append(label)\n",
    "\n",
    "        # create a dataframe of labels(columns) for each pdf\n",
    "        knn_pdfs = pd.DataFrame(pdfs, index=labels).T\n",
    "        knn_pmfs = pd.DataFrame(pmfs, index=labels).T\n",
    "        # Filter out already existing columns to avoid duplication\n",
    "        new_pdf_cols = knn_pdfs.columns.difference(self.knn_pdfs.columns)\n",
    "        new_pmf_cols = knn_pmfs.columns.difference(self.knn_pmfs.columns)\n",
    "        # Concat only new columns\n",
    "        self.knn_pdfs = pd.concat([self.knn_pdfs, knn_pdfs[new_pdf_cols]], axis=1)\n",
    "        self.knn_pmfs = pd.concat([self.knn_pmfs, knn_pmfs[new_pmf_cols]], axis=1)\n",
    "    \n",
    "    \n",
    "    def _delta_spike_pmf_pdf(self, single_val, log_grid):\n",
    "        \"\"\"\n",
    "        Return a spike PMF and compatible PDF centered at the only value in the input.\n",
    "        The spike is placed at the nearest log_grid point to log(single_val).\n",
    "        \"\"\"\n",
    "        log_val = jnp.log(single_val)\n",
    "        spike_idx = jnp.argmin(jnp.abs(log_grid - log_val))\n",
    "        \n",
    "        pmf = jnp.zeros_like(log_grid)\n",
    "        pmf = pmf.at[spike_idx].set(1.0)\n",
    "\n",
    "        dx = jnp.gradient(log_grid)\n",
    "        pdf = pmf / dx  # assign all mass to one bin\n",
    "\n",
    "        return pmf, pdf\n",
    "\n",
    "    \n",
    "    def _compute_nse(self, obs, sim):\n",
    "        \"\"\"Compute the Nash-Sutcliffe Efficiency (NSE) between observed and simulated values.\"\"\"\n",
    "        assert not np.isnan(obs).any(), f'NaN values in obs: {obs}'\n",
    "        assert not np.isnan(sim).any(), f'NaN values in sim: {sim}'\n",
    "        assert (obs >= 0).all(), f'Negative values in obs: {obs}'\n",
    "        assert (sim >= 0).all(), f'Negative values in sim: {sim}'\n",
    "        # Compute the NSE\n",
    "        numerator = jnp.sum((obs - sim) ** 2)\n",
    "        denominator = jnp.sum((obs - obs.mean()) ** 2)\n",
    "        nse = 1 - (numerator / denominator)\n",
    "        return nse\n",
    "\n",
    "\n",
    "    def _compute_KGE(self, obs, sim):\n",
    "        \"\"\"Compute the Kling-Gupta Efficiency (KGE) between observed and simulated values.\"\"\"\n",
    "        assert not np.isnan(obs).any(), f'NaN values in obs: {obs}'\n",
    "        assert not np.isnan(sim).any(), f'NaN values in sim: {sim}'\n",
    "        assert (obs >= 0).all(), f'Negative values in obs: {obs}'\n",
    "        assert (sim >= 0).all(), f'Negative values in sim: {sim}'\n",
    "        # Compute the KGE\n",
    "        r = jnp.corrcoef(obs, sim)[0, 1]\n",
    "        alpha = sim.mean() / obs.mean()\n",
    "        beta = sim.std() / obs.std()\n",
    "        kge = 1 - jnp.sqrt((r - 1) ** 2 + (alpha - 1) ** 2 + (beta - 1) ** 2)\n",
    "        return kge\n",
    "    \n",
    "    \n",
    "    def _compute_ensemble_contribution_metrics(self, df: pd.DataFrame, weights: np.ndarray):\n",
    "        mask = ~df.isna()\n",
    "        \n",
    "        # Mean number of valid values per row\n",
    "        mean_valid_per_row = mask.sum(axis=1).mean()\n",
    "\n",
    "        # Normalized weights per row, masking NaNs\n",
    "        X = df.to_numpy()\n",
    "        W = np.broadcast_to(weights, X.shape)\n",
    "        masked_weights = np.where(mask, W, 0.0)\n",
    "        weight_sums = masked_weights.sum(axis=1)\n",
    "        weight_sums[weight_sums == 0] = np.nan\n",
    "        normalized_weights = masked_weights / weight_sums[:, None]\n",
    "\n",
    "        # Average contribution per column across all rows\n",
    "        mean_w = np.nanmean(normalized_weights, axis=0)\n",
    "        effective_n = 1.0 / np.nansum(mean_w ** 2)\n",
    "\n",
    "        return mean_valid_per_row, effective_n\n",
    "    \n",
    "    \n",
    "    def _generate_temporal_mean_ensemble_runoff_simulation(self, df, weights=None):\n",
    "\n",
    "        assert ~df.empty, 'dataframe is empty'\n",
    "        assert (df != 0).any().any(), \"All values are zero in df[cols] before processing\"\n",
    "        \n",
    "        if weights is not None:\n",
    "            assert ~jnp.any(jnp.isnan(weights)), f'nan weight found: {weights}'\n",
    "            assert jnp.isclose(jnp.sum(weights), 1), f'weights do not sum to 1: {weights}'\n",
    "            # assert (weights > 0).all(), f'not all weights > 0, {weights}'\n",
    "            assert jnp.all(weights != 0), f'Weights must not = 0: weights={weights}'\n",
    "            estimated_uar = df.mul(weights, axis=1).sum(axis=1)\n",
    "        else:\n",
    "            assert df.isna().sum().sum() == 0, f\"Some NaNs still in df: {df[df.isna().any(axis=1)]}\"\n",
    "            assert all(df.dtypes == 'float64') or all(np.issubdtype(t, np.floating) for t in df.dtypes), \"Non-float column in df\"\n",
    "            estimated_uar = df.mean(axis=1, skipna=True)\n",
    "\n",
    "        assert not np.isnan(estimated_uar).any(), \"NaN values found in estimated_uar\"\n",
    "        assert (estimated_uar >= 0).all(), f\"Estimate < 0 detected: {np.min(estimated_uar)}\"\n",
    "        return estimated_uar\n",
    "\n",
    "    \n",
    "    def _weighted_row_mean_ignore_nan(self, df: pd.DataFrame, weights: np.ndarray):\n",
    "        \"\"\"\n",
    "        In the case of computing weighted means across multiple columns,\n",
    "        we need to adjust the column weights to account for NaN values.\n",
    "        This function computes the weighted mean for each row, ignoring NaN values.\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            # set the right shape for the weights\n",
    "            weights = np.ones(df.shape[1], dtype=np.float64)\n",
    "\n",
    "        assert df.shape[1] == len(weights), f\"df.shape[1] != len(weights): {df.shape[1]} != {len(weights)}\"\n",
    "\n",
    "        X = df.to_numpy()\n",
    "        W = np.broadcast_to(weights, X.shape)\n",
    "\n",
    "        mask = ~np.isnan(X)  # valid entries\n",
    "        masked_weights = np.where(mask, W, 0.0)\n",
    "\n",
    "        weight_sums = masked_weights.sum(axis=1)\n",
    "        # avoid division by zero\n",
    "        weight_sums[weight_sums == 0] = np.nan\n",
    "\n",
    "        normalized_weights = masked_weights / weight_sums[:, None]\n",
    "        estimated = np.nansum(X * normalized_weights, axis=1)\n",
    "\n",
    "        mean_valid_per_row, effective_k = self._compute_ensemble_contribution_metrics(df, weights)\n",
    "\n",
    "        return pd.Series(estimated, index=df.index), mean_valid_per_row, effective_k\n",
    "            \n",
    "\n",
    "    def _get_knn_data_effective(self, k, nbr_df, nbr_data, distances, wm, distance_type, effective_k_nbrs):\n",
    "        eff_k_df = self.nbr_dfs[distance_type]['concurrent']['effective_k']\n",
    "        k_to_use = eff_k_df[eff_k_df['effective_k'] >= k].index[0]\n",
    "        mean_furthest_idx = eff_k_df.loc[k_to_use, 'mean_furthest_idx']\n",
    "        \n",
    "        knn_df = nbr_df.iloc[:, :k_to_use].copy()\n",
    "        weights = self._compute_weights(wm, k_to_use, distances[:k_to_use])\n",
    "        temporal_ensemble_mean, mean_valid_per_row, effective_k = self._weighted_row_mean_ignore_nan(knn_df, weights)\n",
    "        nbrs_used = [c.split('_')[0] for c in knn_df.columns]\n",
    "        return temporal_ensemble_mean, weights, knn_df, nbrs_used, effective_k, mean_furthest_idx\n",
    "    \n",
    "\n",
    "    def _finalize_temporal_ensemble(\n",
    "            self, k, label, temporal_ensemble_mean, nbrs_used,\n",
    "            effective_k, mean_valid_per_row\n",
    "            ):\n",
    "\n",
    "        # Clip to prevent zero runoff issues\n",
    "        temporal_ensemble_mean = np.clip(\n",
    "            temporal_ensemble_mean, 1000 * 1e-4 / self.data.target_da, None\n",
    "        )\n",
    "\n",
    "        # Estimate prior from Laplace fit\n",
    "        prior = self._compute_prior_from_laplace_fit(temporal_ensemble_mean, n_cols=1)\n",
    "\n",
    "        # Compute NSE and KGE\n",
    "        sim_df = temporal_ensemble_mean.rename('sim').to_frame()\n",
    "        sim_obs_df = pd.concat(\n",
    "            [self.data.stn_df[[f'{self.target_stn}_uar']].copy(), sim_df],\n",
    "            axis=1, join='inner'\n",
    "        )\n",
    "\n",
    "        obs = sim_obs_df[f'{self.target_stn}_uar'].values\n",
    "        sim = sim_obs_df['sim'].values\n",
    "        nse = self._compute_nse(obs, sim)\n",
    "        kge = self._compute_KGE(obs, sim)\n",
    "\n",
    "        # Estimate PDF/PMF using KDE or \n",
    "        # add small amount of random noise if there is no variance\n",
    "        if len(jnp.unique(temporal_ensemble_mean.values)) == 1:\n",
    "            est_pmf, est_pdf = self._delta_spike_pmf_pdf(\n",
    "                temporal_ensemble_mean.values[0], self.data.baseline_log_grid\n",
    "            )\n",
    "        else:\n",
    "            est_pmf, est_pdf = self.target_kde.compute(\n",
    "                temporal_ensemble_mean.values, self.data.target_da\n",
    "            )\n",
    "\n",
    "        assert est_pmf is not None, f'pmf is None for {label}'\n",
    "\n",
    "        # Store simulation outputs and metadata\n",
    "        self.knn_pdfs[label] = est_pdf\n",
    "        self.knn_pmfs[label] = est_pmf\n",
    "        self.knn_simulation_data[label] = {\n",
    "            'nbrs': nbrs_used,\n",
    "            'k': k,\n",
    "            'prior': prior,\n",
    "            'nse': nse,\n",
    "            'kge': kge,\n",
    "            'n_obs': len(temporal_ensemble_mean),\n",
    "            'mean_': mean_valid_per_row,\n",
    "            'mean_nbrs_per_timestep': effective_k,  # rename if clearer\n",
    "            'effective_k': effective_k,\n",
    "            'DKL': self.data._compute_kld(self.ctx.baseline_pmf, est_pmf),\n",
    "            'EMD': self.data._compute_emd(self.ctx.baseline_pmf, est_pmf)\n",
    "        }\n",
    "\n",
    "\n",
    "    def _compute_temporal_ensemble_distributions(self, distance_type, wm, nbr_df, nbr_data):\n",
    "        distances = nbr_data['distance'].astype(float).values\n",
    "        for k in range(1, self.k_nearest + 1):\n",
    "            knn_df = nbr_df.iloc[:, :k].copy()\n",
    "            label = f'{self.target_stn}_{k}_NN_{distance_type}_ID{wm}_timeEnsemble'            \n",
    "            weights = self._compute_weights(wm, k, distances[:k])\n",
    "            temporal_ensemble_mean, mean_valid_per_row, effective_k = self._weighted_row_mean_ignore_nan(knn_df, weights)\n",
    "            nbrs_used = [c.split('_')[0] for c in knn_df.columns]\n",
    "            self._finalize_temporal_ensemble(\n",
    "                k, label, temporal_ensemble_mean, nbrs_used,\n",
    "                effective_k, mean_valid_per_row\n",
    "            )\n",
    "\n",
    "    \n",
    "    def _compute_distribution_estimates(self, distance_type):\n",
    "\n",
    "        nbr_df = self.nbr_dfs[distance_type]['nbr_df'].copy()\n",
    "        nbr_data = self.nbr_dfs[distance_type]['nbr_data'].copy()\n",
    "\n",
    "        for wm in self.weight_schemes:\n",
    "            # compute the FDC estimate by temporal ensemble mean\n",
    "            self._compute_temporal_ensemble_distributions(distance_type, wm, nbr_df, nbr_data)\n",
    "            # compute the frequency average ensemble pdfs\n",
    "            self._compute_frequency_ensemble_distributions(nbr_df, nbr_data, distance_type)\n",
    "\n",
    "        # Validation\n",
    "        sim_labels = list(self.knn_simulation_data.keys())\n",
    "        pdf_labels = list(self.knn_pdfs.columns)\n",
    "        assert set(sim_labels) == set(pdf_labels)\n",
    "        \n",
    "    \n",
    "    def run_estimators(self, divergence_measures, eps, baseline_pmf):\n",
    "        self.complete_target_years = self._find_complete_years(self.data.stn_df[[f'{self.target_stn}_uar']].copy())\n",
    "                          \n",
    "        self._initialize_nearest_neighbour_data()\n",
    "        # set the baseline pdf by kde\n",
    "        self.target_kde = KDEEstimator(self.data.baseline_log_grid, self.data.log_dx)\n",
    "        for dist in ['spatial_dist', 'attribute_dist']:            \n",
    "            self._compute_distribution_estimates(dist)\n",
    "        return self._format_results()\n",
    "    \n",
    "    \n",
    "    def _make_json_serializable(self, d):\n",
    "        output = {}\n",
    "        for k, v in d.items():\n",
    "            if isinstance(v, (np.ndarray, jnp.ndarray)):\n",
    "                output[k] = v.tolist()\n",
    "            elif hasattr(v, \"tolist\"):\n",
    "                output[k] = v.tolist()\n",
    "            else:\n",
    "                output[k] = v\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def _format_results(self):\n",
    "        pmf_labels, pdf_labels, sim_labels = list(self.knn_pmfs.columns), list(self.knn_pdfs.columns), list(self.knn_simulation_data.keys())\n",
    "        # assert label sets are the same\n",
    "        assert set(pmf_labels) == set(pdf_labels), f'pmf_labels {pmf_labels} != pdf_labels {pdf_labels}'\n",
    "        assert set(pmf_labels) == set(sim_labels), f'pmf_labels {pmf_labels} != sim_labels {sim_labels}'\n",
    "        results = self.knn_simulation_data\n",
    "        for label in pmf_labels:\n",
    "            results[label].pop('prior', None)\n",
    "        for label in pmf_labels:\n",
    "            # add the pmf and pdf in a json serializable format\n",
    "            results[label]['pmf'] = self.knn_pmfs[label].tolist()\n",
    "            results[label]['pdf'] = self.knn_pdfs[label].tolist()\n",
    "            results[label] = self._make_json_serializable(results[label])\n",
    "        return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "bd61235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMFDCEstimator:\n",
    "    def __init__(self, ctx, target_stn, data, *args, **kwargs):\n",
    "        # super().__init__(*args, **kwargs)\n",
    "        self.ctx = ctx\n",
    "        self.target_stn = target_stn\n",
    "        self.data = data\n",
    "        # self.data = data\n",
    "        self.LSTM_forcings_folder = self.ctx.LSTM_forcings_folder\n",
    "        self.LSTM_ensemble_result_folder = self.ctx.LSTM_ensemble_result_folder\n",
    "        self.df = self._load_ensemble_result()\n",
    "        self.df = self._filter_for_complete_years()\n",
    "        self.sim_cols = sorted([c for c in self.df.columns if c.startswith('streamflow_sim_')])\n",
    "\n",
    "\n",
    "    def _load_ensemble_result(self):\n",
    "        fpath = os.path.join(self.LSTM_ensemble_result_folder, f'{self.target_stn}_ensemble.csv')\n",
    "        df = pd.read_csv(fpath)\n",
    "        # rename 'Unnamed: 0' to 'time' and set to index\n",
    "        df.rename(columns={'Unnamed: 0': 'time'}, inplace=True)\n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "        df.set_index('time', inplace=True)\n",
    "        return df\n",
    "    \n",
    "\n",
    "    def _filter_for_complete_years(self):\n",
    "        # Convert to datetime only if necessary\n",
    "        if self.df.empty:\n",
    "            return pd.DataFrame()\n",
    "        date_column = 'time'\n",
    "        self.df.reset_index(inplace=True)\n",
    "        if not np.issubdtype(self.df[date_column].dtype, np.datetime64):\n",
    "            self.df = self.df.copy()\n",
    "            self.df[date_column] = pd.to_datetime(self.df[date_column])\n",
    "\n",
    "        # Filter out missing values first\n",
    "        valid_data = self.df.copy().dropna()\n",
    "\n",
    "        # Extract year and month\n",
    "        valid_data['year'] = valid_data[date_column].dt.year\n",
    "        valid_data['month'] = valid_data[date_column].dt.month\n",
    "        valid_data['day'] = valid_data[date_column].dt.day\n",
    "        \n",
    "        # Count total and missing days per year-month group\n",
    "        month_counts = valid_data.groupby(['year', 'month'])['day'].nunique()\n",
    "        \n",
    "        # Identify complete months (at least 20 observations)\n",
    "        complete_months = (month_counts >= 20)\n",
    "\n",
    "        # count how many complete months per year\n",
    "        complete_month_counts = complete_months.groupby(level=0).sum()\n",
    "        \n",
    "        complete_years = complete_month_counts[complete_month_counts == 12]\n",
    "        self.complete_years = list(complete_years.index.values)\n",
    "\n",
    "        valid_data = valid_data[valid_data['year'].isin(complete_years.index)].copy()\n",
    "        # drop the year column\n",
    "        return valid_data.drop(columns=['year', 'month', 'day'])\n",
    "    \n",
    "    \n",
    "    def _load_LSTM_forcing_file(self):\n",
    "        # retrieve LSTM forcing data\n",
    "        # read the forcing data from the LSTM forcing file\n",
    "        # and return a dataframe with the same index as the LSTM results\n",
    "        ldf = pd.read_csv(os.path.join(self.met_forcings_folder, f'{self.target_stn}_forcing.csv'))\n",
    "        ldf.rename(columns={'Unnamed: 0': 'time'}, inplace=True)\n",
    "        ldf['time'] = pd.to_datetime(ldf['time'])\n",
    "        ldf.set_index('time', inplace=True)\n",
    "        ldf = ldf.loc[self.stn_df.index]\n",
    "        # convert to unit area runoff (L/s/km2)\n",
    "        ldf['uar'] = 1000 * ldf['discharge'] / self.target_da\n",
    "        return ldf\n",
    "\n",
    "    \n",
    "    def _plot_pmfs(self, pmf_time, pmf_freq, line_dash='solid'):\n",
    "        # plot using bokeh\n",
    "        f = figure(title=self.target_stn, width=600, height=400)\n",
    "        f.line(self.data.baseline_log_grid, pmf_time, line_width=2, color='blue', legend_label='Time Ensemble', line_dash=line_dash)\n",
    "        # f.line(self.data.baseline_log_grid, pmf1, line_width=2, color='red', legend_label='T_MeanLinEns PMF', line_dash=line_dash)\n",
    "        f.line(self.data.baseline_log_grid, pmf_freq, line_width=2, color='purple', legend_label='Frequency Ensemble', line_dash=line_dash)\n",
    "        f.line(self.data.baseline_log_grid, self.ctx.baseline_pmf, line_width=2, color='green', legend_label='Observed', line_dash=line_dash)\n",
    "        f.xaxis.axis_label = 'Log UAR (L/s/km2)'\n",
    "        f.yaxis.axis_label = 'PMF'\n",
    "        f.legend.location = 'top_left'\n",
    "        f.legend.background_fill_alpha = 0.25\n",
    "        f.legend.click_policy = 'hide'\n",
    "        f = dpf.format_fig_fonts(f, font_size=14)\n",
    "        show(f)\n",
    "    \n",
    "    \n",
    "    def _compute_time_ensemble_pmf(self):\n",
    "        data = self.df[self.sim_cols].copy()\n",
    "        temporal_ensemble_log = data.mean(axis=1) # this is still in log space\n",
    "        temporal_ensemble = np.exp(temporal_ensemble_log.values)\n",
    "        pmf, _ = self.kde.compute(temporal_ensemble, self.data.target_da)\n",
    "        return pmf\n",
    "\n",
    "    \n",
    "    def _compute_frequency_ensemble_pmf(self):\n",
    "        data = self.df[self.sim_cols].copy()\n",
    "        data.dropna(inplace=True)\n",
    "        # compute the frequency ensemble PMF\n",
    "        # initialize a len(data) x n_sim_cols array\n",
    "        pmfs = np.column_stack([\n",
    "            self.kde.compute(np.exp(data[c].values), self.data.target_da)[0]\n",
    "            for c in self.sim_cols\n",
    "        ])\n",
    "        # average the pmfs over the ensemble \n",
    "        result = pmfs.mean(axis=1)\n",
    "        assert len(result) == len(self.data.baseline_log_grid), f'len(pmfs) = {len(pmfs)} != len(baseline_log_grid) = {len(self.data.baseline_log_grid)}' \n",
    "        return result\n",
    "        \n",
    "    \n",
    "    def _process_result(self, divergence_measures, eps, time_ensemble_pmf, freq_ensemble_pmf, baseline_pmf):\n",
    "        results = {}\n",
    "        # compute the baseline pmf\n",
    "        # compute the divergence measures\n",
    "        for label, raw_pmf in zip(['Time', 'Frequency'], [time_ensemble_pmf, freq_ensemble_pmf]):\n",
    "            # add the prior and renormalize\n",
    "            pmf = raw_pmf #+ eps\n",
    "            pmf /= pmf.sum()\n",
    "            results[label] = {'raw_pmf': raw_pmf.tolist(), 'pmf': pmf.tolist()}\n",
    "            for measure in divergence_measures:\n",
    "                bias = self.data._compute_bias_from_eps(raw_pmf, measure, eps=eps)\n",
    "                d = self.data.divergence_functions[measure](baseline_pmf, pmf)\n",
    "                if bias < 0 or bias == -0.0:\n",
    "                    assert np.isclose(bias, 0, atol=1e-6), f'sum P = {np.sum(pmf)}'\n",
    "                    bias = 0\n",
    "                bias_pct = 100 * bias / d\n",
    "                if bias_pct > 5:\n",
    "                    raise Warning(f'Warning: bias is > 5% of measure: {bias_pct:.1f}%')\n",
    "                \n",
    "                results[label][measure] = {'value': d, 'bias': bias, 'bias_pct': bias_pct}\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "    def run_estimators(self, divergence_measures, eps, baseline_pmf):\n",
    "        # met_forcing = self._load_LSTM_forcing_file()  # Load LSTM forcing data\n",
    "        self.kde = KDEEstimator(self.data.baseline_log_grid, self.data.log_dx)\n",
    "        time_ensemble_pmf = self._compute_time_ensemble_pmf()\n",
    "        freq_ensemble_pmf = self._compute_frequency_ensemble_pmf()\n",
    "        # self._plot_pmfs(time_ensemble_pmf, freq_ensemble_pmf)\n",
    "        return self._process_result(divergence_measures, eps, time_ensemble_pmf, freq_ensemble_pmf, baseline_pmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d0ecf64f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Using all stations in the catchment data with a baseline PMF (validated): 1024\n",
      "    ...overlap dict loaded from data/record_overlap_dict.json\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "target_cols = [\n",
    "    'mean_uar', 'sd_uar', \n",
    "    'mean_logx', 'sd_logx', \n",
    "]\n",
    "\n",
    "# from utils import FDCEstimationContext\n",
    "attr_df_fpath = os.path.join('data', f'catchment_attributes_with_runoff_stats.csv')\n",
    "LSTM_forcings_folder = '/home/danbot/neuralhydrology/data/BCUB_catchment_mean_met_forcings_20250320'\n",
    "LSTM_ensemble_result_folder = '/home/danbot/code/neuralhydrology/data/ensemble_results_20250514'\n",
    "baseline_distribution_folder = os.path.join('data', 'results', 'baseline_distributions')\n",
    "# parameter_prediction_results_folder = os.path.join('data', 'parameter_prediction_results')\n",
    "\n",
    "methods = ('parametric', 'lstm')\n",
    "# methods = ('knn',)\n",
    "exclude_pre_1980_data = False  # use only stations with data 1980-present concurrent with Daymet\n",
    "daymet_start_date = '1950-01-01'  # default start date for Daymet data\n",
    "k_nearest = 10\n",
    "if exclude_pre_1980_data:\n",
    "    daymet_start_date = '1980-01-01'\n",
    "\n",
    "processed = []\n",
    "ESTIMATOR_CLASSES = {\n",
    "    'parametric': ParametricFDCEstimator,\n",
    "    'lstm': LSTMFDCEstimator,\n",
    "    'knn': kNNFDCEstimator,\n",
    "    # add others here\n",
    "}\n",
    "input_data = {\n",
    "    'attr_df_fpath': attr_df_fpath,\n",
    "    'LSTM_forcings_folder': LSTM_forcings_folder,\n",
    "    'LSTM_ensemble_result_folder': LSTM_ensemble_result_folder,\n",
    "    'LSTM_concurrent_network': exclude_pre_1980_data,  # use only stations with data 1980-present concurrent with Daymet\n",
    "    'daymet_start_date': daymet_start_date,\n",
    "    # 'parameter_prediction_results_folder': parameter_prediction_results_folder,\n",
    "    'predicted_param_dict': predicted_param_dict,\n",
    "    'divergence_measures': ['DKL', 'EMD'],\n",
    "    'baseline_pmf_stations': pmf_stations,\n",
    "    'eps': 1e-12,\n",
    "    'min_flow': 1e-4,\n",
    "    'n_grid_points': 2**12,\n",
    "    'min_record_length': 5,\n",
    "    'minimum_days_per_month': 15,\n",
    "    'parametric_target_cols': target_cols,\n",
    "    'all_official_ids': station_ids,\n",
    "    'daymet_concurrent_stations': daymet_concurrent_stations,\n",
    "    'baseline_distribution_folder': baseline_distribution_folder,\n",
    "}\n",
    "\n",
    "context = FDCEstimationContext(**input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d6acab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating FDC for 05AA008...\n",
      "Estimating FDC for 05AA022...\n",
      "Estimating FDC for 05AA023...\n",
      "Estimating FDC for 05AA035...\n",
      "Estimating FDC for 05AD003...\n",
      "Estimating FDC for 05BA001...\n",
      "Estimating FDC for 05BB001...\n",
      "Estimating FDC for 05BF016...\n",
      "Estimating FDC for 05BF017...\n",
      "Estimating FDC for 05BF018...\n",
      "Processed 10/1024 stations in 1.22 seconds per station\n",
      "Estimating FDC for 05BF019...\n",
      "Estimating FDC for 05BG006...\n",
      "Estimating FDC for 05BH015...\n",
      "Estimating FDC for 05BJ004...\n",
      "Estimating FDC for 05BJ010...\n",
      "Estimating FDC for 05BL012...\n",
      "Estimating FDC for 05BL014...\n",
      "Estimating FDC for 05BL022...\n",
      "Estimating FDC for 05CA009...\n",
      "Estimating FDC for 05CB001...\n",
      "Processed 20/1024 stations in 1.28 seconds per station\n",
      "Estimating FDC for 05CB004...\n",
      "Estimating FDC for 05CC001...\n",
      "Estimating FDC for 05CC007...\n",
      "Estimating FDC for 05DA007...\n",
      "Estimating FDC for 05DA009...\n",
      "Estimating FDC for 05DA010...\n",
      "Estimating FDC for 05DB002...\n",
      "Estimating FDC for 05DB006...\n",
      "Estimating FDC for 05DC006...\n",
      "Estimating FDC for 05DC012...\n",
      "Processed 30/1024 stations in 1.43 seconds per station\n",
      "Estimating FDC for 05DD009...\n",
      "Estimating FDC for 07AA001...\n",
      "Estimating FDC for 07AA002...\n",
      "Estimating FDC for 07AD002...\n",
      "Estimating FDC for 07AF002...\n",
      "Estimating FDC for 07AG003...\n",
      "Estimating FDC for 07AG007...\n",
      "Estimating FDC for 07BB002...\n",
      "Estimating FDC for 07BB003...\n",
      "Estimating FDC for 07BC002...\n",
      "Processed 40/1024 stations in 1.34 seconds per station\n",
      "Estimating FDC for 07EA002...\n",
      "Estimating FDC for 07EA004...\n",
      "Estimating FDC for 07EA005...\n",
      "Estimating FDC for 07EA007...\n",
      "Estimating FDC for 07EB002...\n",
      "Estimating FDC for 07EC002...\n",
      "Estimating FDC for 07EC003...\n",
      "Estimating FDC for 07EC004...\n",
      "Estimating FDC for 07ED001...\n",
      "Estimating FDC for 07ED003...\n",
      "Processed 50/1024 stations in 1.50 seconds per station\n",
      "Estimating FDC for 07EE007...\n",
      "Estimating FDC for 07EE009...\n",
      "Estimating FDC for 07EE010...\n",
      "Estimating FDC for 07EF004...\n",
      "Estimating FDC for 07FA003...\n",
      "Estimating FDC for 07FA005...\n",
      "Estimating FDC for 07FA006...\n",
      "Estimating FDC for 07FB001...\n",
      "Estimating FDC for 07FB002...\n",
      "Estimating FDC for 07FB003...\n"
     ]
    }
   ],
   "source": [
    "processed = []\n",
    "t0 = time()\n",
    "for stn in [s for s in context.official_ids if s in daymet_concurrent_stations]:\n",
    "    if stn == '12414900': # this station has no data in the LSTM ensemble results\n",
    "        print(f'    ...skipping {stn} due to naming issue.')\n",
    "        continue\n",
    "    print(f'Estimating FDC for {stn}...')\n",
    "    runner = FDCEstimatorRunner(stn, context, methods, k_nearest, target_cols, ESTIMATOR_CLASSES)\n",
    "    runner.run_selected()\n",
    "    processed.append(stn)\n",
    "    if len(processed) % 10 == 0:\n",
    "        t1 = time()\n",
    "        elapsed = t1 - t0\n",
    "        unit_time = elapsed / len(processed)\n",
    "        print(f'Processed {len(processed)}/{len(context.official_ids)} stations in {unit_time:.2f} seconds per station')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129cc60a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5f9c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
