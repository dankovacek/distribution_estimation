{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "558cd32f-b562-4502-ab63-826aede8f321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from time import time\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import defaultdict\n",
    "\n",
    "import xgboost as xgb\n",
    "xgb.config_context(verbosity=2)\n",
    "from scipy.stats import wasserstein_distance\n",
    "from scipy.stats import norm, laplace\n",
    "\n",
    "from bokeh.plotting import figure, show, save, output_file\n",
    "# from bokeh.io import output_notebook\n",
    "from bokeh.models import LinearColorMapper\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit\n",
    "\n",
    "# from KDEpy import FFTKDE\n",
    "\n",
    "import data_processing_functions as dpf\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from kde_estimator import KDEEstimator\n",
    "\n",
    "from pathlib import Path\n",
    "BASE_DIR = os.getcwd()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1356ccc6-3d6d-4e08-bdbc-53a798187d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"cae9773a-f727-4880-8f1c-ac158f5f797c\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"cae9773a-f727-4880-8f1c-ac158f5f797c\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.7.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.7.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.7.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.7.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.7.2.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"cae9773a-f727-4880-8f1c-ac158f5f797c\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.layouts import gridplot\n",
    "\n",
    "import xyzservices.providers as xyz\n",
    "tiles = xyz['USGS']['USTopo']\n",
    "output_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "894b3bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1308 monitored basins in the attribute set.\n"
     ]
    }
   ],
   "source": [
    "# load the catchment characteristics\n",
    "rev_date = '20250227'\n",
    "fname = f'BCUB_watershed_attributes_updated_{rev_date}.csv'\n",
    "attr_df = pd.read_csv(os.path.join('data', fname))\n",
    "attr_df.columns = [c.lower() for c in attr_df.columns]\n",
    "attr_df['log_drainage_area_km2'] = np.log(attr_df['drainage_area_km2'])\n",
    "# attr_df = attr_df[~attr_df['official_id'].isin(exclude)]\n",
    "# attr_df.columns = [c.lower() for c in attr_df.columns]\n",
    "attr_df['tmean'] = (attr_df['tmin'] + attr_df['tmax']) / 2.0\n",
    "station_ids = attr_df['official_id'].values\n",
    "\n",
    "print(f'There are {len(station_ids)} monitored basins in the attribute set.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b463650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Watershed_ID</th>\n",
       "      <th>Source</th>\n",
       "      <th>Name</th>\n",
       "      <th>Official_ID</th>\n",
       "      <th>Centroid_Lat_deg_N</th>\n",
       "      <th>Centroid_Lon_deg_E</th>\n",
       "      <th>Drainage_Area_km2</th>\n",
       "      <th>Drainage_Area_GSIM_km2</th>\n",
       "      <th>Flag_GSIM_boundaries</th>\n",
       "      <th>Flag_Artificial_Boundaries</th>\n",
       "      <th>...</th>\n",
       "      <th>Land_Use_Wetland_frac</th>\n",
       "      <th>Land_Use_Water_frac</th>\n",
       "      <th>Land_Use_Urban_frac</th>\n",
       "      <th>Land_Use_Shrubs_frac</th>\n",
       "      <th>Land_Use_Crops_frac</th>\n",
       "      <th>Land_Use_Snow_Ice_frac</th>\n",
       "      <th>Flag_Land_Use_Extraction</th>\n",
       "      <th>Permeability_logk_m2</th>\n",
       "      <th>Porosity_frac</th>\n",
       "      <th>Flag_Subsoil_Extraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>847</td>\n",
       "      <td>HYDAT</td>\n",
       "      <td>CROWSNEST RIVER AT FRANK</td>\n",
       "      <td>05AA008</td>\n",
       "      <td>49.59732</td>\n",
       "      <td>-114.4106</td>\n",
       "      <td>402.6522</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>0.0785</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>1</td>\n",
       "      <td>-15.543306</td>\n",
       "      <td>0.170479</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>850</td>\n",
       "      <td>HYDAT</td>\n",
       "      <td>CASTLE RIVER NEAR BEAVER MINES</td>\n",
       "      <td>05AA022</td>\n",
       "      <td>49.48866</td>\n",
       "      <td>-114.1444</td>\n",
       "      <td>820.6510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.1156</td>\n",
       "      <td>0.0246</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>-15.929747</td>\n",
       "      <td>0.150196</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Watershed_ID Source                            Name Official_ID  \\\n",
       "846           847  HYDAT        CROWSNEST RIVER AT FRANK     05AA008   \n",
       "849           850  HYDAT  CASTLE RIVER NEAR BEAVER MINES     05AA022   \n",
       "\n",
       "     Centroid_Lat_deg_N  Centroid_Lon_deg_E  Drainage_Area_km2  \\\n",
       "846            49.59732           -114.4106           402.6522   \n",
       "849            49.48866           -114.1444           820.6510   \n",
       "\n",
       "     Drainage_Area_GSIM_km2  Flag_GSIM_boundaries  Flag_Artificial_Boundaries  \\\n",
       "846                     NaN                     0                           0   \n",
       "849                     NaN                     0                           0   \n",
       "\n",
       "     ...  Land_Use_Wetland_frac  Land_Use_Water_frac  Land_Use_Urban_frac  \\\n",
       "846  ...                 0.0103               0.0065               0.0328   \n",
       "849  ...                 0.0058               0.0023               0.0105   \n",
       "\n",
       "     Land_Use_Shrubs_frac  Land_Use_Crops_frac  Land_Use_Snow_Ice_frac  \\\n",
       "846                0.0785               0.0015                  0.0002   \n",
       "849                0.1156               0.0246                  0.0000   \n",
       "\n",
       "     Flag_Land_Use_Extraction  Permeability_logk_m2  Porosity_frac  \\\n",
       "846                         1            -15.543306       0.170479   \n",
       "849                         1            -15.929747       0.150196   \n",
       "\n",
       "     Flag_Subsoil_Extraction  \n",
       "846                        1  \n",
       "849                        1  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# streamflow folder from (updated) HYSETS\n",
    "HYSETS_DIR = Path('/home/danbot2/code_5820/large_sample_hydrology/common_data/HYSETS_data')\n",
    "STREAMFLOW_DIR = HYSETS_DIR / 'streamflow'\n",
    "\n",
    "hs_df = pd.read_csv(HYSETS_DIR / 'HYSETS_watershed_properties.txt', sep=';')\n",
    "hs_df = hs_df[hs_df['Official_ID'].isin(station_ids)]\n",
    "hs_df.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008336e7-cca2-440e-99da-e3919f351793",
   "metadata": {},
   "source": [
    "### Global Uniform Prior\n",
    "\n",
    "$$f(x) = \\frac{1}{b-a}, \\quad x\\in (a, b) \\text{ and } f(x) = 0 \\text{ otherwise.}$$\n",
    "$$\\int_a^b f(x)\\text{dx} = 1$$\n",
    "\n",
    "Given the target range is a sub interval $(c, d) \\subseteq (a, b)$, then the **total** prior probability mass over (c, d) is:\n",
    "\n",
    "$$M_\\text{target} = \\int_c^d \\frac{1}{b-a}\\text{dx} = \\frac{d-c}{b-a}$$\n",
    "\n",
    "Over the set of intervals $\\Delta x_i$ covering the **target range**, the probability mass associated with each interval (bin) is given by: \n",
    "\n",
    "$$\\Delta x_i \\frac{d-c}{b-a}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2c621f-81ff-4b85-a15f-f3b9565ebf90",
   "metadata": {},
   "source": [
    "A desirable property of the prior is that it reflects the strength of belief in the model (data), where a smaller prior reflects stronger belief in the data/model and vice versa.  Dividing by the number of observations has such an effect, however it also makes for very small priors.  The consequence of very small priors is they have negligible effect on models that provide complete support coverage, and they severely penalize models that do not, resulting in a form of instability.  The very small prior creates a heavy tail in the distribution of a large sample of KL divergences, with further downstream effects in optimization.  \n",
    "\n",
    "A method that uses a prior with negligible effect on a model with complete support coverage and a very big effect on one without can be interpreted in a few ways:  \n",
    "\n",
    "1.  Incomplete support coverage, or underspecification, is very heavily penalized.  The method does not tolerate a model that cannot predict the full observed range.\n",
    "2.  A **proper** probability distribution sums (discrete) or integrates (continuous) to 1.  Very small probabilities are in a sense associated with a high degree of certainty since they reflect the expectation of the system being observed in a particular state.\n",
    "3.  The penalty of underestimating a state frequency is that storing and transmitting information about the state requires (the log ratio) more bandwidth/disk space because it is assigned a longer bit string than the actual frequency calls for under optimal encoding.\n",
    "4.  Assigning a very small probability to a state ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a66c9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def compute_overlap_matrix(mask_jax):\n",
    "    return jnp.matmul(mask_jax, mask_jax.T)\n",
    "\n",
    "class FDCEstimationContext:\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "        self._load_catchment_data()\n",
    "        self._load_and_filter_hysets_data()\n",
    "        self.LN_param_dict = self._get_ln_params()\n",
    "        self.id_to_idx, self.idx_to_id = self._set_tree_idx_mappers()\n",
    "        self._build_network_trees()\n",
    "        self._set_attribute_indexers()\n",
    "        self.overlap_dict = self._compute_concurrent_overlap_dict()\n",
    "        \n",
    "    \n",
    "    def _load_and_filter_hysets_data(self):\n",
    "        hs_df = pd.read_csv(HYSETS_DIR / 'HYSETS_watershed_properties.txt', sep=';')\n",
    "        \n",
    "        hs_df = hs_df[hs_df['Official_ID'].isin(self.study_ids)]\n",
    "        self.global_start_date = pd.to_datetime('1950-01-01') # Hysets streamflow starts 1950-01-01\n",
    "        self.hs_df = hs_df\n",
    "        # load the updated HYSETS data\n",
    "\n",
    "        updated_filename = 'HYSETS_2023_update_QC_stations.nc'\n",
    "        ds = xr.open_dataset(HYSETS_DIR / updated_filename)\n",
    "\n",
    "        # Get valid IDs as a NumPy array\n",
    "        selected_ids = hs_df['Watershed_ID'].values\n",
    "\n",
    "        # Get boolean index where watershedID in selected_set\n",
    "        # safely access watershedID as a variable first\n",
    "        ws_ids = ds['watershedID'].data  # or .values if you prefer\n",
    "        mask = np.isin(ws_ids, selected_ids)\n",
    "\n",
    "        # Apply mask to data\n",
    "        ds = ds.sel(watershed=mask)\n",
    "        # Step 1: Promote 'watershedID' to a coordinate on the 'watershed' dimension\n",
    "        ds = ds.assign_coords(watershedID=(\"watershed\", ds[\"watershedID\"].data))\n",
    "\n",
    "        # filter the timeseries by the global start date\n",
    "        ds = ds.sel(time=slice(self.global_start_date, None))\n",
    "\n",
    "        # Step 2: Set 'watershedID' as the index for the 'watershed' dimension\n",
    "        self.ds = ds.set_index(watershed=\"watershedID\")\n",
    "    \n",
    "    \n",
    "    def _load_catchment_data(self):\n",
    "        gdf = gpd.read_file(self.attr_gdf_fpath)\n",
    "        gdf.columns = [c.lower() for c in gdf.columns]\n",
    "\n",
    "        self.study_ids = gdf['official_id'].values\n",
    "        \n",
    "        # import the license water extraction points\n",
    "        dam_gdf = gpd.read_file('data/Dam_Points_20240103.gpkg')\n",
    "        assert gdf.crs == dam_gdf.crs, \"CRS of catchment and dam dataframes do not match\"\n",
    "        joined = gpd.sjoin(gdf, dam_gdf, how=\"inner\", predicate=\"contains\")\n",
    "        joined = joined[joined['official_id'].isin(self.study_ids)]\n",
    "        # Create a new boolean column 'contains_dam' in catchment_gdf.\n",
    "        # If a polygon's index appears in the joined result, it means it contains at least one point.\n",
    "        regulated = joined['official_id'].values\n",
    "        N = len(gdf)\n",
    "        print(f'{len(regulated)}/{N} catchments contain withdrawal licenses')\n",
    "                \n",
    "        # create dict structures for easier access of attributes and geometries\n",
    "        self.da_dict = gdf[['official_id', 'drainage_area_km2']].set_index('official_id').to_dict()['drainage_area_km2']\n",
    "        self.polygon_dict = gdf[['official_id', 'geometry']].set_index('official_id').to_dict()['geometry']\n",
    "        \n",
    "        centroids = gdf.apply(lambda x: self.polygon_dict[x['official_id']].centroid, axis=1)\n",
    "        attr_gdf = gpd.GeoDataFrame(gdf, geometry=centroids, crs=gdf.crs)\n",
    "        attr_gdf[\"contains_dam\"] = attr_gdf['official_id'].apply(lambda x: x in regulated)\n",
    "        attr_gdf.reset_index(inplace=True, drop=True)\n",
    "        self.attr_gdf = attr_gdf\n",
    "\n",
    "\n",
    "    def _build_network_trees(self, attribute_cols=['log_drainage_area_km2', 'elevation_m', 'prcp', 'tmean', 'swe', 'srad',\n",
    "                            'centroid_lon_deg_e', 'centroid_lat_deg_n', 'land_use_forest_frac_2010', 'land_use_snow_ice_frac_2010',\n",
    "                            #  , 'land_use_wetland_frac_2010', 'land_use_water_frac_2010', \n",
    "                            ]):\n",
    "        self.coords = np.array([[geom.x, geom.y] for geom in self.attr_gdf.geometry.values])\n",
    "        self.spatial_tree = cKDTree(self.coords)\n",
    "\n",
    "        # Create mapping from official_id to index\n",
    "        self.id_to_index = {oid: i for i, oid in enumerate(self.attr_gdf[\"official_id\"])}\n",
    "        self.index_to_id = {i: oid for oid, i in self.id_to_index.items()}  # Reverse mapping\n",
    "\n",
    "        # Extract values (excluding 'official_id' since it's categorical)\n",
    "        self.attr_gdf['tmean'] = (self.attr_gdf['tmin'] + self.attr_gdf['tmax']) / 2.0\n",
    "        self.attr_gdf['log_drainage_area_km2'] = np.log(self.attr_gdf['drainage_area_km2'])\n",
    "        attr_values = self.attr_gdf[attribute_cols].to_numpy()\n",
    "        scaler = StandardScaler()\n",
    "        self.normalized_attr_values = scaler.fit_transform(attr_values)\n",
    "        # Construct the KDTree for the attribute space\n",
    "        self.attribute_tree = cKDTree(self.normalized_attr_values)\n",
    "\n",
    "\n",
    "    def _set_tree_idx_mappers(self):\n",
    "        \"\"\"Set the index mappers for the official_id to index and vice versa.\n",
    "        This is for the network TREES\"\"\"\n",
    "        id_to_idx = {id: i for i, id in enumerate(self.attr_gdf['official_id'].values)}\n",
    "        idx_to_id = {i: id for i, id in enumerate(self.attr_gdf['official_id'].values)}\n",
    "\n",
    "        return id_to_idx, idx_to_id\n",
    "    \n",
    "\n",
    "    def _set_attribute_indexers(self):\n",
    "        # map keys to their \n",
    "        # overlap_dict[1].keys()\n",
    "        # create a dictionary where the keys are Watershed_ID and the values are Official_ID\n",
    "        self.watershed_id_dict = {row['Watershed_ID']: row['Official_ID'] for _, row in self.hs_df.iterrows()}\n",
    "        # and the inverse\n",
    "        self.official_id_dict = {row['Official_ID']: row['Watershed_ID'] for _, row in self.hs_df.iterrows()}\n",
    "        # also for drainage areas\n",
    "        self.da_dict = {row['Official_ID']: row['Drainage_Area_km2'] for _, row in self.hs_df.iterrows()}\n",
    "        \n",
    "\n",
    "    def _get_ln_params(self):\n",
    "        \"\"\"Retrieve log-normal parameters for a station.\"\"\"\n",
    "        target_columns = ['mean_uar', 'sd_uar', 'mean_logx', 'sd_logx']\n",
    "\n",
    "        predicted_param_dict = {}\n",
    "        for t in target_columns:\n",
    "            fpath = os.path.join(self.parameter_prediction_results_folder, f'best_out_of_sample_{t}_predictions.csv')\n",
    "            rdf = pd.read_csv(fpath, index_col='official_id')\n",
    "            rdf = rdf[[c for c in rdf.columns if not c.startswith('Unnamed:')]].sort_values('official_id')\n",
    "            predicted_param_dict[t] = rdf.to_dict(orient='index')\n",
    "        return predicted_param_dict\n",
    "    \n",
    "\n",
    "    def _generate_12_month_windows(self, index):\n",
    "        months = pd.date_range(index.min(), index.max(), freq='MS')\n",
    "        windows = [(start, start + pd.DateOffset(months=12) - pd.Timedelta(days=1)) for start in months]\n",
    "        return [w for w in windows if w[1] <= index.max()]\n",
    "    \n",
    "    \n",
    "    def _is_window_valid(self, ts, start, end):\n",
    "        window = ts.loc[start:end]\n",
    "        if window.empty:\n",
    "            return False\n",
    "        grouped = window.groupby(window.index.month).size()\n",
    "        if set(grouped.index) != set(range(1, 13)):\n",
    "            return False\n",
    "        if grouped.min() < 10:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def _compute_station_valid_windows(self, ts, windows):\n",
    "        return [self._is_window_valid(ts, start, end) for (start, end) in windows]\n",
    "    \n",
    "    \n",
    "    def _count_valid_shared_windows(self, valid_i, valid_j):\n",
    "        return sum(np.logical_and(valid_i, valid_j))\n",
    "    \n",
    "    \n",
    "    def _compute_concurrent_overlap_dict(self, variable='discharge'):\n",
    "        \"\"\"\n",
    "        Compute the concurrent overlap of monitored watersheds in the dataset.\n",
    "        Threshold years represent the minimum number of days of overlap \n",
    "        (ignoring continuity) for a watershed to be considered concurrent.\n",
    "        \"\"\"\n",
    "        overlap_dict_file = os.path.join('data', 'record_overlap_dict.json')\n",
    "        if os.path.exists(overlap_dict_file):\n",
    "            with open(overlap_dict_file, 'r') as f:\n",
    "                overlap_dict = json.load(f)\n",
    "            print(f'    ...overlap dict loaded from {overlap_dict_file}')\n",
    "            return overlap_dict\n",
    "        \n",
    "        watershed_ids = self.ds['watershed'].values\n",
    "        data = self.ds[variable].load().values  # (N, T)\n",
    "        threshold_years = self.minimum_concurrent_years\n",
    "\n",
    "        # Compute mask on GPU\n",
    "        M = jnp.asarray(~np.isnan(data), dtype=jnp.uint16) # \n",
    "        O = compute_overlap_matrix(M)\n",
    "\n",
    "        N = M.shape[0] # number of sites\n",
    "        T = M.shape[1] # number of time steps\n",
    "        connectivity_factor = np.sum(O) / float(N**2 * T)\n",
    "        print(f'Connectivity factor: {connectivity_factor:.4f}')\n",
    "\n",
    "        # Build output\n",
    "        N = len(watershed_ids)\n",
    "        print(f'    ...building overlap dict for N={N} monitored watersheds in the network.')\n",
    "        thresholds_days = 365 * np.array(threshold_years) \n",
    "        overlap_dict = {t: {} for t in threshold_years}\n",
    "\n",
    "        for t_years, t_days in zip(threshold_years, thresholds_days):\n",
    "            over_thresh = O >= t_days\n",
    "            over_thresh = over_thresh.at[jnp.diag_indices(N)].set(False)\n",
    "\n",
    "            over_thresh_np = np.array(over_thresh)\n",
    "            for i in range(N):\n",
    "                ws = int(watershed_ids[i])\n",
    "                overlap_dict[t_years][ws] = list(watershed_ids[over_thresh_np[i]])\n",
    "\n",
    "        # Save the overlap dictionary to a JSON file\n",
    "        with open(overlap_dict_file, 'w') as f:\n",
    "            json.dump(overlap_dict, f)\n",
    "        print(f'    ...overlap dict saved to {overlap_dict_file}')\n",
    "\n",
    "        return overlap_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99ede881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class StationData:\n",
    "    def __init__(self, context, stn):\n",
    "        self.ctx = context\n",
    "        self.target_stn = stn\n",
    "        self.attr_gdf = context.attr_gdf\n",
    "        self.LN_param_dict = context.LN_param_dict\n",
    "        self.n_grid_points = context.n_grid_points\n",
    "        # self.catchments = catchments\n",
    "        self.min_flow = context.min_flow # don't allow flows below this value\n",
    "        self.divergence_measures = context.divergence_measures\n",
    "        self.met_forcings_folder = context.LSTM_forcings_folder\n",
    "        self.LSTM_ensemble_result_folder = context.LSTM_ensemble_result_folder\n",
    "        \n",
    "        self.target_da = self.attr_gdf[self.attr_gdf['official_id'] == stn]['drainage_area_km2'].values[0]\n",
    "        self._initialize_target_streamflow_data()\n",
    "        self.global_max, self.global_min = self._set_global_range(epsilon=1e-12)\n",
    "        self._set_grid()\n",
    "        self._set_divergence_measure_functions()\n",
    "    \n",
    "\n",
    "    def retrieve_timeseries_discharge(self, stn):\n",
    "        watershed_id = self.ctx.official_id_dict[stn]\n",
    "        df = self.ctx.ds['discharge'].sel(watershed=str(watershed_id)).to_dataframe(name='discharge').reset_index()\n",
    "        df = df.set_index('time')[['discharge']]\n",
    "        df.dropna(inplace=True)\n",
    "        # clip minimum flow to 1e-4\n",
    "        df['discharge'] = np.clip(df['discharge'], self.ctx.min_flow, None)\n",
    "        df.rename(columns={'discharge': stn}, inplace=True)\n",
    "        df[f'{stn}_uar'] = 1000 * df[stn] / self.ctx.da_dict[stn]      \n",
    "        return df\n",
    "        \n",
    "\n",
    "    def _initialize_target_streamflow_data(self):\n",
    "        # self.stn_df = dpf.get_timeseries_data(self.target_stn)\n",
    "        self.stn_df = self.retrieve_timeseries_discharge(self.target_stn)\n",
    "        self.uar_label = f'{self.target_stn}_uar'\n",
    "        self.n_observations = len(self.stn_df[self.uar_label].dropna())  \n",
    "    \n",
    "\n",
    "    def _set_grid(self):        \n",
    "        # self.baseline_log_grid = np.linspace(np.log(adjusted_min_uar), np.log(max_uar), self.n_grid_points)\n",
    "        self.baseline_log_grid = np.linspace(self.global_min, self.global_max, self.n_grid_points)\n",
    "        self.baseline_lin_grid = np.exp(self.baseline_log_grid)\n",
    "        self.log_dx = np.gradient(self.baseline_log_grid)\n",
    "        # max_step_size = self.baseline_lin_grid[-1] - self.baseline_lin_grid[-2]\n",
    "        # print(f'    max step size = {max_step_size:.1f} L/s/km^2 for n={self.n_grid_points:.1e} grid points')        \n",
    "        \n",
    "        \n",
    "    def _set_global_range(self, xminglobal=1e-1, xmaxglobal=1e5, epsilon=1e-12):\n",
    "        \"\"\"\n",
    "        xminglobal should be expressed in terms that equate to \n",
    "        a minimum flow of 0.1 L/s, or 1e-4 m^3/s must be divided by area to make UAR\n",
    "        NOTE: xmaxglobal is already expressed in unit area (100m^3/s/km^2 = 1e5 L/s/km^2\n",
    "        \"\"\"\n",
    "        # shift the minimum flow by a small amount to force the left interval bound\n",
    "        # to include the assumed global minimum flow (1e-4 m^/3) or 0.1 L/s) \n",
    "        # expressed in unit area terms on a site-specific basis\n",
    "        gmin_uar = (xminglobal - epsilon) / self.target_da\n",
    "        # w_global = np.log(xmaxglobal) - np.log(gmin_uar)\n",
    "        # check to make sure that the maximum UAR contains the max observed value\n",
    "        max_uar = self.stn_df[self.uar_label].max()\n",
    "        assert max_uar <= xmaxglobal, f'max UAR > max global assumption {max_uar:.1e} > {xmaxglobal:.1e}'\n",
    "        return np.log(xmaxglobal), np.log(gmin_uar)\n",
    "    \n",
    "\n",
    "    def _adjust_Q_pdf_with_prior(self, Q, label):\n",
    "        \"\"\"\n",
    "        Adjusts the simulated PDF Q(x), originally defined on x_sim, by incorporating a Dirichlet prior,\n",
    "        to produce an adjusted (posterior) PDF Q_mod on x_target.\n",
    "\n",
    "        \"\"\"\n",
    "        # Ensure the target grid lies within the global range.\n",
    "        # Compute the global log-width.\n",
    "                        \n",
    "        # Convert Q_interp (a PDF) into effective \"counts\" using the number of observations.\n",
    "        # counts_Q = N_obs * Q\n",
    "        \n",
    "        # Convert the prior density into pseudo-counts.\n",
    "        # years_equiv = n_series * (N_obs / 365.25)\n",
    "        # counts_prior = (self.pdf_prior / years_equiv) * dx\n",
    "        # prior_pdf_interp = jnp.interp(self.baseline_log_grid, self.ba, prior_pdf, left=0, right=0)\n",
    "        prior_pseudo_counts = self.knn_simulation_data[label]['prior']\n",
    "        n_observations = self.knn_simulation_data[label]['n_obs']\n",
    "        \n",
    "        # Combine the counts from Q and the prior.\n",
    "        Q_mod = Q * n_observations + prior_pseudo_counts\n",
    "        # Renormalize to obtain the adjusted PDF (discrete PMF) on x_target.\n",
    "        Q_mod /= np.sum(Q_mod)\n",
    "        assert np.all(np.isfinite(Q_mod)), 'Q_mod is messed up'\n",
    "        if not np.min(Q_mod) > 0:\n",
    "            print('Q_mod min:', np.min(Q_mod))\n",
    "            print('Q_mod sum:', np.sum(Q_mod))\n",
    "            print('Q_mod:', Q_mod)\n",
    "            print('Q:', Q)\n",
    "            print('prior_pseudo_counts:', prior_pseudo_counts)\n",
    "            print('n_observations:', n_observations)\n",
    "            Q_mod += 1e-18\n",
    "            Q_mod /= np.sum(Q_mod)\n",
    "            # raise ValueError(f'Q_mod min < 0: {np.min(Q_mod)}')\n",
    "        assert np.min(Q_mod) > 0, f'qmod_i < 0 ({np.min(Q_mod)})'\n",
    "        assert np.isclose(np.sum(Q_mod), 1), f\"Q_mod doesn't sum to 1: {np.sum(Q_mod):.5f}\"\n",
    "\n",
    "        q_mask = (Q > 0)\n",
    "        prior_bias = jnp.sum(jnp.where(q_mask, Q * jnp.log2(Q / Q_mod), 0))\n",
    "        if prior_bias < -0.0001:\n",
    "            prior_pdf = self.knn_simulation_data[label]['prior']\n",
    "            print('prior pmf sum =', np.sum(prior_pdf))\n",
    "            print('Q_sum = ', np.sum(Q))\n",
    "            print('Q_mod sum = ', np.sum(Q_mod))\n",
    "            msg = f'    Prior bias {prior_bias:.3f} bits/sample bias'\n",
    "            raise ValueError(msg)\n",
    "        \n",
    "        return Q_mod\n",
    "    \n",
    "\n",
    "    def _compute_emd(self, p, q, label=None):\n",
    "        assert np.isclose(np.sum(p), 1, atol=1e-3), f'sum P = {np.sum(p)}'\n",
    "        assert np.all(q >= 0), f'min q_i < 0: {np.min(q)}'\n",
    "        emd = wasserstein_distance(self.baseline_log_grid, self.baseline_log_grid, p, q)\n",
    "        return float(round(emd, 3))#, {'bias': None, 'unsupported_mass': None, 'pct_of_signal': None}\n",
    "\n",
    "    \n",
    "    def _compute_kld(self, p, q, label=None):\n",
    "        # assert p and q are 1d\n",
    "        assert jnp.ndim(p) >= 1, f'p is not 1D: {jnp.ndim(p)}'\n",
    "        assert jnp.ndim(q) >= 1, f'q is not 1D: {jnp.ndim(q)}'\n",
    "        # Ensure q is at least 2D for consistent broadcasting\n",
    "        assert jnp.isclose(np.sum(p), 1, atol=1e-3), f'sum P = {np.sum(p)}'\n",
    "        assert jnp.isclose(np.sum(q), 1, atol=1e-3), f'sum Q = {np.sum(q)}'\n",
    "        assert jnp.all(q >= 0), f'min q_i < 0: {np.min(q)}'\n",
    "        assert jnp.all(p >= 0), f'min p_i < 0: {np.min(p)}'\n",
    "        p_mask = (p > 0)\n",
    "        dkl = jnp.sum(jnp.where(p_mask, p * jnp.log2(p / q), 0)).item()\n",
    "        \n",
    "        return round(dkl, 3)\n",
    "\n",
    "\n",
    "    def _set_divergence_measure_functions(self):\n",
    "        self.divergence_functions = {k: None for k in self.divergence_measures}\n",
    "        for dm in self.divergence_measures:\n",
    "            # set the divergence measure functions\n",
    "            if dm == 'DKL':\n",
    "                self.divergence_functions[dm] = self._compute_kld\n",
    "            elif dm == 'EMD':\n",
    "                self.divergence_functions[dm] = self._compute_emd\n",
    "            else:\n",
    "                raise Exception(\"only KL divergence (DKL) and Earth Mover's Distance (EMD) are implemented\")\n",
    "            \n",
    "\n",
    "    def _compute_bias_from_eps(self, pmf: jnp.ndarray, divergence_measure: str, eps: float = 1e-12) -> float:\n",
    "        \"\"\"Compute KL divergence between original PMF and PMF + eps.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pmf : jnp.ndarray\n",
    "            The original PMF (should sum to 1).\n",
    "        eps : float\n",
    "            Small value added to avoid zero bins.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            D_KL(pmf || pmf_eps) representing the bias introduced by smoothing.\n",
    "        \"\"\"\n",
    "        pmf_eps = pmf + eps\n",
    "        pmf_eps /= pmf_eps.sum()\n",
    "        return self.divergence_functions[divergence_measure](pmf, pmf_eps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1e0d15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class kNNFDCEstimator:\n",
    "    def __init__(self, ctx, target_stn, data, *args, **kwargs):\n",
    "        self.ctx = ctx\n",
    "        self.target_stn = target_stn\n",
    "        self.data = data\n",
    "        self.k_nearest = data.k_nearest\n",
    "        self.weight_schemes = ['ID1', 'ID2'] #inverse distance and inverse square distance\n",
    "        self.knn_simulation_data = {}\n",
    "        self.knn_pdfs = pd.DataFrame()\n",
    "        self.knn_pmfs = pd.DataFrame()\n",
    "\n",
    "\n",
    "    def _find_k_nearest_neighbors(self, target_index, tree_type, overlapping_tree_idxs):\n",
    "        # Query the k+1 nearest neighbors because the first neighbor is the target point itself\n",
    "\n",
    "        if len(overlapping_tree_idxs) >= 10: \n",
    "            max_search = len(overlapping_tree_idxs)\n",
    "        else:\n",
    "            max_search = self.data.max_to_check\n",
    "\n",
    "        if tree_type == 'spatial_dist':\n",
    "            distances, indices = self.ctx.spatial_tree.query(self.ctx.coords[target_index], k=max_search+1)\n",
    "            distances *= 1 / 1000\n",
    "        elif tree_type == 'attribute_dist':\n",
    "            # Example query: Find the nearest neighbors for the first point\n",
    "            distances, indices = self.ctx.attribute_tree.query(self.ctx.normalized_attr_values[target_index], k=max_search+1)\n",
    "        else:\n",
    "            raise Exception('tree type not identified, must be one of spatial_dist, or attribute_dist.')\n",
    "        \n",
    "        # Remove target (self) from the results\n",
    "        self_index = target_index\n",
    "        keep = indices != self_index\n",
    "        indices = indices[keep]\n",
    "        distances = distances[keep]\n",
    "\n",
    "        # Filter by the pre-processed overlapping tree indices\n",
    "        if len(overlapping_tree_idxs) >= 10:\n",
    "            overlap_set = set(overlapping_tree_idxs)\n",
    "            filtered = [(i, d) for i, d in zip(indices, distances) if i in overlap_set]\n",
    "        else:\n",
    "            filtered = list(zip(indices, distances))\n",
    "\n",
    "        neighbour_indices, neighbour_distances = zip(*filtered)\n",
    "        return np.array(neighbour_indices), np.round(np.array(neighbour_distances), 3)\n",
    "    \n",
    "\n",
    "    def _check_time_series_completeness(self, timeseries):\n",
    "        df = timeseries.copy().dropna(subset=[c for c in timeseries.columns if c.endswith('_uar')])\n",
    "        # print(f'           After dropping len={len(df)}, {df.index.min()} - {df.index.max()}')\n",
    "        # compute the total number of observations per month\n",
    "        # we want to ensure the simulation period is not seasonal\n",
    "        if df.empty:\n",
    "            return False\n",
    "        df['month'] = df.index.month\n",
    "        days_by_month = df.groupby(['month']).size()\n",
    "        # Check that all 12 months are present\n",
    "        if set(days_by_month.index) != set(range(1, 13)):\n",
    "            print('    Not all months represented year.  Skipping.')\n",
    "            return False\n",
    "        min_days = days_by_month.min()\n",
    "        # Check that every month has at least 10 unique days\n",
    "        if np.any(min_days < 10):\n",
    "            print('    At least one month is not represented by at least ten days, skipping comparison.')\n",
    "            return False\n",
    "        # count the number of complete years of record\n",
    "        return True\n",
    "\n",
    "\n",
    "    def _compute_effective_k(self, df, max_k=None):\n",
    "        arr = df.to_numpy()\n",
    "        T, K = arr.shape\n",
    "        max_k = max_k or K\n",
    "\n",
    "        nan_mask = np.isnan(arr)\n",
    "        sorted_idx = np.argsort(nan_mask, axis=1)\n",
    "        row_idx = np.arange(T)[:, None]\n",
    "\n",
    "        ks = np.arange(1, max_k + 1)\n",
    "        effective_k = []\n",
    "        mean_furthest = []\n",
    "\n",
    "        for k in ks:\n",
    "            idx = sorted_idx[:, :k]\n",
    "            valid = ~nan_mask[row_idx, idx]\n",
    "            valid_count = valid.sum(axis=1)\n",
    "\n",
    "            effective_k.append(valid_count.mean())\n",
    "            furthest_idx = np.where(valid, idx, -1).max(axis=1)\n",
    "            mean_furthest.append(furthest_idx[valid_count >= k].mean() if np.any(valid_count >= k) else np.nan)\n",
    "\n",
    "        return pd.DataFrame({\n",
    "            'effective_k': np.round(effective_k, 1),\n",
    "            'mean_furthest_idx': np.round(mean_furthest, 2)\n",
    "        }, index=ks)\n",
    "\n",
    "        \n",
    "    def _check_neighbours(self, stn, distance, concurrent, target_df, min_years=5):\n",
    "        # proxy_df = dpf.get_timeseries_data(stn)\n",
    "        proxy_df = self.data.retrieve_timeseries_discharge(stn)[[f'{stn}_uar']]\n",
    "        if concurrent == 'concurrent':\n",
    "            proxy_df = proxy_df.reindex(target_df.index)\n",
    "\n",
    "        full_year_represented = self._check_time_series_completeness(proxy_df)\n",
    "        n_years = proxy_df.dropna().shape[0] // 365\n",
    "        if n_years < min_years or not full_year_represented:\n",
    "            print(f'    Skipping {stn}: <1 year or incomplete.')\n",
    "            return None\n",
    "        return [stn, round(distance, 3), n_years, proxy_df]\n",
    "    \n",
    "    \n",
    "    def _query_distance(self, tree, id1, id2):\n",
    "        \"\"\"Query distance between two points in a tree using official_id.\"\"\"\n",
    "        if id1 not in self.ctx.id_to_index or id2 not in self.ctx.id_to_index:\n",
    "            raise ValueError(f\"One or both IDs ({id1}, {id2}) not found.\")\n",
    "    \n",
    "        # Get indices from ID mapping\n",
    "        index1, index2 = self.ctx.id_to_index[id1], self.ctx.id_to_index[id2]        \n",
    "        # Query the distance\n",
    "        distance = np.linalg.norm(tree.data[index1] - tree.data[index2])  # Euclidean distance\n",
    "        return distance\n",
    "    \n",
    "\n",
    "    def _retrieve_nearest_nbr_data(self, tree_type, concurrent, min_concurrent_years):\n",
    "        tree_idx = self.ctx.id_to_index[self.target_stn]\n",
    "        # get the pre-screened concurrent stations\n",
    "        target_ws_id = self.ctx.official_id_dict[self.target_stn]\n",
    "\n",
    "        max_nbrs = self.data.max_to_check\n",
    "\n",
    "        # use the pre-computed overlap dictionary to find the concurrent stations\n",
    "        overlapping_tree_idxs = []\n",
    "        if concurrent == 'concurrent':\n",
    "            assert min_concurrent_years > 0, f'concurrent min years must be > 0, not {min_concurrent_years}'\n",
    "            overlapping_ws_ids = self.ctx.overlap_dict[min_concurrent_years].get(target_ws_id, [])\n",
    "            overlapping_stn_official_ids = [self.ctx.watershed_id_dict[e] for e in overlapping_ws_ids]\n",
    "\n",
    "            existing_keys = [e for e in overlapping_stn_official_ids if e in self.ctx.id_to_index]\n",
    "            overlapping_tree_idxs = [self.ctx.id_to_index[e] for e in existing_keys]\n",
    "            overlapping_tree_idxs = [e for e in overlapping_tree_idxs if e is not None]\n",
    "\n",
    "        neighbour_idxs, distances = self._find_k_nearest_neighbors(tree_idx, tree_type, overlapping_tree_idxs)\n",
    "        neighbours = [self.ctx.index_to_id[e] for e in neighbour_idxs.tolist()][:max_nbrs]\n",
    "\n",
    "        assert self.target_stn not in neighbours, f'{self.target_stn} is in the list of neighbours: {neighbours}'\n",
    "        concurrent_inputs = [concurrent] * len(neighbours)\n",
    "        df_inputs = [self.data.stn_df.copy()]*len(neighbours)\n",
    "\n",
    "        # don't need to recheck neighbours\n",
    "        with ThreadPoolExecutor(max_workers=20) as executor:  # Adjust max workers\n",
    "            checked_neighbours = list(executor.map(self._check_neighbours, neighbours, distances, concurrent_inputs, df_inputs))\n",
    "        \n",
    "        good_nbrs = [e for e in checked_neighbours if e is not None]\n",
    "        print(f'     ...found {len(good_nbrs)} ({tree_type} tree) neighbours ({concurrent}) for {self.target_stn} from {max_nbrs} nearest neighbours')\n",
    "        if len(good_nbrs) < 10:\n",
    "            print('   need', 10 - len(good_nbrs), '  more good records')\n",
    "            self.data.max_to_check += 50\n",
    "            max_nbrs = self.data.max_to_check\n",
    "            if max_nbrs <= 600:\n",
    "                return self._retrieve_nearest_nbr_data(tree_type, concurrent, min_concurrent_years)\n",
    "            else:\n",
    "                raise Exception(f'{len(good_nbrs)}/10 suitable nearest neighbours found ({max_nbrs} searched)')\n",
    "\n",
    "        complement_dist = 'attribute_dist' if tree_type == 'spatial_dist' else 'spatial_dist' \n",
    "        # set the OPPOSITE tree to the current tree type\n",
    "        complement_tree = self.ctx.attribute_tree if tree_type == 'spatial_dist' else self.ctx.spatial_tree \n",
    "        # the multiplier should reflect conversion to km from m for spatial distance \n",
    "        multiplier = 1 / 1000 if complement_dist == 'spatial_dist' else 1 \n",
    "\n",
    "        # sort the neighbours by distance\n",
    "        # output is ['official_id', 'distance', 'n_years', 'timeseries df']\n",
    "        good_nbrs = sorted(good_nbrs, key=lambda tup: tup[1])        \n",
    "        nbr_df = pd.concat([e[3] for e in good_nbrs], join='outer', axis=1)\n",
    "\n",
    "        # Align with target index if concurrent, leave as is if async.\n",
    "        if concurrent == 'concurrent':\n",
    "            target_series = self.data.stn_df.dropna(subset=[f'{self.target_stn}_uar'])\n",
    "            nbr_df = nbr_df.reindex(index=target_series.index)\n",
    "\n",
    "        nbr_data = pd.DataFrame([e[:3] for e in good_nbrs], columns=['official_id', 'distance', 'n_years'])  \n",
    "        nbr_data[complement_dist] = (nbr_data['official_id'].apply(lambda x: self._query_distance(complement_tree, x, self.target_stn)) * multiplier).round(3)\n",
    "\n",
    "        effective_k = self._compute_effective_k(nbr_df)\n",
    "        return nbr_df, nbr_data, effective_k\n",
    "\n",
    "\n",
    "    def _contributing_ensemble_check(self, nbr_data, min_years, previous_ids, min_years_prev):\n",
    "        current_ids = set([c.split('_')[0] for c in nbr_data.columns])\n",
    "        if previous_ids is not None:\n",
    "            if current_ids == previous_ids:\n",
    "                print(f\"    No change in neighbor set between {min_years_prev} and {min_years} years.\")\n",
    "            else:\n",
    "                added = current_ids - previous_ids\n",
    "                removed = previous_ids - current_ids\n",
    "                print(f\"    Change detected for min_concurrent_years {min_years_prev} â†’ {min_years}:\")\n",
    "                if added:\n",
    "                    print(f\"    + Added:   {sorted(added)}\")\n",
    "                if removed:\n",
    "                    print(f\"    - Removed: {sorted(removed)}\")\n",
    "\n",
    "        previous_ids = current_ids\n",
    "        min_years_prev = min_years\n",
    "        return previous_ids, min_years_prev\n",
    "    \n",
    "    \n",
    "    def _initialize_nearest_neighbour_data(self):\n",
    "        \"\"\"\n",
    "        Generate nearest neighbours for spatial and attribute selected k-nearest neighbours for both concurrent and asynchronous records.\n",
    "        \"\"\"\n",
    "        print(f'    ...initializing {self.data.max_to_check} nearest neighbours with minimum concurrent record.')\n",
    "        self.nbr_dfs = defaultdict(lambda: defaultdict(dict))\n",
    "        \n",
    "        for tree_type in ['spatial_dist', 'attribute_dist']:\n",
    "            for concurrent in ['concurrent', 'async']:\n",
    "                if (self.ctx.LSTM_concurrent_network is False) & (concurrent == 'concurrent'):\n",
    "                    print('    Skipping concurrent check for non-LSTM concurrent network.')\n",
    "                    continue\n",
    "\n",
    "                if concurrent == 'async':\n",
    "                    min_concurrent_years = [0]\n",
    "                else:\n",
    "                    min_concurrent_years = self.ctx.minimum_concurrent_years\n",
    "                previous_ids, min_years_prev = None, None\n",
    "                for min_overlap in min_concurrent_years:\n",
    "                    nbr_df, nbr_data, effective_k = self._retrieve_nearest_nbr_data(tree_type, concurrent, min_overlap)\n",
    "                    previous_ids, min_years_prev = self._contributing_ensemble_check(nbr_df, min_overlap, previous_ids, min_years_prev)\n",
    "                    assert not nbr_df.empty, f'{tree_type} attr concurrent={concurrent} nbr df empty'\n",
    "                    print('       ', tree_type, concurrent, min_overlap, len(nbr_df))\n",
    "                    self.nbr_dfs[tree_type][concurrent][min_overlap] = {\n",
    "                        'nbr_df': nbr_df,\n",
    "                        'nbr_data': nbr_data,\n",
    "                        'effective_k': effective_k\n",
    "                    }\n",
    "    \n",
    "    \n",
    "    def _run_spatial_knn(self, num_neighbours, time_type, weight_scheme):\n",
    "        target_idx = self.ctx.id_to_idx[self.target_stn]\n",
    "        \n",
    "        for k in range(1, num_neighbours+1):\n",
    "            sim_label = f'{k}NN_{time_type}_{weight_scheme}'\n",
    "\n",
    "    \n",
    "    def _compute_weights(self, m, k, distances, epsilon=1e-3):\n",
    "        \"\"\"Compute normalized inverse (square) distance weights to a given power.\"\"\"\n",
    "\n",
    "        distances = jnp.where(distances == 0, epsilon, distances)\n",
    "\n",
    "        if m == 'ID1':\n",
    "            power = 1.0\n",
    "        elif m == 'ID2':\n",
    "            power = 2.0\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown weighting method: {m}.  Only ID1 (inverse distance) or ID2 (inverse square distance) are implemented.\")\n",
    "        \n",
    "        if k == 1:\n",
    "            return jnp.array([1])\n",
    "        else:\n",
    "            inv_weights = 1 / (jnp.abs(distances) ** power)\n",
    "            return inv_weights / jnp.sum(inv_weights)\n",
    "    \n",
    "    \n",
    "    def _compute_prior_from_laplace_fit(self, predicted_uar, n_cols=1, min_prior=1e-10, scale_factor=1.05, recursion_depth=0, max_depth=100):\n",
    "        \"\"\"\n",
    "        Fit a Laplace distribution to the simulation and define a \n",
    "        pdf across a pre-determined \"global\" range to avoid data\n",
    "        leakage.  \"Normalize\" by setting the total prior mass to\n",
    "        integrate to a factor related to the number of observations.\n",
    "        \"\"\"\n",
    "        # assert no nan values\n",
    "        assert np.isfinite(predicted_uar).all(), f'NaN values in predicted_uar: {predicted_uar}'\n",
    "        # assert all positive values\n",
    "        # assert np.all(predicted_uar > 0), f'Negative values in predicted_uar: {np.min(predicted_uar)}'\n",
    "        # replace anything <= 0 with 1e-4 scaled by the drainage area\n",
    "        predicted_uar = np.where(predicted_uar <= 0, 1000 * 1e-4 / self.data.target_da, predicted_uar)\n",
    "        assert np.all(predicted_uar > 0), f'Negative values in predicted_uar: {np.min(predicted_uar)}'\n",
    "        # print('min/max: ', np.min(predicted_uar), np.max(predicted_uar))\n",
    "        loc, scale = laplace.fit(np.log(predicted_uar))       \n",
    "\n",
    "        # Apply scale factor in case of recursion\n",
    "        if scale <= 0:\n",
    "            original_scale = scale\n",
    "            scale = scale_factor ** recursion_depth\n",
    "            print(f'   Adjusting scale from {original_scale:.3f} to {scale:.3f} for recursion depth {recursion_depth}')\n",
    "\n",
    "        prior_pdf = laplace.pdf(self.data.baseline_log_grid, loc=loc, scale=scale)\n",
    "        prior_check = jnp.trapezoid(prior_pdf, x=self.data.baseline_log_grid)\n",
    "        prior_pdf /= prior_check\n",
    "\n",
    "        # Check for zeros\n",
    "        if np.any(prior_pdf == 0) | np.any(np.isnan(prior_pdf)):\n",
    "            # Prevent scale from being too small\n",
    "            if recursion_depth >= max_depth:\n",
    "                # set a very small prior\n",
    "                prior_pdf = np.ones_like(self.data.baseline_log_grid)\n",
    "                err_msg = f\"Recursion limit reached. Scale={scale}, setting default prior to 1 pseudo-count uniform distribution.\"\n",
    "                print(err_msg)\n",
    "                return prior_pdf\n",
    "                # raise ValueError(err_msg)\n",
    "            # print(f\"Recursion {recursion_depth}: Zero values detected. Increasing scale to {scale:.6f}\")\n",
    "            return self._compute_prior_from_laplace_fit(predicted_uar, n_cols=n_cols, recursion_depth=recursion_depth + 1)\n",
    "        \n",
    "        second_check = jnp.trapezoid(prior_pdf, x=self.data.baseline_log_grid)\n",
    "        assert np.isclose(second_check, 1, atol=2e-4), f'prior check != 1, {second_check:.6f} N={len(predicted_uar)} {predicted_uar}'\n",
    "        assert np.min(prior_pdf) > 0, f'min prior == 0, scale={scale:.5f}'\n",
    "\n",
    "        # convert prior PDF to PMF (pseudo-count mass function)\n",
    "        prior_pmf = prior_pdf * self.data.log_dx\n",
    "\n",
    "        # scale the number of pseudo-counts based on years of record  (365 / n_observations)\n",
    "        # and number of models in the ensemble (given by n_cols)\n",
    "        prior_pseudo_counts = prior_pmf * (365 / (len(predicted_uar) * n_cols))\n",
    "        \n",
    "        # return weighted_prior_pdf\n",
    "        return prior_pseudo_counts\n",
    "    \n",
    "\n",
    "    def _compute_frequency_ensemble_mean(self, pdfs, weights):\n",
    "        \"\"\"\n",
    "        This function computes the weighted ensemble distribution estimates.\n",
    "        \"\"\"\n",
    "        # Normalize distance weights\n",
    "        if weights is not None:\n",
    "            weights /= jnp.sum(weights).astype(float)\n",
    "            weights = jnp.array(weights)  # Ensure 1D array\n",
    "            pdf_est = jnp.asarray(pdfs.to_numpy() @ weights)\n",
    "        else:\n",
    "            pdf_est = jnp.asarray(pdfs.mean(axis=1).to_numpy())\n",
    "\n",
    "\n",
    "        # Check integral before normalization\n",
    "        pdf_check = jnp.trapezoid(pdf_est, x=self.data.baseline_log_grid)\n",
    "        normalized_pdf = pdf_est / pdf_check\n",
    "        assert jnp.isclose(jnp.trapezoid(normalized_pdf, x=self.data.baseline_log_grid), 1), f'ensemble pdf does not integrate to 1: {pdf_check:.4f}'\n",
    "                \n",
    "        # Compute PMF\n",
    "        pmf_est = normalized_pdf * self.data.log_dx\n",
    "        pmf_est /= jnp.sum(pmf_est)\n",
    "\n",
    "        return pmf_est, pdf_est\n",
    "    \n",
    "\n",
    "    def _compute_ensemble_member_distribution_estimates(self, df):\n",
    "        \"\"\"\n",
    "        Compute the ensemble distribution estimates based on the KNN dataframe.\n",
    "        \"\"\"    \n",
    "        pdfs, prior_biases = pd.DataFrame(), {}\n",
    "        # initialize a kde estimator object\n",
    "        kde = KDEEstimator(self.data.baseline_log_grid, self.data.log_dx)\n",
    "        for c in df.columns: \n",
    "            # evaluate the laplace on the prediction as a prior\n",
    "            # drop the nan values\n",
    "            values = df[c].dropna().values\n",
    "            obs_count = len(values)\n",
    "            assert len(values) > 0, f'0 values for {c}'\n",
    "\n",
    "            # compute the pdf and pmf using kde\n",
    "            assert sum(np.isnan(values)) == 0, f'NaN values in {c} {values[:5]}'\n",
    "\n",
    "            kde_pmf, _ = kde.compute(\n",
    "                values, self.data.target_da\n",
    "            )\n",
    "\n",
    "            prior = self._compute_prior_from_laplace_fit(values, n_cols=1) # priors are expressed in pseudo-counts\n",
    "            # convert the pdf to counts and apply the prior\n",
    "            counts = kde_pmf * obs_count + prior\n",
    "\n",
    "            # re-normalize the pmf\n",
    "            pmf = counts / jnp.sum(counts)\n",
    "            pdf = pmf / self.data.log_dx\n",
    "\n",
    "            pdf_check = jnp.trapezoid(pdf, x=self.data.baseline_log_grid)\n",
    "            pdf /= pdf_check\n",
    "            # pdf /= pdf_check\n",
    "            assert jnp.isclose(jnp.trapezoid(pdf, x=self.data.baseline_log_grid), 1.0, atol=0.001), f'pdf does not integrate to 1 in compute_ensemble_member_distribution_estimates: {pdf_check:.4f}'\n",
    "            pdfs[c] = pdf\n",
    "\n",
    "            # convert the pdf to pmf\n",
    "            pmf = pdf * self.data.log_dx\n",
    "            pmf /= jnp.sum(pmf)\n",
    "            # assert np.isclose(np.sum(pmf), 1, atol=1e-4), f'pmf does not sum to 1 in compute_ensemble_member_distribution_estimates: {np.sum(pmf):.5f}'\n",
    "            \n",
    "            # compute the bias added by the prior\n",
    "            prior_biases[c.split('_')[0]] = {'DKL': self.data._compute_kld(kde_pmf, pmf), 'EMD': self.data._compute_emd(kde_pmf, pmf)}\n",
    "        return pdfs, prior_biases\n",
    "    \n",
    "    \n",
    "    def _compute_frequency_ensemble_distribution(self, knn_df_all, knn_data_all, distance_type, concurrent, ensemble_pdfs, min_overlap=0):\n",
    "        \"\"\"\n",
    "        For asynchronous comparisons, we estimate pdfs for ensemble members, then compute the mean in the time domain\n",
    "        to represent the FDC simulation.  We do not do temporal averaging in this case.\n",
    "        Default min_overlap is 0 for asynchronous comparisons.\n",
    "        \"\"\"\n",
    "        # distances_all = knn_data_all['distance'].values[:self.k_nearest]\n",
    "        # nbr_ids_all = knn_data_all['official_id'].values[:self.k_nearest]\n",
    "        \n",
    "        # distances = jnp.array(nbr_data['distance'].astype(float).values)\n",
    "        labels, pdfs, pmfs = [], [], []\n",
    "        all_distances = knn_data_all['distance'].values\n",
    "        all_ids = knn_data_all['official_id'].values\n",
    "        # prior_bias_df = pd.DataFrame(prior_bias_dict)\n",
    "        for wm in self.weight_schemes:\n",
    "            for k in range(1, self.k_nearest + 1):\n",
    "                distances = all_distances[:k]\n",
    "                nbr_ids = all_ids[:k]\n",
    "                knn_pdfs = ensemble_pdfs.iloc[:, :k].copy()\n",
    "\n",
    "                label = f'{self.target_stn}_{k}_NN_{min_overlap}_minYears_{concurrent}_{distance_type}_{wm}_freqEnsemble'\n",
    "                weights = self._compute_weights(wm, k, distances)\n",
    "                pmf_est, pdf_est = self._compute_frequency_ensemble_mean(knn_pdfs, weights)\n",
    "                assert pmf_est is not None, f'pmf_est is None for {label}'\n",
    "            \n",
    "                # compute the mean number of observations (non-nan values) per row\n",
    "                mean_obs_per_timestep = knn_df_all.iloc[:, :k].notna().sum(axis=1).mean()\n",
    "                mean_obs_per_proxy = knn_df_all.iloc[:, :k].notna().sum(axis=0).mean()\n",
    "\n",
    "                # max_prior_bias = prior_bias_df.iloc[:k].max(axis=0)                \n",
    "                # compute the frequency-based ensemble pdf estimate\n",
    "                self.knn_simulation_data[label] = {'k': k, 'n_obs': mean_obs_per_proxy,\n",
    "                                                'mean_obs_per_timestep': mean_obs_per_timestep,\n",
    "                                                'nbrs': ','.join(nbr_ids)}\n",
    "\n",
    "                self.knn_simulation_data[label]['DKL'] = self.data._compute_kld(self.ctx.baseline_pmf, pmf_est)\n",
    "                self.knn_simulation_data[label]['EMD'] = self.data._compute_emd(self.ctx.baseline_pmf, pmf_est)\n",
    "\n",
    "                # print(k, wm, label, self.knn_simulation_data[label]['DKL'])\n",
    "                \n",
    "                pdfs.append(np.asarray(pdf_est))\n",
    "                pmfs.append(np.asarray(pmf_est))\n",
    "                labels.append(label)\n",
    "\n",
    "        # create a dataframe of labels(columns) for each pdf\n",
    "        knn_pdfs = pd.DataFrame(pdfs, index=labels).T\n",
    "        knn_pmfs = pd.DataFrame(pmfs, index=labels).T\n",
    "        # Filter out already existing columns to avoid duplication\n",
    "        new_pdf_cols = knn_pdfs.columns.difference(self.knn_pdfs.columns)\n",
    "        new_pmf_cols = knn_pmfs.columns.difference(self.knn_pmfs.columns)\n",
    "        # Concat only new columns\n",
    "        self.knn_pdfs = pd.concat([self.knn_pdfs, knn_pdfs[new_pdf_cols]], axis=1)\n",
    "        self.knn_pmfs = pd.concat([self.knn_pmfs, knn_pmfs[new_pmf_cols]], axis=1)\n",
    "    \n",
    "    \n",
    "    def _run_async_knn(self, dist):\n",
    "        min_overlap = 0\n",
    "        nbr_df = self.nbr_dfs[dist]['async'][min_overlap]['nbr_df'].copy()\n",
    "        nbr_data = self.nbr_dfs[dist]['async'][min_overlap]['nbr_data'].copy()\n",
    "        knn_df_all = nbr_df.iloc[:, :self.k_nearest].copy()\n",
    "        knn_data_all = nbr_data.iloc[:, :self.k_nearest].copy()\n",
    "        frequency_ensemble_pdfs, _ = self._compute_ensemble_member_distribution_estimates(knn_df_all)\n",
    "        self._compute_frequency_ensemble_distribution(knn_df_all, knn_data_all, dist, 'async', frequency_ensemble_pdfs)\n",
    "\n",
    "    \n",
    "    def _delta_spike_pmf_pdf(self, single_val, log_grid):\n",
    "        \"\"\"\n",
    "        Return a spike PMF and compatible PDF centered at the only value in the input.\n",
    "        The spike is placed at the nearest log_grid point to log(single_val).\n",
    "        \"\"\"\n",
    "        log_val = jnp.log(single_val)\n",
    "        spike_idx = jnp.argmin(jnp.abs(log_grid - log_val))\n",
    "        \n",
    "        pmf = jnp.zeros_like(log_grid)\n",
    "        pmf = pmf.at[spike_idx].set(1.0)\n",
    "\n",
    "        dx = jnp.gradient(log_grid)\n",
    "        pdf = pmf / dx  # assign all mass to one bin\n",
    "\n",
    "        return pmf, pdf\n",
    "\n",
    "    \n",
    "    def _compute_nse(self, obs, sim):\n",
    "        \"\"\"Compute the Nash-Sutcliffe Efficiency (NSE) between observed and simulated values.\"\"\"\n",
    "        assert not np.isnan(obs).any(), f'NaN values in obs: {obs}'\n",
    "        assert not np.isnan(sim).any(), f'NaN values in sim: {sim}'\n",
    "        assert (obs >= 0).all(), f'Negative values in obs: {obs}'\n",
    "        assert (sim >= 0).all(), f'Negative values in sim: {sim}'\n",
    "        # Compute the NSE\n",
    "        numerator = jnp.sum((obs - sim) ** 2)\n",
    "        denominator = jnp.sum((obs - obs.mean()) ** 2)\n",
    "        nse = 1 - (numerator / denominator)\n",
    "        return nse\n",
    "\n",
    "\n",
    "    def _compute_KGE(self, obs, sim):\n",
    "        \"\"\"Compute the Kling-Gupta Efficiency (KGE) between observed and simulated values.\"\"\"\n",
    "        assert not np.isnan(obs).any(), f'NaN values in obs: {obs}'\n",
    "        assert not np.isnan(sim).any(), f'NaN values in sim: {sim}'\n",
    "        assert (obs >= 0).all(), f'Negative values in obs: {obs}'\n",
    "        assert (sim >= 0).all(), f'Negative values in sim: {sim}'\n",
    "        # Compute the KGE\n",
    "        r = jnp.corrcoef(obs, sim)[0, 1]\n",
    "        alpha = sim.mean() / obs.mean()\n",
    "        beta = sim.std() / obs.std()\n",
    "        kge = 1 - jnp.sqrt((r - 1) ** 2 + (alpha - 1) ** 2 + (beta - 1) ** 2)\n",
    "        return kge\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _compute_ensemble_contribution_metrics(self, df: pd.DataFrame, weights: np.ndarray):\n",
    "        mask = ~df.isna()\n",
    "        \n",
    "        # Mean number of valid values per row\n",
    "        mean_valid_per_row = mask.sum(axis=1).mean()\n",
    "\n",
    "        # Normalized weights per row, masking NaNs\n",
    "        X = df.to_numpy()\n",
    "        W = np.broadcast_to(weights, X.shape)\n",
    "        masked_weights = np.where(mask, W, 0.0)\n",
    "        weight_sums = masked_weights.sum(axis=1)\n",
    "        weight_sums[weight_sums == 0] = np.nan\n",
    "        normalized_weights = masked_weights / weight_sums[:, None]\n",
    "\n",
    "        # Average contribution per column across all rows\n",
    "        mean_w = np.nanmean(normalized_weights, axis=0)\n",
    "        effective_n = 1.0 / np.nansum(mean_w ** 2)\n",
    "\n",
    "        return mean_valid_per_row, effective_n\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    def run_estimators(self, divergence_measures, eps, baseline_pmf):\n",
    "        self._initialize_nearest_neighbour_data()\n",
    "        \n",
    "        for dist in ['spatial_dist', 'attribute_dist']:            \n",
    "            self._run_async_knn(dist) \n",
    "            if self.ctx.LSTM_concurrent_network is False:\n",
    "                print('    Skipping concurrent check for non-LSTM concurrent network.')\n",
    "                continue \n",
    "            for min_concurrent_years in self.ctx.minimum_concurrent_years[::-1]: # go from most to least minimum required concurrent years \n",
    "                self._initialize_concurrent_ensemble_inputs(dist, min_concurrent_years)\n",
    "        return self._process_results()\n",
    "\n",
    "    \n",
    "    def _make_json_serializable(self, d):\n",
    "        output = {}\n",
    "        for k, v in d.items():\n",
    "            if isinstance(v, (np.ndarray, jnp.ndarray)):\n",
    "                output[k] = v.tolist()\n",
    "            elif hasattr(v, \"tolist\"):\n",
    "                output[k] = v.tolist()\n",
    "            else:\n",
    "                output[k] = v\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def _process_results(self):        \n",
    "        pmf_labels, pdf_labels, sim_labels = list(self.knn_pmfs.columns), list(self.knn_pdfs.columns), list(self.knn_simulation_data.keys())\n",
    "        # assert label sets are the same\n",
    "        assert set(pmf_labels) == set(pdf_labels), f'pmf_labels {pmf_labels} != pdf_labels {pdf_labels}'\n",
    "        assert set(pmf_labels) == set(sim_labels), f'pmf_labels {pmf_labels} != sim_labels {sim_labels}'\n",
    "        results = self.knn_simulation_data\n",
    "        for label in pmf_labels:\n",
    "            # add the pmf and pdf in a json serializable format\n",
    "            results[label]['pmf'] = self.knn_pmfs[label].tolist()\n",
    "            results[label]['pdf'] = self.knn_pdfs[label].tolist()\n",
    "            results[label] = self._make_json_serializable(results[label])\n",
    "        return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46b4bbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFEstimatorRunner:\n",
    "    def __init__(self, stn_id, ctx, methods, k_nearest, max_to_check, **kwargs):\n",
    "        self.stn_id = stn_id\n",
    "        self.ctx = ctx\n",
    "        self.methods = methods\n",
    "        self.k_nearest = k_nearest\n",
    "        self.max_to_check = max_to_check\n",
    "        self._check_min_overlap()\n",
    "        self._create_results_folders()\n",
    "\n",
    "\n",
    "    def _create_results_folders(self):\n",
    "        # create a results foder for each method if it doesn't exist\n",
    "        self.results_folder = os.path.join('data', f'pdf_estimates',)\n",
    "        os.makedirs(self.results_folder)\n",
    "\n",
    "\n",
    "    def _process_ground_truth(self):\n",
    "        self.kde = KDEEstimator(self.data.baseline_log_grid, self.data.log_dx)\n",
    "        self.baseline_pmf, self.baseline_pdf = self.kde.compute(\n",
    "            self.data.stn_df[self.data.uar_label].values, self.data.target_da\n",
    "        )\n",
    "        self.ctx.baseline_pmf = self.baseline_pmf\n",
    "\n",
    "\n",
    "    def _save_result(self, result):\n",
    "        with open(self.result_file, 'w') as file:\n",
    "            json.dump(result, file, indent=4)\n",
    "\n",
    "\n",
    "    def _check_min_overlap(self):\n",
    "        for min_years, cdict in self.ctx.overlap_dict.items():\n",
    "            min_concurrent_stns = 1e6\n",
    "            n_less_than_ten, lonely_stns = 0, []\n",
    "            print(f'Processing {min_years} years of concurrent record')\n",
    "            for stn, concurrent_ids in cdict.items():\n",
    "                n_stns = len(concurrent_ids)\n",
    "                if n_stns < 10:\n",
    "                    n_less_than_ten += 1\n",
    "                    lonely_stns.append(stn)\n",
    "                if n_stns < min_concurrent_stns:\n",
    "                    min_concurrent_stns = n_stns\n",
    "            print(f\"    {n_less_than_ten} stations do not have at least 10 viable sensors in the network with at least {min_years} years of concurrent record.\")\n",
    "\n",
    " \n",
    "    def run_selected(self):\n",
    "        # check the minimum number of years of overlap for all stations in self.ctx.overlap_dict\n",
    "        \n",
    "        self.result_file = os.path.join(self.results_folder, f'{self.stn_id}_estimated_pdfs.json')\n",
    "        if os.path.exists(self.result_file):\n",
    "            return None\n",
    "        else:\n",
    "            self.data = StationData(self.ctx, self.stn_id)\n",
    "            # self.data.k_nearest = self.k_nearest\n",
    "            # self.data.max_to_check = self.max_to_check\n",
    "            self._process_ground_truth()\n",
    "        try:\n",
    "            EstimatorClass = ESTIMATOR_CLASSES[method]\n",
    "            estimator = EstimatorClass(\n",
    "                self.ctx, self.stn_id, self.data\n",
    "            )\n",
    "            result = estimator.run_estimators(\n",
    "                divergence_measures=self.ctx.divergence_measures, \n",
    "                eps=self.ctx.eps,\n",
    "                baseline_pmf=self.baseline_pmf,\n",
    "                )\n",
    "            self._save_result(result)\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"  {method} estimator failed for {self.stn_id}: {str(e)}\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0ecf64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2889/1324 catchments contain withdrawal licenses\n",
      "    ...overlap dict loaded from data/record_overlap_dict.json\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# from utils import FDCEstimationContext\n",
    "rev_date = '20250227'\n",
    "attr_gdf_fpath = os.path.join('data', f'BCUB_watershed_attributes_updated_{rev_date}.geojson')\n",
    "LSTM_forcings_folder = '/home/danbot2/code_5820/neuralhydrology/data/BCUB_catchment_mean_met_forcings_20250320'\n",
    "LSTM_ensemble_result_folder = '/home/danbot2/code_5820/neuralhydrology/data/ensemble_results'\n",
    "parameter_prediction_results_folder = os.path.join('data', 'parameter_prediction_results')\n",
    "\n",
    "# methods = ('parametric', 'lstm',)\n",
    "methods = ('parametric', 'lstm', 'knn',)\n",
    "methods = ('knn',)\n",
    "\n",
    "processed = []\n",
    "ESTIMATOR_CLASSES = {\n",
    "}\n",
    "input_data = {\n",
    "    'attr_gdf_fpath': attr_gdf_fpath,\n",
    "    'LSTM_forcings_folder': LSTM_forcings_folder,\n",
    "    'LSTM_ensemble_result_folder': LSTM_ensemble_result_folder,\n",
    "    'LSTM_concurrent_network': False,  # use only stations with data 1980-present concurrent with Daymet\n",
    "    'parameter_prediction_results_folder': parameter_prediction_results_folder,\n",
    "    'streamflow_dir': STREAMFLOW_DIR,\n",
    "    # 'predicted_param_sample': predicted_param_sample,\n",
    "    'divergence_measures': ['DKL', 'EMD'],\n",
    "    'minimum_concurrent_years': [0, 1, 2, 5, 10, 20],\n",
    "    'eps': 1e-12,\n",
    "    'min_flow': 1e-4,\n",
    "    'n_grid_points': 2**12,\n",
    "    'min_overlap_years': 5,\n",
    "}\n",
    "\n",
    "context = FDCEstimationContext(**input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d73fc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_network_records = False\n",
    "if plot_network_records:\n",
    "    discharge = context.ds['discharge']\n",
    "    # mask = (~np.isnan(discharge)).astype(int)\n",
    "    output_file(\"images/data_availability_matrix.html\")\n",
    "    # Step 1: Convert to DataArray and group by week\n",
    "    # Convert time coordinate to Pandas Index first\n",
    "    time_index = pd.DatetimeIndex(discharge.time.values)\n",
    "\n",
    "    # Convert to weekly periods and back to timestamps (start of week)\n",
    "    week = time_index.to_period(\"W\").to_timestamp()\n",
    "\n",
    "    # Assign 'week' as a new coordinate aligned with time\n",
    "    d = context.ds['discharge']  # shape: (w, time)\n",
    "\n",
    "    d.coords[\"week\"] = (\"time\", week)\n",
    "    # Count non-NaNs per watershed/week\n",
    "    weekly_counts = d.groupby(\"week\").map(lambda x: np.isfinite(x).sum(dim=\"time\"))\n",
    "\n",
    "    # Boolean: weeks with â‰¥3 days of data\n",
    "    weekly_mask = (weekly_counts >= 3).transpose(\"watershed\", \"week\")\n",
    "    week_dt = pd.to_datetime(weekly_mask['week'].values) \n",
    "    # Convert to float and flip for plotting\n",
    "    img = weekly_mask.astype(float).values.T#[::-1, :]\n",
    "\n",
    "    watershed_ids = d.watershed.values\n",
    "    x_start, x_end = 0, len(watershed_ids)\n",
    "    y_start, y_end = week_dt.min(), week_dt.max()\n",
    "    output_file(\"images/weekly_data_availability_matrix.html\")\n",
    "    # Assume weekly_mask is a DataArray: dimensions ('watershed', 'week')\n",
    "    img = weekly_mask.astype(float).values.T[::-1, :]  # shape: (ny, nx)\n",
    "\n",
    "    # Get the datetime values for the weekly bins\n",
    "    week_dt = pd.to_datetime(weekly_mask['week'].values)  # len = nx\n",
    "    watersheds = weekly_mask['watershed'].values          # len = ny\n",
    "\n",
    "\n",
    "    p = figure(\n",
    "        width=1200,\n",
    "        height=300,\n",
    "        y_axis_type=\"datetime\",\n",
    "        y_range=(week_dt[0], week_dt[-1]),\n",
    "        x_range=(0, len(watersheds)),\n",
    "        title='',\n",
    "        # toolbar_location=None,\n",
    "        # title=\"Weekly Data Availability Matrix (â‰¥3 days/week)\"\n",
    "    )\n",
    "\n",
    "    # Define color mapper\n",
    "    mapper = LinearColorMapper(palette=[\"#ffffff\", \"#444444\"], low=0, high=1)\n",
    "\n",
    "    # Use image glyph; Bokeh handles datetime x when x is a datetime64\n",
    "    p.image(\n",
    "        image=[img],\n",
    "        y=week_dt[0],\n",
    "        x=0,\n",
    "        dh=(week_dt[-1] - week_dt[0]),#.astype('timedelta64[D]').astype(int),  # width in days\n",
    "        dw=len(watersheds),\n",
    "        color_mapper=mapper\n",
    "    )\n",
    "\n",
    "    # Format ticks to show only years\n",
    "    # p.xaxis.formatter = DatetimeTickFormatter(years=\"%Y\")\n",
    "\n",
    "    # Minimalist style\n",
    "    p.xaxis.axis_label = 'Watershed ID'\n",
    "    p.yaxis.axis_label = 'Date'\n",
    "    p.grid.visible = False\n",
    "    p.outline_line_color = None\n",
    "\n",
    "    save(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "25d6acab",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FDCEstimationContext' object has no attribute 'official_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m processed \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stn \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mofficial_ids\u001b[49m:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(stn)\n\u001b[1;32m      5\u001b[0m     runner \u001b[38;5;241m=\u001b[39m PDFEstimatorRunner(stn, context, methods, k_nearest\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, max_to_check\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FDCEstimationContext' object has no attribute 'official_ids'"
     ]
    }
   ],
   "source": [
    "processed = []\n",
    "t0 = time()\n",
    "for stn in context.official_ids:\n",
    "    print(stn)\n",
    "    runner = PDFEstimatorRunner(stn, context, methods, k_nearest=10, max_to_check=20)\n",
    "    runner.run_selected()\n",
    "    processed.append(stn)\n",
    "    if len(processed) % 10 == 0:\n",
    "        t1 = time()\n",
    "        elapsed = t1 - t0\n",
    "        unit_time = elapsed / len(processed)\n",
    "        print(f'Processed {len(processed)}/{len(context.official_ids)} stations in {unit_time:.2f} seconds per station')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0671534-8167-46aa-babd-38f0c7435d85",
   "metadata": {},
   "source": [
    "#### Kernel Density Estimator in 1D\n",
    "\n",
    "$$\\hat g (x) = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{h(x_i)} K \\left[ \\frac{x-x_i}{h(x_i)} \\right]$$\n",
    "\n",
    "Where: \n",
    "* $N$ is the number of data points\n",
    "* $x_i$ is the $i^\\text{th}$ data point\n",
    "* $K(x)$ is the kernel function, normalized to 1.\n",
    "* $h$ is the bandwidth, in this case it is a function of x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c369a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the measurement error:\n",
    "\n",
    "efig = figure(title=\"Estimated Measurement Error Model\", width=600, height=400, x_axis_type='log')\n",
    "error_points = jnp.array([1e-4, 1e-3, 1e-2, 1e-1, 1., 1e1, 1e2, 1e3, 1e4, 1e5])  #  Reference flow points in m^3/s\n",
    "error_values = jnp.array([1.0, 0.5, 0.2, 0.1, 0.1, 0.1, 0.1, 0.15, 0.2, 0.25])\n",
    "efig.line(error_points, error_values, line_color='red', line_width=2, legend_label='Measurement Error Model')\n",
    "efig.xaxis.axis_label = r'$$\\text{Flow } m^3/s$$'\n",
    "efig.yaxis.axis_label = r'$$\\text{Error } [\\text{x}100\\%]$$'\n",
    "efig.legend.background_fill_alpha = 0.5\n",
    "efig = dpf.format_fig_fonts(efig, font_size=14)\n",
    "\n",
    "layout = gridplot([efig], ncols=2, width=500, height=350)\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682320ab-58e3-4baa-aad8-d9f9604abd01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5312ed1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
