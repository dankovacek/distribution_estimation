{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5102c59-e6d1-40c9-8fba-3503a7b34cfa",
   "metadata": {},
   "source": [
    "# Predict Runoff Statistics from Catchment Attributes\n",
    "\n",
    "```{figure} images/param_prediction_test_result.png\n",
    "---\n",
    "width: 800px\n",
    "name: Example result showing the prediction error after each group of predictors is added, a scatter plot of observed and predicted mean runoff predicted from catchment attributes based on all predictors, the learning curve of training and test sets, and the distribution of target variables.\n",
    "---\n",
    "Example result showing the prediction error after each group of predictors is added, a scatter plot of observed and predicted mean runoff predicted from catchment attributes based on all predictors, the learning curve of training and test sets, and the distribution of target variables.\n",
    "```\n",
    "\n",
    "## Introduction\n",
    "\n",
    "We compute the (ordinary) statistics of runoff time series for a large sample of catchments.  We then test the predictability of these statistics from catchment attributes using a gradient boosting machine learning approach.  The purpose of testing predictability of the order statistics and L-moments is to fully describe parametric distributions for estimating streamflow distributions (flow duration curves), a common problem in applied hydrology.  \n",
    "\n",
    "Catchment attributes are used as predictors of each order statistic.  Attributes are added cumulatively in successive tests to compare the contribution of catchment attribute groups related to climate, terrain, land cover, and soil.  \n",
    "\n",
    "We test if transforming the target variable has an effect on the xgboost model. For transformations that do not change the rank of the target variable, i would not expect to see an effect, however the metric used as the objective function may be sensitive to the distribution of target variables, that is sensitive to outliers.  We test the predictability of the following:\n",
    "\n",
    "| Statistic       | Description                                                         |\n",
    "|-----------------|---------------------------------------------------------------------|\n",
    "| `uar_mean`  | Mean of the `<param>` series                                        |\n",
    "| `uar_median`| Median of the `<param>` series                                      |\n",
    "| `uar_std`   | Standard deviation of the `<param>` series                          |\n",
    "| `uar_mad`   | Mean Absolute Deviation (MAD) of the `<param>` series               |\n",
    "| `log_<param>_mean`  | Mean of the log transformed `<param>` series (mean, median, stdev, MAD)                        |\n",
    "\n",
    "\n",
    "\n",
    "**Notes:**\n",
    "- `uar`: unit area runoff is expressed in $L/s/\\text{km}^2$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b56f760d-4fe7-4a86-bfc4-a9fa56018c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"e192abd0-0030-47f4-8dc9-42c898d4c998\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"e192abd0-0030-47f4-8dc9-42c898d4c998\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.8.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.8.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.8.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.8.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.8.0.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"e192abd0-0030-47f4-8dc9-42c898d4c998\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "from scipy.stats import skew, kurtosis\n",
    "from math import comb\n",
    "\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.layouts import gridplot, row, column\n",
    "from bokeh.models import HoverTool, ColumnDataSource, Whisker, Legend, LegendItem\n",
    "from bokeh.transform import factor_cmap, factor_mark\n",
    "from bokeh.palettes import Category10, Category20, Category20c, Viridis\n",
    "from bokeh.io import output_notebook\n",
    "# from bokeh.palettes import Sunset10, Vibrant7\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from utils import data_processing_functions as dpf\n",
    "\n",
    "from scipy.stats import linregress\n",
    "output_notebook()\n",
    "\n",
    "BASE_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8a99f5-1e80-4412-9d09-baaa5d80ca82",
   "metadata": {},
   "source": [
    "## Load Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8eb7b960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the catchment characteristics\n",
    "HYSETS_DIR = Path('/home/danbot/code/common_data/HYSETS')\n",
    "\n",
    "# load camels hydro attributes to compare with the BCUB data\n",
    "camels_df = pd.read_csv('data/camels/camels_hydro.txt', sep=';')\n",
    "camels_df['gauge_id'] = camels_df['gauge_id'].astype(str)\n",
    "camels_df.head()\n",
    "\n",
    "hs_df = pd.read_csv('data/HYSETS_watershed_properties.txt', sep=';', dtype={'Watershed_ID': int, 'Official_ID': str})\n",
    "\n",
    "# create dictionaries for quick lookups\n",
    "da_dict = {row['Official_ID']: row['Drainage_Area_km2'] for _, row in hs_df.iterrows()}\n",
    "# needed to map between watershed ID in Hysets data and official_ID\n",
    "watershed_id_dict = {row['Watershed_ID']: row['Official_ID'] for _, row in hs_df.iterrows()}\n",
    "# and the inverse\n",
    "official_id_dict = {row['Official_ID']: row['Watershed_ID'] for _, row in hs_df.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c74a62c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_with_padding(oid):\n",
    "    if oid in hs_df['Official_ID'].values:\n",
    "        return oid\n",
    "    print(f'{oid} not found in HYSETS data, trying padded versions...')\n",
    "    for pad in range(1, 4):\n",
    "        padded = oid.zfill(len(oid) + pad)\n",
    "        if padded in hs_df['Official_ID'].values:\n",
    "            print(f'    Found padded version: {padded}')\n",
    "            return padded\n",
    "    raise ValueError(f\"Official ID {oid} not found in HYSETS data, even with padding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df02db60",
   "metadata": {},
   "source": [
    "Get the watershed attributes file from:\n",
    "\n",
    "https://doi.org/10.5683/SP3/JNKZVT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b90c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra stations to exclude (see Notebook 1: Data)\n",
    "exclude_stations = ['08FA009', '08GA037', '08NC003', '12052500', '12090480', '12107950', '12108450', '12119300', \n",
    "                    '12119450', '12200684', '12200762', '12203000', '12409500', '15056070', '15081510',\n",
    "                    '12323760', '12143700', '12143900', '12398000', '12058800', '12137800', '12100000']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d87b5be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_date = '20250227'\n",
    "attribute_file = f'BCUB_watershed_attributes_updated_{rev_date}.csv'\n",
    "updated_attribute_file = 'catchment_attributes_with_runoff_stats.csv'\n",
    "if not os.path.exists(os.path.join('data', updated_attribute_file)):\n",
    "    print(f'Updated attribute file {updated_attribute_file} not found. Using {attribute_file} instead.')\n",
    "    updated_attribute_path = os.path.join('data', attribute_file)\n",
    "    process_statistics = True\n",
    "else:\n",
    "    updated_attribute_path = os.path.join(os.getcwd(), 'data', updated_attribute_file)\n",
    "    process_statistics = False\n",
    "\n",
    "df = pd.read_csv(updated_attribute_path, dtype={'official_id': str})\n",
    "df['official_id'] = df['official_id'].apply(lambda x: match_with_padding(x))\n",
    "df = df[~df['official_id'].isin(exclude_stations)]\n",
    "df = df[[c for c in df.columns if 'unnamed:' not in c.lower()]]\n",
    "df.columns = [c.lower() for c in df.columns]\n",
    "df.sort_values('official_id', inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "filtered_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "32919a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replaced_zero_flow_uar_mean\n",
      "count    1017.000000\n",
      "mean        2.369946\n",
      "std         1.304504\n",
      "min        -3.725966\n",
      "25%         1.622844\n",
      "50%         2.419596\n",
      "75%         3.363415\n",
      "max         5.249998\n",
      "Name: replaced_zero_flow_uar_mean, dtype: float64\n",
      "\n",
      "replaced_zero_flow_uar_median\n",
      "count    1017.000000\n",
      "mean        2.285861\n",
      "std         1.378161\n",
      "min        -5.081404\n",
      "25%         1.423557\n",
      "50%         2.289153\n",
      "75%         3.389269\n",
      "max         5.301632\n",
      "Name: replaced_zero_flow_uar_median, dtype: float64\n",
      "\n",
      "replaced_zero_flow_uar_mad\n",
      "count    1017.000000\n",
      "mean        0.954875\n",
      "std         0.373888\n",
      "min         0.176362\n",
      "25%         0.744543\n",
      "50%         0.894321\n",
      "75%         1.060063\n",
      "max         3.523664\n",
      "Name: replaced_zero_flow_uar_mad, dtype: float64\n",
      "\n",
      "replaced_zero_flow_uar_std\n",
      "count    1017.000000\n",
      "mean        1.158944\n",
      "std         0.458342\n",
      "min         0.227834\n",
      "25%         0.916154\n",
      "50%         1.071047\n",
      "75%         1.253966\n",
      "max         4.275182\n",
      "Name: replaced_zero_flow_uar_std, dtype: float64\n",
      "\n",
      "replaced_zero_flow_uar_skew\n",
      "count    1017.000000\n",
      "mean        0.244512\n",
      "std         0.598438\n",
      "min        -3.825114\n",
      "25%        -0.002380\n",
      "50%         0.279603\n",
      "75%         0.602981\n",
      "max         3.022792\n",
      "Name: replaced_zero_flow_uar_skew, dtype: float64\n",
      "\n",
      "replaced_zero_flow_uar_kurt\n",
      "count    1017.000000\n",
      "mean       -0.123377\n",
      "std         1.916781\n",
      "min        -1.647395\n",
      "25%        -0.823783\n",
      "50%        -0.477856\n",
      "75%         0.025475\n",
      "max        39.585869\n",
      "Name: replaced_zero_flow_uar_kurt, dtype: float64\n",
      "\n",
      "log_uar_mean\n",
      "count    1017.000000\n",
      "mean        2.369946\n",
      "std         1.304504\n",
      "min        -3.725966\n",
      "25%         1.622844\n",
      "50%         2.419596\n",
      "75%         3.363415\n",
      "max         5.249998\n",
      "Name: log_uar_mean, dtype: float64\n",
      "\n",
      "log_uar_median\n",
      "count    1017.000000\n",
      "mean        2.285861\n",
      "std         1.378161\n",
      "min        -5.081404\n",
      "25%         1.423557\n",
      "50%         2.289153\n",
      "75%         3.389269\n",
      "max         5.301632\n",
      "Name: log_uar_median, dtype: float64\n",
      "\n",
      "log_uar_mad\n",
      "count    1017.000000\n",
      "mean        0.954875\n",
      "std         0.373888\n",
      "min         0.176362\n",
      "25%         0.744543\n",
      "50%         0.894321\n",
      "75%         1.060063\n",
      "max         3.523664\n",
      "Name: log_uar_mad, dtype: float64\n",
      "\n",
      "log_uar_std\n",
      "count    1017.000000\n",
      "mean        1.158944\n",
      "std         0.458342\n",
      "min         0.227834\n",
      "25%         0.916154\n",
      "50%         1.071047\n",
      "75%         1.253966\n",
      "max         4.275182\n",
      "Name: log_uar_std, dtype: float64\n",
      "\n",
      "log_uar_skew\n",
      "count    1017.000000\n",
      "mean        0.244512\n",
      "std         0.598438\n",
      "min        -3.825114\n",
      "25%        -0.002380\n",
      "50%         0.279603\n",
      "75%         0.602981\n",
      "max         3.022792\n",
      "Name: log_uar_skew, dtype: float64\n",
      "\n",
      "log_uar_kurt\n",
      "count    1017.000000\n",
      "mean       -0.123377\n",
      "std         1.916781\n",
      "min        -1.647395\n",
      "25%        -0.823783\n",
      "50%        -0.477856\n",
      "75%         0.025475\n",
      "max        39.585869\n",
      "Name: log_uar_kurt, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uar_cols = [c for c in filtered_df.columns if 'uar' in c]\n",
    "for c in uar_cols:\n",
    "    print(c)\n",
    "    print(filtered_df[c].describe())\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91ae415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_filter_hysets_data(station_ids, hs_df):\n",
    "\n",
    "    # load the updated HYSETS data\n",
    "    updated_filename = 'HYSETS_2023_update_QC_stations.nc'\n",
    "    ds = xr.open_dataset(HYSETS_DIR / updated_filename)\n",
    "\n",
    "    # Get valid IDs as a NumPy array\n",
    "    hs_df = hs_df[hs_df['Official_ID'].isin(station_ids)]\n",
    "    selected_ids = hs_df['Watershed_ID'].values\n",
    "\n",
    "    # Get boolean index where watershedID in selected_set\n",
    "    # safely access watershedID as a variable first\n",
    "    ws_ids = ds['watershedID'].data  # or .values if you prefer\n",
    "    mask = np.isin(ws_ids, selected_ids)\n",
    "\n",
    "    # Apply mask to data\n",
    "    ds = ds.sel(watershed=mask)\n",
    "    # Step 1: Promote 'watershedID' to a coordinate on the 'watershed' dimension\n",
    "    ds = ds.assign_coords(watershedID=(\"watershed\", ds[\"watershedID\"].data))\n",
    "\n",
    "    # Step 2: Set 'watershedID' as the index for the 'watershed' dimension\n",
    "    return ds.set_index(watershed=\"watershedID\")\n",
    "\n",
    "\n",
    "def retrieve_timeseries_discharge(stn, ds):\n",
    "    watershed_id = official_id_dict[stn]\n",
    "    # drainage_area = self.ctx.da_dict[stn]\n",
    "    # data = self.ctx.data\n",
    "    da = da_dict[stn]\n",
    "    df = ds['discharge'].sel(watershed=str(watershed_id)).to_dataframe(name='discharge').reset_index()\n",
    "    df = df.set_index('time')[['discharge']]\n",
    "    df.dropna(inplace=True)\n",
    "    # clip minimum flow to 1e-4\n",
    "    df['discharge'] = np.clip(df['discharge'], 1e-4, None)\n",
    "    df.rename(columns={'discharge': stn}, inplace=True)\n",
    "    df[f'{stn}_uar'] = 1000 * df[stn] / da\n",
    "    df[f'{stn}_mm'] = df[stn] * (24 * 3.6 / da)\n",
    "    df['log_x'] = np.log(df[f'{stn}_uar'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa140426",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_and_filter_hysets_data(df['official_id'], hs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e157304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = [\n",
    "    'uar_mean', 'uar_std', 'uar_median', 'uar_mad',\n",
    "    'log_uar_mean', 'log_uar_std', 'log_uar_median', 'log_uar_mad',\n",
    "]\n",
    "\n",
    "# target_cols += [f'prob_q_lessthan_{threshold}' for threshold in [1e-4, 5e-4, 1e-3, 5e-3, 1e-2]]\n",
    "assert np.all([c in filtered_df.columns for c in target_cols]), f\"Not all target columns found in filtered df: {target_cols} \\n {list(df.columns)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9681d7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import ColorBar, LogColorMapper, ColumnDataSource\n",
    "\n",
    "filtered_df['uar_mean'] = filtered_df['replaced_zero_flow_uar_mean']\n",
    "filtered_df['uar_std'] = filtered_df['replaced_zero_flow_uar_std']\n",
    "# visualize the distribution of the mean runoff\n",
    "p1 = figure(title=\"Mean runoff distribution\", width=500, height=300)\n",
    "hist, edges = np.histogram(filtered_df['uar_mean'], density=True, bins=50)\n",
    "p1.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:], line_color='white')\n",
    "p1.xaxis.axis_label = 'Mean runoff [L/s/km^2]'\n",
    "p1.yaxis.axis_label = 'Frequency'\n",
    "\n",
    "# visualize a scatter plot of the mean and standard deviation\n",
    "p2 = figure(title=f\"Mean vs. Standard deviation (N={len(df)})\", width=500, height=300)\n",
    "# add color mapper to encode drainage_area_km2\n",
    "mapper = LogColorMapper(palette='Viridis256', low=filtered_df['drainage_area_km2'].min(), \n",
    "                        high=filtered_df['drainage_area_km2'].max())\n",
    "source = ColumnDataSource(filtered_df)\n",
    "color_bar = ColorBar(color_mapper=mapper, width=8, location=(0,0), title='Drainage area [km²]',)\n",
    "p2.add_layout(color_bar, 'right')\n",
    "# add the scatter plot with color mapping\n",
    "p2.scatter('uar_mean', 'uar_std', source=source, color={'field': 'drainage_area_km2', 'transform': mapper})\n",
    "\n",
    "x = np.linspace(0, filtered_df['uar_mean'].max(), 100)\n",
    "slope, intercept, r, pval, se = linregress(filtered_df['uar_mean'].values, filtered_df['uar_std'].values)\n",
    "y = [slope*e + intercept for e in x]\n",
    "p2.line(x, y, legend_label=f'L2: y={slope:.2f}x + {intercept:.2f} (R²={r**2:.2f})', \n",
    "       line_width=2, color='red', line_dash='dashed')\n",
    "\n",
    "# plot the L1 Norm -- L2 is sensitive to outliers and is biased for low mean values\n",
    "# l1_slope, l1_intercept = L1_fit_line(df['mean_uar'].values, df['sd_uar'].values)\n",
    "# yl1 = [l1_slope*e + l1_intercept for e in x]\n",
    "# p2.line(x, yl1, legend_label=f'L1: y={l1_slope:.2f}x + {l1_intercept:.2f}', \n",
    "#        line_width=2, color='red', line_dash='dotted')\n",
    "p2.line([0, filtered_df['uar_mean'].max()], [0, filtered_df['uar_mean'].max()], \n",
    "        line_width=2, color='black', line_dash='dotted', legend_label='1:1')\n",
    "\n",
    "# add hover tool to show official_id\n",
    "p2.add_tools(HoverTool(tooltips=[('Official ID', '@official_id')]))\n",
    "# p2.xaxis.axis_label = 'Mean runoff [mm/day]'\n",
    "p2.xaxis.axis_label = 'Mean runoff [L/s/km^2]'\n",
    "p2.yaxis.axis_label = 'Standard deviation'\n",
    "p2.legend.location = 'top_left'\n",
    "p2.legend.background_fill_alpha = 0.5\n",
    "p2.legend.click_policy = 'hide'\n",
    "show(row(p1, p2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34ccf39-9504-4f26-a1b1-7286a7bf80ef",
   "metadata": {},
   "source": [
    "## Define attribute groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407c1113-81ed-4428-a448-8b7f2bd771c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "terrain = ['drainage_area_km2', 'elevation_m', 'slope_deg', 'aspect_deg']\n",
    "land_cover = [\n",
    "    'land_use_forest_frac_2010', 'land_use_grass_frac_2010', 'land_use_wetland_frac_2010', 'land_use_water_frac_2010', \n",
    "    'land_use_urban_frac_2010', 'land_use_shrubs_frac_2010', 'land_use_crops_frac_2010', 'land_use_snow_ice_frac_2010']\n",
    "climate = ['prcp', 'srad', 'swe', 'tmax', 'tmin', 'vp', 'high_prcp_freq', 'high_prcp_duration', 'low_prcp_freq', 'low_prcp_duration']\n",
    "soil = ['logk_ice_x100', 'porosity_x100']\n",
    "\n",
    "all_attributes = terrain + land_cover + soil + climate\n",
    "assert len([c for c in all_attributes if c not in df.columns]) == 0\n",
    "len(all_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f5cd7b-882c-40db-b12d-87bc810e6e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folder = os.path.join(BASE_DIR, 'data', 'results', 'parameter_prediction_results')\n",
    "if not os.path.exists(results_folder):\n",
    "    os.makedirs(results_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1a16f7-84df-4e42-9acf-b2a990b30812",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfolds = 5\n",
    "n_boost_rounds = 500\n",
    "n_optimization_rounds = 20\n",
    "loss = 'reg:squarederror'\n",
    "# loss = 'reg:absoluteerror'\n",
    "\n",
    "all_test_results = {}\n",
    "attribute_set_dict = {\n",
    "    'climate': climate, \n",
    "    '+land_cover': land_cover,\n",
    "    '+terrain': terrain, \n",
    "    '+soil': soil,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec0b64e-34cc-4e51-a349-01caff47d498",
   "metadata": {},
   "source": [
    "## Set Attribute Groupings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9fad8a-2937-463e-aad2-cd2f53259092",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_1 = ['climate', '+terrain', '+land_cover', '+soil']\n",
    "# for group 2, just reverse group 1\n",
    "group_2 = group_1[::-1]\n",
    "group_3 = ['+land_cover', '+terrain', '+soil', 'climate']\n",
    "group_4 = ['+soil', 'climate', '+land_cover', '+terrain']\n",
    "attribute_group_orders = [group_1, group_2, group_3, group_4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0cb174-d0ba-4255-9a4a-e5def949dfb9",
   "metadata": {},
   "source": [
    "## Run XGBoost Models\n",
    "\n",
    "Separate the test set at the outset so the attribute group ordering is tested on the same hold-out set but necessarily on unique training optimizations.  This ensures that at least the presence of outliers in the hold-out set should at least be constant across the attribute group reordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b0fc7d-115a-4749-bd26-8c1edc973e6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.xgb_functions import run_xgb_CV_trials\n",
    "\n",
    "def predict_runoff_from_attributes(df, target_column, group_order, results_folder, n_boost_rounds, n_optimization_rounds, loss):\n",
    "    \"\"\"\n",
    "    Note that here we're predicting the log mean unit area runoff\n",
    "    \"\"\"\n",
    "    # set the target column\n",
    "    predictor_attributes = []\n",
    "    results = {}\n",
    "    # add attribute groups successively\n",
    "    for set_name in group_order:\n",
    "        print(f' Processing {set_name} attribute set')\n",
    "        predictor_attributes += attribute_set_dict[set_name] \n",
    "        input_data = df[['official_id'] + predictor_attributes + [target_column]].copy()\n",
    "        result_df, all_predictions_df, all_convergence_df = run_xgb_CV_trials(\n",
    "            set_name, predictor_attributes, target_column, input_data, \n",
    "            n_optimization_rounds, nfolds, n_boost_rounds, results_folder, \n",
    "            loss=loss,\n",
    "        )\n",
    "        # store the test set predictions and actuals\n",
    "        results[set_name] = {\n",
    "            'all_results': result_df,\n",
    "            'convergence': all_convergence_df,\n",
    "            'oos_predictions': all_predictions_df,\n",
    "        } \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51c3401-0e98-4162-ae4c-b86a7763d30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "group_results_dict = {}\n",
    "eval_metrics = {'reg:squarederror': 'test_rmse', 'reg:absoluteerror': 'test_mae'}\n",
    "for target_col in target_cols:\n",
    "    n  = 0\n",
    "    min_error = 1e9    \n",
    "    best_set = None\n",
    "    results_dict[target_col] = {}\n",
    "    group_results_dict[target_col] = {}\n",
    "    print(f'TARGET: {target_col}')\n",
    "    eval_metric = eval_metrics[loss]\n",
    "\n",
    "    for n, group in enumerate(attribute_group_orders):\n",
    "        print(f'Processing: {group} ordering. {n}/{len(attribute_group_orders)}.')\n",
    "        \n",
    "        group_results_fname = f'{target_col}_prediction_results_{\"\".join(group)}.npy'\n",
    "        group_results_fpath = os.path.join(results_folder, group_results_fname)\n",
    "\n",
    "        print(f'Group results file: {group_results_fpath}')\n",
    "        \n",
    "        if os.path.exists(group_results_fpath):\n",
    "            print(f'Retrieving existing results from {group_results_fpath}')\n",
    "            group_results = np.load(group_results_fpath, allow_pickle=True).item()\n",
    "        else:\n",
    "            group_results = predict_runoff_from_attributes(filtered_df, target_col, group, results_folder, n_boost_rounds, n_optimization_rounds, loss)\n",
    "            np.save(group_results_fpath, group_results)\n",
    "    \n",
    "        group_results_dict[target_col][n] = {'order': group, 'results': group_results}\n",
    "        for k, test_data in group_results.items():\n",
    "            # Save all results, not just the best\n",
    "            results_dict[target_col][f'group_{n}'] = {\n",
    "                'group_order': group,\n",
    "                'test_error': test_data['all_results'][f'{eval_metric}_mean'],\n",
    "                'test_error_std': test_data['all_results'][f'{eval_metric}_stdev'],\n",
    "                'convergence': test_data['convergence'],\n",
    "                'oos_predictions': test_data['oos_predictions'],\n",
    "            }\n",
    "            # print(f'Group {n} {k} {eval_metric}: {results_dict[target_col][f\"group_{n}\"][\"test_error\"]}')\n",
    "            summary_csv = f'{target_col}_summary_group_{n}.csv'\n",
    "            test_data['oos_predictions'].to_csv(os.path.join(results_folder, summary_csv), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994cbe1c-af06-4c61-a14e-d2af78de18ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "    'uar_mean': {'x': r'$$\\mu_i \\left[ L s^{-1} \\text{km}^{-2}\\right]$$', 'y': r'$$\\hat \\mu_i \\left[L s^{-1} \\text{km}^{-2} \\right]$$'}, \n",
    "    'uar_std': {'x': r'$$\\sigma_i \\left[ L s^{-1} \\text{km}^{-2}\\right]$$', 'y': r'$$\\hat \\sigma_i \\left[ L s^{-1} \\text{km}^{-2} \\right]$$'}, \n",
    "    'uar_median': {'x': r'$$\\text{Median}_i \\left[ L s^{-1} \\text{km}^{-2}\\right]$$', 'y': r'$$ \\text{Median}_\\text{est} \\left[ L s^{-1} \\text{km}^{-2} \\right]$$'}, \n",
    "    'uar_mad': {'x': r'$$\\text{MAD}_i \\left[ L s^{-1} \\text{km}^{-2}\\right]$$', 'y': r'$$\\text{MAD}_\\text{est} \\left[ L s^{-1} \\text{km}^{-2} \\right]$$'}, \n",
    "    'log_uar_mean': {'x': r'$$\\log \\mu \\left[ L s^{-1} \\text{km}^{-2} \\right]$$', 'y': r'$$ \\log \\hat \\mu  \\left[ L s^{-1} \\text{km}^{-2} \\right]$$'}, \n",
    "    'log_uar_median': {'x': r'$$\\log \\text{Median} \\left[ L s^{-1} \\text{km}^{-2} \\right]$$', 'y': r'$$ \\text{Log Median}_\\text{est}  \\left[ L s^{-1} \\text{km}^{-2} \\right]$$'}, \n",
    "    'log_uar_std': {'x': r'$$\\log\\ \\sigma_i \\left[ L s^{-1} \\text{km}^{-2} \\right]$$', 'y': r'$$\\log \\hat \\sigma_i  \\left[ L s^{-1} \\text{km}^{-2} \\right]$$'}, \n",
    "    'log_uar_mad': {'x': r'$$\\log\\ \\text{MAD}_i \\left[ L s^{-1} \\text{km}^{-2} \\right]$$', 'y': r'$$ \\text{Log MAD}_\\text{est} \\left[ L s^{-1} \\text{km}^{-2} \\right]$$'},\n",
    "}\n",
    "\n",
    "target_cols = [\n",
    "    'uar_mean', 'uar_std', 'uar_median', 'uar_mad',\n",
    "    'log_uar_mean', 'log_uar_std', 'log_uar_median', 'log_uar_mad',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5f4a55-eb98-4006-8bdc-b97f1cfb104b",
   "metadata": {},
   "source": [
    "## View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4557abc-675c-4948-945e-1d593170eace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results_plots(target_col, results_df, attribute_sets, eval_metric, title=''):    \n",
    "    plots = []\n",
    "    def _plot_attribute_order_line():\n",
    "        means, lbs, ubs = [], [], []\n",
    "\n",
    "        for e in attribute_sets:\n",
    "            df = results_df[e]['all_results']\n",
    "            trial_means = df[f'{eval_metric}_mean'].values\n",
    "            means.append(np.mean(trial_means))\n",
    "            lb, ub = np.percentile(trial_means, [2.5, 97.5])\n",
    "            lbs.append(lb)\n",
    "            ubs.append(ub)\n",
    "\n",
    "        max_range = (0.95*min(lbs), max(ubs)*1.05)\n",
    "        group_order = [e if not e.startswith('+') else e[1:] for e in attribute_sets]\n",
    "        group_order = group_order[:1] + [f'+{e}' for e in group_order[1:]]  # add '+' to all but the first element\n",
    "        source = ColumnDataSource({'x': group_order, 'y1': means, 'ub': ubs, 'lb': lbs})\n",
    "    \n",
    "        fig = figure(title='', x_range=group_order, y_range=max_range)#y_range=max_range)\n",
    "        fig.scatter('x', 'y1', legend_label=eval_metric.upper(), color='green', source=source, line_width=3, marker='square', size=4)\n",
    "        fig.add_layout(Whisker(source=source, base='x', upper='ub', lower='lb', line_width=1))\n",
    "        fig.legend.background_fill_alpha = 0.6\n",
    "        fig.yaxis.axis_label = r'$$\\text{RMSE}$$'\n",
    "        fig.xaxis.axis_label = r'$$\\text{Attribute Groups}$$'\n",
    "        best_set = min(attribute_sets, key=lambda x: results_df[x]['all_results'][f'{eval_metric}_mean'].mean())\n",
    "        return fig, best_set\n",
    "\n",
    "\n",
    "    def _plot_scatter_with_regression(best_result_df, xlabel, ylabel, target_col):\n",
    "        trial_r2 = (\n",
    "            best_result_df.groupby('trial')[['actual', 'predicted']]\n",
    "            .apply(lambda df: r2_score(df['actual'], df['predicted']))\n",
    "        )\n",
    "        # r2_mean = trial_r2.mean()\n",
    "        r2_std = trial_r2.std()\n",
    "        grouped = best_result_df.groupby('trial').agg({\n",
    "            'actual': 'median',\n",
    "            'predicted': 'median',\n",
    "        })\n",
    "        grouped['diff'] = (grouped['actual'] - grouped['predicted']).abs()\n",
    "        median_trial = grouped.sort_values('diff').index[len(grouped) // 2]\n",
    "        best_result = best_result_df[best_result_df['trial'] == median_trial].copy()\n",
    "        xx, yy = best_result['actual'].values, best_result['predicted'].values\n",
    "        source = ColumnDataSource({'x': xx, 'y': yy, 'ID': best_result['official_id'].values})\n",
    "        slope, intercept, r, p, se = linregress(xx, yy)\n",
    "\n",
    "        sfig = figure(title='')\n",
    "        sfig.scatter('x', 'y', size=3, alpha=0.8, source=source, legend_label=target_col, color='blue')\n",
    "        sfig.add_tools(HoverTool(tooltips=[('ID', '@ID')]))\n",
    "        x_obs = np.linspace(min(xx), max(xx), 1000)\n",
    "        ybf = [slope * e + intercept for e in x_obs]\n",
    "        sfig.line(x_obs, ybf, color='red', line_width=3, line_dash='dashed', legend_label=f'R²={r**2:.2f} ± {r2_std:.3f}')\n",
    "        sfig.line([0, max(ybf)], [0, max(ybf)], color='black', line_dash='dotted', line_width=2, legend_label='1:1')\n",
    "        sfig.xaxis.axis_label = xlabel\n",
    "        sfig.yaxis.axis_label = ylabel\n",
    "        sfig.legend.background_fill_alpha = 0.5\n",
    "        sfig.legend.location = 'top_left'\n",
    "        return sfig, median_trial\n",
    "\n",
    "    def _plot_convergence(convergence_df, median_trial):\n",
    "        cfig = figure(title='')\n",
    "\n",
    "        data = convergence_df[convergence_df['trial'] == median_trial].copy()\n",
    "        fold_nos = sorted(set(convergence_df['fold']))\n",
    "\n",
    "        min_pred_risk = 1e9\n",
    "        for fn in fold_nos:\n",
    "            fold_data = data[data['fold'] == fn].copy()\n",
    "            cfig.line(fold_data['round'], fold_data[f'test'], line_alpha=0.6, line_color='red', line_dash='dotted', legend_label=f'Test Folds')\n",
    "            cfig.line(fold_data['round'], fold_data[f'train'], line_alpha=0.7, line_color='grey', line_dash='dotted', legend_label=f'Train Folds')\n",
    "\n",
    "        train_pivot = data.pivot_table(index='round', columns='fold', values='train')#, aggfunc='first')\n",
    "        test_pivot = data.pivot_table(index='round', columns='fold', values='test')#, aggfunc='first')\n",
    "        train_pivot.columns = [f'fold_{col}' for col in train_pivot.columns]\n",
    "        test_pivot.columns = [f'fold_{col}' for col in test_pivot.columns]\n",
    "\n",
    "        train_pivot['mean'] = train_pivot.mean(axis=1)\n",
    "        test_pivot['mean'] = test_pivot.mean(axis=1)\n",
    "        cfig.line(train_pivot.index, train_pivot['mean'], line_alpha=0.5, line_color='grey', line_width=2, legend_label='CV Mean (Train)')\n",
    "        cfig.line(test_pivot.index, test_pivot['mean'], line_alpha=0.5, line_color='red', line_width=2, legend_label='CV Mean (Test)')\n",
    "\n",
    "        min_pred_risk_idx = test_pivot['mean'].idxmin()\n",
    "        min_pred_risk = test_pivot.loc[min_pred_risk_idx, 'mean']\n",
    "        if min_pred_risk_idx == max(test_pivot['mean'].index):\n",
    "            print(f'Min prediction risk occurs at the maximum iteration, try increasing the number of boosting rounds')\n",
    "        cfig.line([min_pred_risk_idx, min_pred_risk_idx], [0, min_pred_risk], legend_label='Min risk', color='green', line_width=2, line_dash='dashed')\n",
    "        cfig.legend.location = 'top_right'\n",
    "        cfig.legend.background_fill_alpha = 0.5\n",
    "        cfig.xaxis.axis_label = r'$$\\text{Iteration}$$'\n",
    "        cfig.yaxis.axis_label = r'$$\\text{RMSE}$$'\n",
    "        return cfig\n",
    "\n",
    "    def _plot_target_cdfs(cdf_arrays, xlabel):\n",
    "        cdffig = figure(title='', x_axis_type='log')\n",
    "        for (cdfx, cdfy) in cdf_arrays:\n",
    "            cdffig.line(cdfx, cdfy, color='black', line_alpha=0.6, line_width=2, legend_label='Fold CDFs')\n",
    "        cdffig.xaxis.axis_label = xlabel\n",
    "        cdffig.yaxis.axis_label = r'$$\\text{Pr}(X\\leq x) $$'\n",
    "        cdffig.legend.location = 'top_left'\n",
    "        return cdffig\n",
    "    \n",
    "    def _compute_empirical_cdf(data):\n",
    "        sorted_data = np.sort(data)\n",
    "        cdf = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n",
    "        return sorted_data, cdf\n",
    "\n",
    "    xlabel = label_dict[target_col]['x']\n",
    "    ylabel = label_dict[target_col]['y']\n",
    "\n",
    "    # Plot 1: RMSE/MAE across attribute sets\n",
    "    attr_fig, best_attr_set = _plot_attribute_order_line()\n",
    "    # attr_fig = _plot_attribute_order_cdf()\n",
    "    plots.append(attr_fig)\n",
    "\n",
    "    # Plot 2: Actual vs Predicted for best set\n",
    "    best_attr = [e for e in results_df.keys() if e.endswith(best_attr_set)][0]\n",
    "    best_result = results_df[best_attr]['oos_predictions']\n",
    "    sfig, median_trial = _plot_scatter_with_regression(best_result, xlabel, ylabel, target_col)\n",
    "    plots.append(sfig)\n",
    "\n",
    "    # Plot 3: Convergence plot\n",
    "    convergence_df = results_df[best_attr]['convergence']\n",
    "    cfig = _plot_convergence(convergence_df, median_trial)\n",
    "    plots.append(cfig)\n",
    "\n",
    "    # Plot 4: CDFs\n",
    "    cdf_arrays = []\n",
    "    for i, grp_result in results_df[best_attr]['oos_predictions'].groupby('fold'):\n",
    "        sorted_data, cdf = _compute_empirical_cdf(grp_result['actual'].values)\n",
    "        cdf_arrays.append((sorted_data, cdf))\n",
    "    cdffig = _plot_target_cdfs(cdf_arrays, xlabel)\n",
    "    plots.append(cdffig)\n",
    "\n",
    "    return plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74d78aa-283f-495f-8adb-1f7fd667ade5",
   "metadata": {},
   "source": [
    "In the sequence of plots below, we change the order that groups of attributes are added to training.  \n",
    "\n",
    "We split the CF folds using a stratified approach to balance the target variable distribution in each fold (right-most plot).\n",
    "\n",
    "The plot 3rd from left shows the objective score at each iteration to show how the training and test sets compare.  We don't want to continue training when the test sets are not improving.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf29208",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_results_dict[target_cols[0]].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eed93bf-4428-46f2-851c-58964ffc75f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# target_ranges = {'mean_uar': (14, 35), 'sd_uar': (20, 45),\n",
    "#                 'mean_logx': (0.45, 1.25), 'sd_logx': (0.4, 0.5)}\n",
    "# target_ranges = {'mean_uar': (14, 35), 'sd_uar': (20, 45),\n",
    "#                 'mean_logx': (0.45, 1.25), 'sd_logx': (0.4, 0.5)}\n",
    "n = 0\n",
    "all_plots = []\n",
    "for c in target_cols:\n",
    "    data = group_results_dict[c][n]\n",
    "    eval_metric = eval_metrics[loss]\n",
    "    grp_plots = create_results_plots(c, data['results'], data['order'], eval_metric)\n",
    "    all_plots += grp_plots\n",
    "\n",
    "layout = gridplot(all_plots, ncols=4, width=300, height=275)\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c1571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "all_plots = []\n",
    "for c in target_cols:\n",
    "    data = group_results_dict[c][n]\n",
    "    eval_metric = eval_metrics[loss]\n",
    "    grp_plots = create_results_plots(c, data['results'], data['order'], eval_metric)\n",
    "    all_plots += grp_plots\n",
    "\n",
    "layout = gridplot(all_plots, ncols=4, width=300, height=275)\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b05b1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "all_plots = []\n",
    "for c in target_cols:\n",
    "    data = group_results_dict[c][n]\n",
    "    eval_metric = eval_metrics[loss]\n",
    "    grp_plots = create_results_plots(c, data['results'], data['order'], eval_metric)\n",
    "    all_plots += grp_plots\n",
    "\n",
    "layout = gridplot(all_plots, ncols=4, width=300, height=275)\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c039e77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "all_plots = []\n",
    "for c in target_cols:\n",
    "    data = group_results_dict[c][n]\n",
    "    eval_metric = eval_metrics[loss]\n",
    "    grp_plots = create_results_plots(c, data['results'], data['order'], eval_metric)\n",
    "    all_plots += grp_plots\n",
    "\n",
    "layout = gridplot(all_plots, ncols=4, width=300, height=275)\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a87ca0-9f51-43b2-9304-f7c4d77dec89",
   "metadata": {},
   "source": [
    "### Test the sensitivity to Order of attribute groups\n",
    "\n",
    "Note that in the first plot (at left) the attribute groups are additive. That is, the GBM is trained on just the first group listed in the categorical x-axis, then the second group attributes are added to the first, then the third group is added to the first and second, and so forth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1593627-9a0b-492b-b7d9-f38f47373ca0",
   "metadata": {},
   "source": [
    "### Test randomly permuted target values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6205f3-f7c3-4e3a-8567-0d6607995674",
   "metadata": {},
   "source": [
    "As a last iteration, randomize the order of the mean_runoff column to test what the algorithm is learning.\n",
    "\n",
    "The predictive power decreases substantially across all groupings of input attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374287fd-d83d-49d5-8df9-f57ab64e616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_results = {}\n",
    "group_order = group_1\n",
    "for tc in target_cols:    \n",
    "    print(f'Processing randomization for target column: {tc}')\n",
    "    test_results_fname = f'{tc}_prediction_results_shuffled.npy'\n",
    "    test_results_fpath = os.path.join(results_folder, test_results_fname)\n",
    "    \n",
    "    if os.path.exists(test_results_fpath):\n",
    "\n",
    "        shuffled_test_results = np.load(test_results_fpath, allow_pickle=True).item()\n",
    "    else:\n",
    "        shuffled_df = filtered_df.copy()\n",
    "        for attr in all_attributes:\n",
    "            # randomly shuffle the order of attribute values\n",
    "            attr_values = filtered_df[attr].values\n",
    "            np.random.shuffle(attr_values)\n",
    "            shuffled_df[attr] = attr_values\n",
    "        \n",
    "        shuffled_test_results = predict_runoff_from_attributes(shuffled_df, tc, group_order, results_folder, n_boost_rounds, n_optimization_rounds, loss)\n",
    "        np.save(test_results_fpath, shuffled_test_results)\n",
    "\n",
    "    random_results[tc] = {'order': group_order, 'results': shuffled_test_results}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362e6084-b3d6-42dd-ad55-f1a7fe0b1c5e",
   "metadata": {},
   "source": [
    "### View results of shuffled target variable (mean runoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19599ae9-a6bd-4362-883a-d094c833df3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_shuffled_plots = []\n",
    "for tc in target_cols:\n",
    "    shuffled_results = random_results[tc]['results'].copy()\n",
    "    group_order = random_results[tc]['order']\n",
    "    shuffled_runoff_plots = create_results_plots(tc, shuffled_results, group_order, eval_metric, title='')\n",
    "    all_shuffled_plots += shuffled_runoff_plots\n",
    "\n",
    "layout = gridplot(all_shuffled_plots, ncols=4, width=300, height=275)\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eb04ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the results into a dictionary object for easy access to predicted values\n",
    "mean_predictions = []\n",
    "for tc in target_cols:\n",
    "    group_results = group_results_dict[tc]\n",
    "    group, group_data = 1, group_results[1]\n",
    "    eval_group = '+land_cover' # evaluate on climate + terrain + land_cover set (neglect soil)\n",
    "    data = group_data['results'][eval_group]\n",
    "    oos_predictions = (\n",
    "        data['oos_predictions']\n",
    "        .groupby('official_id')['predicted']\n",
    "        .mean()\n",
    "        .to_frame()\n",
    "    )\n",
    "    # make a dict of official_id: actual from data['oos_predictions']\n",
    "    actual_dict = data['oos_predictions'].set_index('official_id')['actual'].to_dict()\n",
    "    # store the predictions in a dataframe\n",
    "    oos_predictions.rename(columns={'predicted': f'{tc}_mean_predicted'}, inplace=True)\n",
    "    oos_predictions = oos_predictions[[f'{tc}_mean_predicted']]\n",
    "    oos_predictions[f'{tc}_actual'] = oos_predictions.index.map(actual_dict)\n",
    "    mean_predictions.append(oos_predictions)\n",
    "\n",
    "mean_predictions_df = pd.concat(mean_predictions, axis=1)\n",
    "mean_predictions_df.reset_index(inplace=True)\n",
    "mean_predictions_df.to_csv(os.path.join(results_folder, 'mean_parameter_predictions.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ffc38f-a4fb-4a65-80fc-e8ac81a9548c",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "- Reordering the attribute groupings suggests there are interactions between attributes in model training.  \n",
    "- Across all orderings, the climate attributes provide most predictive information,\n",
    "- Soil attributes contribute little or no explanatory power to the model.\n",
    "- Terrain and land cover attributes provide some predictive information over soil, but the joint entropy with climate seems to represent much of this information gain,\n",
    "- Randomly permuting the order of the target variable, `mean_runoff` erases all predictive power.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726a1db2-7b71-45e5-859b-fc3cbd7f0a87",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fe9e0a-d5f6-40f6-a93e-5629516b9afb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
