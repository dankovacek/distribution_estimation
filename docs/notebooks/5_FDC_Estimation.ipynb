{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a180e32",
   "metadata": {},
   "source": [
    "# FDC Estimation\n",
    "\n",
    "This notebook contains the main code for computing FDCs by the different methods. \n",
    "\n",
    "It requires the runoff statistics to have been computed, and the results of the XGBoost prediction model (catchment attributes $\\rightarrow$ hydrologic signatures / runoff statistics) to have been processed in Notebook 3.  It also requires the pre-processing of reference (baseline) distributions by KDE for validation from Notebook 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "558cd32f-b562-4502-ab63-826aede8f321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from time import time\n",
    "import geopandas as gpd\n",
    "\n",
    "import xgboost as xgb\n",
    "xgb.config_context(verbosity=2)\n",
    "from scipy.stats import norm, laplace, genextreme\n",
    "\n",
    "from collections import defaultdict\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from utils.kde_estimator import KDEEstimator\n",
    "from utils.knn_estimator import kNNEstimator\n",
    "from utils.fdc_estimator_context import FDCEstimationContext \n",
    "from utils.fdc_data import StationData\n",
    "from utils.evaluation_metrics import EvaluationMetrics\n",
    "\n",
    "import utils.data_processing_functions as dpf\n",
    "\n",
    "from pathlib import Path\n",
    "BASE_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1356ccc6-3d6d-4e08-bdbc-53a798187d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"b7913dba-7f99-49ca-9996-d5eb3547806a\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"b7913dba-7f99-49ca-9996-d5eb3547806a\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.7.3.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"b7913dba-7f99-49ca-9996-d5eb3547806a\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.layouts import gridplot\n",
    "\n",
    "import xyzservices.providers as xyz\n",
    "tiles = xyz['USGS']['USTopo']\n",
    "output_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef2d909f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "def warn_with_traceback(message, category, filename, lineno, file=None, line=None):\n",
    "    log = file if hasattr(file, 'write') else sys.stderr\n",
    "    traceback.print_stack(file=log)\n",
    "    log.write(warnings.formatwarning(message, category, filename, lineno, line))\n",
    "\n",
    "warnings.showwarning = warn_with_traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "894b3bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1098 monitored basins in the attribute set.\n"
     ]
    }
   ],
   "source": [
    "# load the catchment characteristics\n",
    "fname = f'catchment_attributes_with_runoff_stats.csv'\n",
    "attr_df = pd.read_csv(os.path.join('data', fname), dtype={'official_id': str, 'drainage_area_km2': float})\n",
    "attr_df.columns = [c.lower() for c in attr_df.columns]\n",
    "attr_df['log_drainage_area_km2'] = np.log(attr_df['drainage_area_km2'])\n",
    "# attr_df = attr_df[~attr_df['official_id'].isin(exclude)]\n",
    "# attr_df.columns = [c.lower() for c in attr_df.columns]\n",
    "attr_df['tmean'] = (attr_df['tmin'] + attr_df['tmax']) / 2.0\n",
    "station_ids = attr_df['official_id'].values\n",
    "# assert '12414900' in station_ids\n",
    "\n",
    "print(f'There are {len(station_ids)} monitored basins in the attribute set.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b463650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Watershed_ID</th>\n",
       "      <th>Source</th>\n",
       "      <th>Name</th>\n",
       "      <th>Official_ID</th>\n",
       "      <th>Centroid_Lat_deg_N</th>\n",
       "      <th>Centroid_Lon_deg_E</th>\n",
       "      <th>Drainage_Area_km2</th>\n",
       "      <th>Drainage_Area_GSIM_km2</th>\n",
       "      <th>Flag_GSIM_boundaries</th>\n",
       "      <th>Flag_Artificial_Boundaries</th>\n",
       "      <th>...</th>\n",
       "      <th>Land_Use_Wetland_frac</th>\n",
       "      <th>Land_Use_Water_frac</th>\n",
       "      <th>Land_Use_Urban_frac</th>\n",
       "      <th>Land_Use_Shrubs_frac</th>\n",
       "      <th>Land_Use_Crops_frac</th>\n",
       "      <th>Land_Use_Snow_Ice_frac</th>\n",
       "      <th>Flag_Land_Use_Extraction</th>\n",
       "      <th>Permeability_logk_m2</th>\n",
       "      <th>Porosity_frac</th>\n",
       "      <th>Flag_Subsoil_Extraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>847</td>\n",
       "      <td>HYDAT</td>\n",
       "      <td>CROWSNEST RIVER AT FRANK</td>\n",
       "      <td>05AA008</td>\n",
       "      <td>49.59732</td>\n",
       "      <td>-114.4106</td>\n",
       "      <td>402.6522</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>0.0785</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>1</td>\n",
       "      <td>-15.543306</td>\n",
       "      <td>0.170479</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>850</td>\n",
       "      <td>HYDAT</td>\n",
       "      <td>CASTLE RIVER NEAR BEAVER MINES</td>\n",
       "      <td>05AA022</td>\n",
       "      <td>49.48866</td>\n",
       "      <td>-114.1444</td>\n",
       "      <td>820.6510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.1156</td>\n",
       "      <td>0.0246</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>-15.929747</td>\n",
       "      <td>0.150196</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Watershed_ID Source                            Name Official_ID  \\\n",
       "846           847  HYDAT        CROWSNEST RIVER AT FRANK     05AA008   \n",
       "849           850  HYDAT  CASTLE RIVER NEAR BEAVER MINES     05AA022   \n",
       "\n",
       "     Centroid_Lat_deg_N  Centroid_Lon_deg_E  Drainage_Area_km2  \\\n",
       "846            49.59732           -114.4106           402.6522   \n",
       "849            49.48866           -114.1444           820.6510   \n",
       "\n",
       "     Drainage_Area_GSIM_km2  Flag_GSIM_boundaries  Flag_Artificial_Boundaries  \\\n",
       "846                     NaN                     0                           0   \n",
       "849                     NaN                     0                           0   \n",
       "\n",
       "     ...  Land_Use_Wetland_frac  Land_Use_Water_frac  Land_Use_Urban_frac  \\\n",
       "846  ...                 0.0103               0.0065               0.0328   \n",
       "849  ...                 0.0058               0.0023               0.0105   \n",
       "\n",
       "     Land_Use_Shrubs_frac  Land_Use_Crops_frac  Land_Use_Snow_Ice_frac  \\\n",
       "846                0.0785               0.0015                  0.0002   \n",
       "849                0.1156               0.0246                  0.0000   \n",
       "\n",
       "     Flag_Land_Use_Extraction  Permeability_logk_m2  Porosity_frac  \\\n",
       "846                         1            -15.543306       0.170479   \n",
       "849                         1            -15.929747       0.150196   \n",
       "\n",
       "     Flag_Subsoil_Extraction  \n",
       "846                        1  \n",
       "849                        1  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# streamflow folder from (updated) HYSETS\n",
    "HYSETS_DIR = Path('/home/danbot/code/common_data/HYSETS')\n",
    "# STREAMFLOW_DIR = HYSETS_DIR / 'streamflow'\n",
    "\n",
    "hs_df = pd.read_csv('data/HYSETS_watershed_properties.txt', sep=';', dtype={'Official_ID': str})\n",
    "hs_df = hs_df[hs_df['Official_ID'].isin(station_ids)]\n",
    "hs_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54a492f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1097\n"
     ]
    }
   ],
   "source": [
    "# load the baseline PMFs from the previous notebook\n",
    "pmf_path = Path(os.getcwd()) / 'data' / 'results' / 'baseline_distributions' / f'bcub_pmfs.csv'\n",
    "pmf_df = pd.read_csv(pmf_path, index_col=0)\n",
    "pmf_stations = pmf_df.columns\n",
    "station_ids = list(set(station_ids).intersection(set(pmf_stations)))\n",
    "print(len(station_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6751a465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 12414900 is in LSTM results but not in the station attributes.\n",
      "Warning: 15056030 is in LSTM results but not in the station attributes.\n",
      "There are 723 monitored basins concurrent with LSTM ensemble results.\n",
      "There are 1097 monitored basins with baseline PMFs.\n"
     ]
    }
   ],
   "source": [
    "# retrieve LSTM ensemble predictions\n",
    "lstm_result_folder = '/home/danbot/code/neuralhydrology/data/ensemble_results_20250514'\n",
    "lstm_result_files = os.listdir(lstm_result_folder)\n",
    "lstm_result_stns = [e.split('_')[0] for e in lstm_result_files]\n",
    "assert '12414900' in lstm_result_stns\n",
    "\n",
    "# find any non-matching station ids in the lstm result files\n",
    "for stn in lstm_result_stns:\n",
    "    if stn not in station_ids:\n",
    "        # try adding a leading zero\n",
    "        ending_in = [e for e in station_ids if e.endswith(stn)]\n",
    "        if len(ending_in) > 0:\n",
    "            print(stn, 'matches', ending_in)\n",
    "        modified_stn = stn.zfill(8)\n",
    "        if modified_stn in station_ids:\n",
    "            print(f'Found modified station id: {modified_stn} for {stn}')\n",
    "        else:\n",
    "            print(f'Warning: {stn} is in LSTM results but not in the station attributes.')\n",
    "\n",
    "# filter for the common stations between BCUB region and LSTM-compatible (i.e. 1980-)\n",
    "daymet_concurrent_stations = list(set(station_ids) & set(lstm_result_stns) & set(pmf_stations))\n",
    "# assert '12414900' in daymet_concurrent_stations\n",
    "print(f'There are {len(daymet_concurrent_stations)} monitored basins concurrent with LSTM ensemble results.')\n",
    "print(f'There are {len(pmf_stations)} monitored basins with baseline PMFs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4782d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import updated catchment polygons\n",
    "# poly_fpath = os.path.join(os.path.join('data', f'BCUB_watershed_attributes_updated_{rev_date}.csv'))\n",
    "# catchment_gdf = pd.read_csv(poly_fpath)\n",
    "# catchment\n",
    "# catchment_gdf = catchment_gdf[catchment_gdf['Official_ID'].isin(station_ids)]\n",
    "# print(len(catchment_gdf), 'catchments in the polygon set')\n",
    "\n",
    "# import the license water extraction points\n",
    "# dam_gdf = gpd.read_file('data/Dam_Points_20240103.gpkg')\n",
    "# assert dam_gdf.crs == catchment_gdf.crs, \"Catchment and dam geometries must have the same CRS\"\n",
    "# joined = gpd.sjoin(catchment_gdf, dam_gdf, how=\"inner\", predicate=\"contains\")\n",
    "# Create a new boolean column 'contains_dam' in catchment_gdf.\n",
    "# If a polygon's index appears in the joined result, it means it contains at least one point.\n",
    "# regulated = joined['Official_ID'].values\n",
    "# catchment_gdf[\"contains_dam\"] = catchment_gdf['Official_ID'].apply(lambda x: x in regulated)\n",
    "# n_regulated = catchment_gdf['contains_dam'].sum()\n",
    "# print(f'{n_regulated}/{len(catchment_gdf)} catchments contain withdrawal licenses')\n",
    "\n",
    "# # create dicts for easier access to 'official_id': 'drainage area', geometry, regulation status\n",
    "# da_dict = attr_df[['official_id', 'drainage_area_km2']].copy().set_index('official_id').to_dict()['drainage_area_km2']\n",
    "# dam_dict = catchment_gdf[['Official_ID', 'contains_dam']].copy().set_index('Official_ID').to_dict()['contains_dam']\n",
    "# polygon_dict = catchment_gdf[['Official_ID', 'geometry']].copy().set_index('Official_ID').to_dict()['geometry']\n",
    "\n",
    "# # add the centroid point geometry to the attributes dataframe\n",
    "# attr_df = attr_df[attr_df['official_id'].isin(catchment_gdf['Official_ID'].values)].copy()\n",
    "# centroids = attr_df.apply(lambda x: polygon_dict[x['official_id']].centroid, axis=1)\n",
    "# attr_gdf = gpd.GeoDataFrame(attr_df, geometry=centroids, crs=catchment_gdf.crs)\n",
    "# attr_gdf[\"contains_dam\"] = attr_gdf['official_id'].apply(lambda x: dam_dict[x] if x in dam_dict else False)\n",
    "# add the concurrency status as a boolean column\n",
    "# attr_df['LSTM_concurrent'] = attr_df['official_id'].apply(lambda x: x in daymet_concurrent_stations)\n",
    "# attr_df.reset_index(inplace=True, drop=True)\n",
    "# print(f'N network stations={len(attr_df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6f0e6e-e089-4951-a047-e0b49b6410c1",
   "metadata": {},
   "source": [
    "## Non-Parametric Simulation\n",
    "\n",
    "### Time-based ensemble\n",
    "\n",
    "A probability distribution $\\hat p = f(\\tilde x(t))$ is estimated for a target (ungauged location) by a weighted mean of runoff time-series from k nearest neighbour stations, $\\tilde x(t) = \\textbf{X}(t)\\cdot w$ where $X(t) \\in \\mathbb{R}^{N \\times k}$ and $w \\in \\mathbb{R}^{k\\times 1}$ is a vector of k weights.  So $\\hat p = f(\\textbf{X}(t) \\cdot w )$  Weights $w$ are computed in three ways, described in the next subsection, and k-nearest neighbours are selected using the criteria defined below.  Each gauged station in the monitoring network is treated as an ungauged location to generate a large sample of simulations across hydrologically diverse catchments, or rather as many catchments as can be tested.\n",
    "\n",
    "### Frequency-based ensembles\n",
    "\n",
    "A simulated probability density function is estimated from observations of k nearest neighbour stations.  First, k simulated series are generated by equal unit area runoff , $\\hat p = \\hat P \\cdot w$ where $\\hat P = [\\hat p_1, \\hat p_2, \\cdots, \\hat p_k]$ and each $\\hat p_i = f(X_i(t))$.\n",
    "\n",
    "In both cases, the function $f \\rightarrow \\hat p(x)$ represents kernel density estimation, which defines the probability density as $$\\hat p(x) = \\frac{1}{n \\cdot h(x)} \\sum_{i=1}^{n}K\\left( \\frac{x-x_i}{h(x)}\\right)$$ \n",
    "\n",
    "Where $h(x)$ reflects an adaptive kernel bandwidth that addresses vestiges of precision in the observed data to reflect the nature of streamflow as a continuous variable, and additionally incorporates piecewise linear model to represent overall measurement uncertainty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7d4695",
   "metadata": {},
   "source": [
    "## Notes on k-nearest neighbours\n",
    "\n",
    "Time series streamflow records vary widely in their temporal coverage, and finding k-nearest neighbours presents a tradeoff between selecting nearest neighbours and maximizing the number of observations concurrent with the target.  From the literature, concurrency is assured by pre-selecting a subset of stations with continuous records over a common period of record, or by infilling gaps with k-nearest neighbours simulation.  Some kind of tradeoff must be made, and we aim to use a method that maximizes information content while minimizing the number of assumptions.  The following notes are intended to clarify the implications of using k-nearest neighbours to fill gaps in the time series.\n",
    "\n",
    "1. **Infilled-by-kNN != Independent Proxy**: If a gap in an observation record is inferred from neighbors, it becomes redundant in the ensemble and increases the weight of the other (k minus n) neighbours.  So at that time step, its influence is non-unique, and including it in the ensemble is functionally equivalent to using the same set of other proxies directly, or just reducing the ensemble size.\n",
    "\n",
    "2. **Inflated Ensemble Size**: Filling gaps by \"nested\" k-nearest neighbours inflates the expresed number of independent neighbors.  Comparing the effectiveness of ensemble simulations as a function of k is then misleading because the effective number of independent proxies is *at most* k. \n",
    "\n",
    "3. **Information leakage risk**: If you repeatedly use kNN to fill missing data from within the same pool, especially when simulating extreme values, you risk suppressing variability by biasing toward the central tendency of the ensemble.  This defeats one of the core motivations for kNN: to preserve structure and variability from observations at neighboring stations.\n",
    "\n",
    "To address the nuance above, we propose three time-based methods for selecting k-nearest neighbours beyond strictly nodes in the network.  The problem is related to the set-cover problem where the goal is to select a subset of stations that maximizes the intersection of their data availability over a specified time period.  The following sections outline the three methods for selecting k-nearest neighbours based on availability of concurrent data.\n",
    "\n",
    "### Summary: Set-Theoretic Foundations of Strict k-NN Concurrency Selection\n",
    "\n",
    "This problem is closely related to classic combinatorial and set-theoretic optimization problems.\n",
    "\n",
    "#### Set-Theoretic Definition\n",
    "\n",
    "Let each column $( S_i \\subseteq T )$ represent the set of timestamps where station $( i )$ has valid (non-NaN) data.  \n",
    "Let $( \\mathcal{S} = \\{ S_1, S_2, \\dots, S_n \\} )$ be the collection of all such subsets, sorted by proximity (e.g., distance or attribute similarity).  \n",
    "The goal is to select a subset $( \\mathcal{K} \\subset \\mathcal{S} )$ such that:\n",
    "- $( |\\mathcal{K}| = k )$\n",
    "- $( \\bigcap_{S \\in \\mathcal{K}} S )$ satisfies a temporal completeness constraint (e.g., ≥5 years with ≥10 observations in each of 12 months)\n",
    "\n",
    "This is a constrained subset selection problem on the intersection of sets.\n",
    "\n",
    "#### Related Concepts\n",
    "\n",
    "| Concept                                 | Description |\n",
    "|----------------------------------------|-------------|\n",
    "| Set Intersection Selection             | Select \\( k \\) sets whose intersection satisfies a completeness constraint. |\n",
    "| Maximum Coverage under Cardinality Constraint | Choose \\( k \\) sets to maximize the coverage (or completeness) of their intersection. |\n",
    "| Recursive k-Subset Validation          | If the initial \\( k \\) sets fail, iteratively add more candidates and evaluate all \\( \\binom{k+1}{k} \\) combinations, and so on. |\n",
    "| NP-Hard Nature                         | This problem is computationally hard and shares structure with the Set Cover and Maximum Coverage problems. |\n",
    "\n",
    "#### Practical Implication\n",
    "\n",
    "This formulation justifies using greedy or approximate subset selection strategies when exhaustively testing all combinations becomes computationally infeasible.\n",
    "## Define a universal parametric prior\n",
    "\n",
    "In order to fairly test how parametric and non-parametric pdf estimation methods compare to each other, we need a consistent way to deal with indeterminate cases where the simulated distribution does not provide support coverage of the \"ground truth\" observations.  I feel two ways about this: the KL divergence is the culprit here, and the problem could be avoided by choosing another divergence measure.  However the definintion of KL divergence in information theoretic terms of compression make it seem more foundational than other measures, but ultimately is this true?  Should we look to math statistics to make more direct links between f-divergences and what we use as a discriminant for a particular application?  Should we be more concerned about \"Bayesian consistency\" of the discriminant (or surrogate loss function) with the choice of divergence measure?\n",
    "\n",
    "\n",
    "1.  **Quantify the distribution of unsupported mass across all models**.  It is important to describe the extent of the problem across the sample **and** across various methods.  i.e. discrete distributions have the issue of support coverage, but so do all methods!\n",
    "2.  Even in kNN / ensemble simulation approaches, the problem of incomplete support coverage necessitates assuming some prior probability.  The issue is that setting a uniform prior over the observed range takes advantage of information about the observed range.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008336e7-cca2-440e-99da-e3919f351793",
   "metadata": {},
   "source": [
    "### Global Uniform Prior\n",
    "\n",
    "$$f(x) = \\frac{1}{b-a}, \\quad x\\in (a, b) \\text{ and } f(x) = 0 \\text{ otherwise.}$$\n",
    "$$\\int_a^b f(x)\\text{dx} = 1$$\n",
    "\n",
    "Given the target range is a sub interval $(c, d) \\subseteq (a, b)$, then the **total** prior probability mass over (c, d) is:\n",
    "\n",
    "$$M_\\text{target} = \\int_c^d \\frac{1}{b-a}\\text{dx} = \\frac{d-c}{b-a}$$\n",
    "\n",
    "Over the set of intervals $\\Delta x_i$ covering the **target range**, the probability mass associated with each interval (bin) is given by: \n",
    "\n",
    "$$\\Delta x_i \\frac{d-c}{b-a}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2c621f-81ff-4b85-a15f-f3b9565ebf90",
   "metadata": {},
   "source": [
    "A desirable property of the prior is that it reflects the strength of belief in the model (data), where a smaller prior reflects stronger belief in the data/model and vice versa.  Dividing by the number of observations has such an effect, however it also makes for very small priors.  The consequence of very small priors is they have negligible effect on models that provide complete support coverage, and they severely penalize models that do not, resulting in a form of instability.  The very small prior creates a heavy tail in the distribution of a large sample of KL divergences, with further downstream effects in optimization.  \n",
    "\n",
    "A method that uses a prior with negligible effect on a model with complete support coverage and a very big effect on one without can be interpreted in a few ways:  \n",
    "\n",
    "1.  Incomplete support coverage, or underspecification, is very heavily penalized.  The method does not tolerate a model that cannot predict the full observed range.\n",
    "2.  A **proper** probability distribution sums (discrete) or integrates (continuous) to 1.  Very small probabilities are in a sense associated with a high degree of certainty since they reflect the expectation of the system being observed in a particular state.\n",
    "3.  The penalty of underestimating a state frequency is that storing and transmitting information about the state requires (the log ratio) more bandwidth/disk space because it is assigned a longer bit string than the actual frequency calls for under optimal encoding.\n",
    "4.  Assigning a very small probability to a state ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71a48368-9dda-4442-a09c-c506db94ba1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['uar_mean_mean_predicted', 'uar_mean_actual', 'uar_std_mean_predicted', 'uar_std_actual', 'uar_median_mean_predicted', 'uar_median_actual', 'uar_mad_mean_predicted', 'uar_mad_actual', 'log_uar_mean_mean_predicted', 'log_uar_mean_actual', 'log_uar_std_mean_predicted', 'log_uar_std_actual', 'log_uar_median_mean_predicted', 'log_uar_median_actual', 'log_uar_mad_mean_predicted', 'log_uar_mad_actual'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the predicted parameter results\n",
    "parameter_prediction_results_folder = os.path.join('data', 'results', 'parameter_prediction_results', )\n",
    "predicted_params_fpath   = os.path.join(parameter_prediction_results_folder, 'mean_parameter_predictions.csv')\n",
    "rdf = pd.read_csv(predicted_params_fpath, index_col=['official_id'], dtype={'official_id': str})\n",
    "predicted_param_dict = rdf.to_dict(orient='index')\n",
    "predicted_param_dict['0212414900'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "646ea56f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"dfe6f996-36c3-4340-9550-9a80c81719ed\" data-root-id=\"p1102\" style=\"display: contents;\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function embed_document(root) {\n  const docs_json = {\"9e378111-5cff-41f9-b9d7-fabbc00ef210\":{\"version\":\"3.7.3\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"GridPlot\",\"id\":\"p1102\",\"attributes\":{\"rows\":null,\"cols\":null,\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1101\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"ToolProxy\",\"id\":\"p1095\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1025\"},{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1071\"}]}},{\"type\":\"object\",\"name\":\"ToolProxy\",\"id\":\"p1096\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1026\",\"attributes\":{\"renderers\":\"auto\"}},{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1072\",\"attributes\":{\"renderers\":\"auto\"}}]}},{\"type\":\"object\",\"name\":\"ToolProxy\",\"id\":\"p1097\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1027\",\"attributes\":{\"dimensions\":\"both\",\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1028\",\"attributes\":{\"syncable\":false,\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5,\"level\":\"overlay\",\"visible\":false,\"left\":{\"type\":\"number\",\"value\":\"nan\"},\"right\":{\"type\":\"number\",\"value\":\"nan\"},\"top\":{\"type\":\"number\",\"value\":\"nan\"},\"bottom\":{\"type\":\"number\",\"value\":\"nan\"},\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"handles\":{\"type\":\"object\",\"name\":\"BoxInteractionHandles\",\"id\":\"p1034\",\"attributes\":{\"all\":{\"type\":\"object\",\"name\":\"AreaVisuals\",\"id\":\"p1033\",\"attributes\":{\"fill_color\":\"white\",\"hover_fill_color\":\"lightgray\"}}}}}}}},{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1073\",\"attributes\":{\"dimensions\":\"both\",\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1074\",\"attributes\":{\"syncable\":false,\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5,\"level\":\"overlay\",\"visible\":false,\"left\":{\"type\":\"number\",\"value\":\"nan\"},\"right\":{\"type\":\"number\",\"value\":\"nan\"},\"top\":{\"type\":\"number\",\"value\":\"nan\"},\"bottom\":{\"type\":\"number\",\"value\":\"nan\"},\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"handles\":{\"type\":\"object\",\"name\":\"BoxInteractionHandles\",\"id\":\"p1080\",\"attributes\":{\"all\":{\"type\":\"object\",\"name\":\"AreaVisuals\",\"id\":\"p1079\",\"attributes\":{\"fill_color\":\"white\",\"hover_fill_color\":\"lightgray\"}}}}}}}}]}},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1098\"},{\"type\":\"object\",\"name\":\"ToolProxy\",\"id\":\"p1099\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1036\"},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1082\"}]}},{\"type\":\"object\",\"name\":\"ToolProxy\",\"id\":\"p1100\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"HelpTool\",\"id\":\"p1037\"},{\"type\":\"object\",\"name\":\"HelpTool\",\"id\":\"p1083\"}]}}]}},\"children\":[[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1003\",\"attributes\":{\"width\":400,\"height\":400,\"x_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1004\"},\"y_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1005\"},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1013\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1014\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1006\",\"attributes\":{\"text\":\"Predicted log_uar_mean_mean_predicted\"}},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1044\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1038\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1039\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1040\"},\"data\":{\"type\":\"map\",\"entries\":[[\"top\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"IPle8xx7hz8q+V7zHHt3P+A6h7ZVnJE/2DqHtlWcoT8l+V7zHHuXPwAAAAAAAAAA2DqHtlWckT8q+V7zHHunP01LUeejJLA/aiq9hQcUsz8g+V7zHHunPyr5XvMce6c/TUtR56MksD9dtzYw5Fm9PwanbP+V0b4/dLc2MORZvT9yKr2FBxTDPzwiWG3gz8M/O9jKkYBqyj8Y0GV5WSbLP+A6h7ZVnNE/oo1bsH6l1T+rZGI2u1DZPyr5XvMce9c/V6aK+fNx0z/z8Pna9TbYP0V9kX8wHdc/xDqHtlWc0T8Gp2z/ldHOP9CeB+dujc8/Bqds/5XRzj9hAcQLRL/GPz6v0Re9Fc4/PCJYbeDP0z9Xpor583HTP+LHAGEy4ss/NEtR56MksD8q+V7zHHuXPyr5XvMce4c/Kvle8xx7hz8=\"},\"shape\":[40],\"dtype\":\"float64\",\"order\":\"little\"}],[\"left\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"h8+MNKeh+L8itiaj6xb2v76cwBEwjPO/WoNagHQB8b/q0+jdce3svyGhHLv61+e/WG5QmIPC4r8cdwjrGFrbv4wRcKUqL9G/8K9ef/EQvL/AzAUujzWpP1g+slZAo8o/vITxcI581z8m9URbvtPgP/InEX416eU/ulrdoKz+6j/BxtThEQrwPyXgOnPNlPI/ifmgBIkf9T/vEgeWRKr3P1MsbScANfo/t0XTuLu//D8dXzlKd0r/P0C8z22Z6gBA8siCNvcvAkCk1TX/VHUDQFbi6MeyugRACO+bkBAABkC8+05ZbkUHQG4IAiLMighAIBW16inQCUDSIWizhxULQIQuG3zlWgxANjvOREOgDUDoR4ENoeUOQE0qGmt/FRBAprBzTy64EEAAN80z3VoRQFm9JhiM/RFAskOA/DqgEkA=\"},\"shape\":[40],\"dtype\":\"float64\",\"order\":\"little\"}],[\"right\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"IrYmo+sW9r++nMARMIzzv1qDWoB0AfG/6tPo3XHt7L8hoRy7+tfnv1huUJiDwuK/HHcI6xha27+MEXClKi/Rv/CvXn/xELy/wMwFLo81qT9YPrJWQKPKP7yE8XCOfNc/JvVEW77T4D/yJxF+NenlP7pa3aCs/uo/wcbU4REK8D8l4DpzzZTyP4n5oASJH/U/7xIHlkSq9z9TLG0nADX6P7dF07i7v/w/HV85SndK/z9AvM9tmeoAQPLIgjb3LwJApNU1/1R1A0BW4ujHsroEQAjvm5AQAAZAvPtOWW5FB0BuCAIizIoIQCAVteop0AlA0iFos4cVC0CELht85VoMQDY7zkRDoA1A6EeBDaHlDkBNKhprfxUQQKawc08uuBBAADfNM91aEUBZvSYYjP0RQLJDgPw6oBJAC8rZ4OlCE0A=\"},\"shape\":[40],\"dtype\":\"float64\",\"order\":\"little\"}]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1045\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1046\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Quad\",\"id\":\"p1041\",\"attributes\":{\"left\":{\"type\":\"field\",\"field\":\"left\"},\"right\":{\"type\":\"field\",\"field\":\"right\"},\"bottom\":{\"type\":\"value\",\"value\":0},\"top\":{\"type\":\"field\",\"field\":\"top\"},\"fill_color\":{\"type\":\"value\",\"value\":\"lightblue\"}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Quad\",\"id\":\"p1042\",\"attributes\":{\"left\":{\"type\":\"field\",\"field\":\"left\"},\"right\":{\"type\":\"field\",\"field\":\"right\"},\"bottom\":{\"type\":\"value\",\"value\":0},\"top\":{\"type\":\"field\",\"field\":\"top\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_color\":{\"type\":\"value\",\"value\":\"lightblue\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Quad\",\"id\":\"p1043\",\"attributes\":{\"left\":{\"type\":\"field\",\"field\":\"left\"},\"right\":{\"type\":\"field\",\"field\":\"right\"},\"bottom\":{\"type\":\"value\",\"value\":0},\"top\":{\"type\":\"field\",\"field\":\"top\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"value\",\"value\":\"lightblue\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1012\",\"attributes\":{\"tools\":[{\"id\":\"p1025\"},{\"id\":\"p1026\"},{\"id\":\"p1027\"},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1035\"},{\"id\":\"p1036\"},{\"id\":\"p1037\"}]}},\"toolbar_location\":null,\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1020\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1021\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1022\"},\"axis_label\":\"$$P(x)$$\",\"axis_label_text_font\":\"Bitstream Charter\",\"axis_label_text_font_size\":\"14pt\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1023\"},\"major_label_text_font\":\"Bitstream Charter\",\"major_label_text_font_size\":\"12pt\"}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1015\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1016\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1017\"},\"axis_label\":\"$$\\\\text{Log Mean UAR }(L/s/\\\\text{km}^2)$$\",\"axis_label_text_font\":\"Bitstream Charter\",\"axis_label_text_font_size\":\"14pt\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1018\"},\"major_label_text_font\":\"Bitstream Charter\",\"major_label_text_font_size\":\"12pt\"}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1019\",\"attributes\":{\"axis\":{\"id\":\"p1015\"}}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1024\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1020\"}}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1047\",\"attributes\":{\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1048\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"\"},\"renderers\":[{\"id\":\"p1044\"}]}}]}}]}},0,0],[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1049\",\"attributes\":{\"width\":400,\"height\":400,\"x_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1050\"},\"y_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1051\"},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1059\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1060\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1052\",\"attributes\":{\"text\":\"Predicted log_uar_std_mean_predicted\"}},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1090\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1084\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1085\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1086\"},\"data\":{\"type\":\"map\",\"entries\":[[\"top\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"8FgmWx7mqT+zwlzElmzTPw0kiybiIt0/iPYWWM014j+qih6tSbvsPxJAKrKIe/g/crwW/3SQ/z+uUCwK6hgBQEDW80gQ9f4/Cnamh8o48z+jwJqCi3jnP4739/jSL+A/x43Bj1qp1j/0j4PRZZ3SP/tYJlse5sk/x43Bj1qptj+PiT0MRMHOP7vCXMSWbLM/3I3Bj1qptj/kWCZbHuapPwAAAAAAAAAAAAAAAAAAAAARWSZbHuaZP+RYJlse5pk/q8JcxJZsoz8RWSZbHuaJP+RYJlse5pk/5FgmWx7miT/kWCZbHuaZPwAAAAAAAAAA5FgmWx7miT8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAORYJlse5ok/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA5FgmWx7miT8=\"},\"shape\":[40],\"dtype\":\"float64\",\"order\":\"little\"}],[\"left\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"TL8rH9Wy4j83v7WhzwDlPyK/PyTKTuc/Db/JpsSc6T/4vlMpv+rrP+O+3au5OO4/Z98zF1pD8D9c33hYV2rxP1LfvZlUkfI/SN8C21G48z8930ccT9/0PzLfjF1MBvY/KN/Rnkkt9z8e3xbgRlT4PxPfWyFEe/k/CN+gYkGi+j/+3uWjPsn7P/TeKuU78Pw/6d5vJjkX/j/e3rRnNj7/P2rvfNSZMgBAZW8fdRjGAEBg78EVl1kBQFpvZLYV7QFAVe8GV5SAAkBQb6n3EhQDQErvS5iRpwNARW/uOBA7BEBA75DZjs4EQDtvM3oNYgVANe/VGoz1BUAwb3i7CokGQCvvGlyJHAdAJm+9/AewB0Ah71+dhkMIQBtvAj4F1whAFu+k3oNqCUARb0d/Av4JQAvv6R+BkQpABm+MwP8kC0A=\"},\"shape\":[40],\"dtype\":\"float64\",\"order\":\"little\"}],[\"right\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"N7+1oc8A5T8ivz8kyk7nPw2/yabEnOk/+L5TKb/q6z/jvt2ruTjuP2ffMxdaQ/A/XN94WFdq8T9S372ZVJHyP0jfAttRuPM/Pd9HHE/f9D8y34xdTAb2Pyjf0Z5JLfc/Ht8W4EZU+D8T31shRHv5PwjfoGJBovo//t7loz7J+z/03irlO/D8P+nebyY5F/4/3t60ZzY+/z9q73zUmTIAQGVvH3UYxgBAYO/BFZdZAUBab2S2Fe0BQFXvBleUgAJAUG+p9xIUA0BK70uYkacDQEVv7jgQOwRAQO+Q2Y7OBEA7bzN6DWIFQDXv1RqM9QVAMG94uwqJBkAr7xpciRwHQCZvvfwHsAdAIe9fnYZDCEAbbwI+BdcIQBbvpN6DaglAEW9HfwL+CUAL7+kfgZEKQAZvjMD/JAtAAe8uYX64C0A=\"},\"shape\":[40],\"dtype\":\"float64\",\"order\":\"little\"}]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1091\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1092\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Quad\",\"id\":\"p1087\",\"attributes\":{\"left\":{\"type\":\"field\",\"field\":\"left\"},\"right\":{\"type\":\"field\",\"field\":\"right\"},\"bottom\":{\"type\":\"value\",\"value\":0},\"top\":{\"type\":\"field\",\"field\":\"top\"},\"fill_color\":{\"type\":\"value\",\"value\":\"lightblue\"}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Quad\",\"id\":\"p1088\",\"attributes\":{\"left\":{\"type\":\"field\",\"field\":\"left\"},\"right\":{\"type\":\"field\",\"field\":\"right\"},\"bottom\":{\"type\":\"value\",\"value\":0},\"top\":{\"type\":\"field\",\"field\":\"top\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_color\":{\"type\":\"value\",\"value\":\"lightblue\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Quad\",\"id\":\"p1089\",\"attributes\":{\"left\":{\"type\":\"field\",\"field\":\"left\"},\"right\":{\"type\":\"field\",\"field\":\"right\"},\"bottom\":{\"type\":\"value\",\"value\":0},\"top\":{\"type\":\"field\",\"field\":\"top\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"value\",\"value\":\"lightblue\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1058\",\"attributes\":{\"tools\":[{\"id\":\"p1071\"},{\"id\":\"p1072\"},{\"id\":\"p1073\"},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1081\"},{\"id\":\"p1082\"},{\"id\":\"p1083\"}]}},\"toolbar_location\":null,\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1066\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1067\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1068\"},\"axis_label\":\"$$P(x)$$\",\"axis_label_text_font\":\"Bitstream Charter\",\"axis_label_text_font_size\":\"14pt\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1069\"},\"major_label_text_font\":\"Bitstream Charter\",\"major_label_text_font_size\":\"12pt\"}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1061\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1062\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1063\"},\"axis_label\":\"$$\\\\text{Log SD UAR }(L/s/\\\\text{km}^2)$$\",\"axis_label_text_font\":\"Bitstream Charter\",\"axis_label_text_font_size\":\"14pt\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1064\"},\"major_label_text_font\":\"Bitstream Charter\",\"major_label_text_font_size\":\"12pt\"}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1065\",\"attributes\":{\"axis\":{\"id\":\"p1061\"}}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1070\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1066\"}}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1093\",\"attributes\":{\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1094\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"\"},\"renderers\":[{\"id\":\"p1090\"}]}}]}}]}},0,1]]}}]}};\n  const render_items = [{\"docid\":\"9e378111-5cff-41f9-b9d7-fabbc00ef210\",\"roots\":{\"p1102\":\"dfe6f996-36c3-4340-9550-9a80c81719ed\"},\"root_ids\":[\"p1102\"]}];\n  void root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n  }\n  if (root.Bokeh !== undefined) {\n    embed_document(root);\n  } else {\n    let attempts = 0;\n    const timer = setInterval(function(root) {\n      if (root.Bokeh !== undefined) {\n        clearInterval(timer);\n        embed_document(root);\n      } else {\n        attempts++;\n        if (attempts > 100) {\n          clearInterval(timer);\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n        }\n      }\n    }, 10, root)\n  }\n})(window);",
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "p1102"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plots = []\n",
    "predicted_param_sample = {}\n",
    "for l, al in zip(['log_uar_mean_mean_predicted', 'log_uar_std_mean_predicted'], [r'$$\\text{Log Mean UAR }(L/s/\\text{km}^2)$$', r'$$\\text{Log SD UAR }(L/s/\\text{km}^2)$$']):\n",
    "    vals = [d[l] for _, d in predicted_param_dict.items()]\n",
    "    predicted_param_sample[l] = vals\n",
    "    # plot the histogram of the mean_uar values\n",
    "    hist, edges = np.histogram(vals, bins=40, density=True)\n",
    "    # create a scatter plot of the predicted parameter vs the target parameter\n",
    "    f = figure(title=f'Predicted {l}', width=600, height=400)\n",
    "    f.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:], fill_color='lightblue', line_color='black', legend_label='')\n",
    "    f.xaxis.axis_label = al\n",
    "    f.yaxis.axis_label = r'$$P(x)$$'\n",
    "    f = dpf.format_fig_fonts(f, font_size=14)\n",
    "    plots.append(f)\n",
    "# retrieve all the mean_uar values \n",
    "\n",
    "lt = gridplot(plots, ncols=2, width=400, height=400)\n",
    "show(lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46b4bbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FDCEstimatorRunner:\n",
    "    def __init__(self, stn_id, ctx, methods, k_nearest, parametric_target_cols, estimator_classes, **kwargs):\n",
    "        self.stn_id = stn_id\n",
    "        self.ctx = ctx\n",
    "        self.methods = methods\n",
    "        self.k_nearest = k_nearest\n",
    "        self.parametric_target_cols = parametric_target_cols\n",
    "        # self._check_min_overlap()\n",
    "        self._create_results_folders()\n",
    "        self._create_readme()\n",
    "        self.ESTIMATOR_CLASSES = estimator_classes\n",
    "        self.prior_strength = ctx.prior_strength\n",
    "\n",
    "    def _create_results_folders(self):\n",
    "        # create a results foder for each method if it doesn't exist\n",
    "        self.results_folder = os.path.join('data', 'results', f'fdc_estimation_results',)\n",
    "        for method in self.methods:\n",
    "            method_folder = os.path.join(self.results_folder, method)\n",
    "            if not os.path.exists(method_folder):\n",
    "                os.makedirs(method_folder)\n",
    "\n",
    "    \n",
    "    def _create_readme(self):\n",
    "        # create a readme file in the results folder to list constraints\n",
    "        readme_file = os.path.join(self.results_folder, 'README.txt')\n",
    "        \n",
    "        with open(readme_file, 'w') as file:\n",
    "            file.write(\"This folder contains the results of the FDC estimation.\\n\")\n",
    "            file.write(f\"Methods evaluated: {', '.join(self.methods)}\\n\")\n",
    "            # add the concurrency constraint and number of stations represented in the network\n",
    "            N = len(self.ctx.official_ids)\n",
    "            if self.ctx.LSTM_concurrent_network == True:\n",
    "                file.write(f'Uses only stations within Daymet input period of record / LSTM results: N={N} stations in the network.\\n')\n",
    "                file.write(f'Global start date on streamflow data: {self.ctx.global_start_date}\\n')\n",
    "            else:\n",
    "                file.write(f'Uses all available network stations in the BCUB region (1950-2024): N={N} stationsin the network.')\n",
    "                \n",
    "\n",
    "    def _load_reference_distributions(self):\n",
    "        self.kde = KDEEstimator(self.data.baseline_log_grid, self.data.log_dx)\n",
    "        self.baseline_pmf, self.baseline_pdf = self.data.baseline_pmf, self.data.baseline_pdf        \n",
    "        self.ctx.baseline_pmf = self.baseline_pmf\n",
    "\n",
    "\n",
    "    def _save_result(self, result):\n",
    "        with open(self.result_file, 'w') as file:\n",
    "            json.dump(result, file, indent=4)\n",
    "\n",
    " \n",
    "    def run_selected(self):\n",
    "        # check the minimum number of years of overlap for all stations in self.ctx.overlap_dict\n",
    "        for method in self.methods:\n",
    "            self.result_file = os.path.join(self.results_folder, method, f'{self.stn_id}_fdc_results.json')\n",
    "            if os.path.exists(self.result_file):\n",
    "                continue\n",
    "            else:\n",
    "                self.data = StationData(self.ctx, self.stn_id)\n",
    "                self.data.k_nearest = self.k_nearest\n",
    "                self.data.parametric_target_cols = self.parametric_target_cols\n",
    "                self._load_reference_distributions()\n",
    "            try:\n",
    "                eval_metrics = EvaluationMetrics(self.data.baseline_log_grid, self.data.log_dx)\n",
    "                EstimatorClass = self.ESTIMATOR_CLASSES[method]\n",
    "                estimator = EstimatorClass(\n",
    "                    self.ctx, self.stn_id, self.data\n",
    "                )\n",
    "                self.data.eval_metrics = eval_metrics\n",
    "                result = estimator.run_estimators()\n",
    "                self._save_result(result)\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"  {method} estimator failed for {self.stn_id}: {str(e)}\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b680881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParametricFDCEstimator:\n",
    "    def __init__(self, ctx, target_stn, data, *args, **kwargs):\n",
    "        # super().__init__(*args, **kwargs)\n",
    "        self.ctx = ctx\n",
    "        self.target_stn = target_stn\n",
    "        self.data = data\n",
    "        # self.data = data\n",
    "        self.predicted_param_dict = self.ctx.predicted_param_dict\n",
    "        self.predicted_param_df = pd.DataFrame(self.predicted_param_dict).T\n",
    "\n",
    "\n",
    "    def _compute_lognorm_pmf(self, mu, sigma):\n",
    "        pdf = norm.pdf(self.data.baseline_log_grid, loc=mu, scale=sigma)\n",
    "        pdf /= jnp.trapezoid(pdf, x=self.data.baseline_log_grid)\n",
    "        pmf = pdf * self.data.log_dx\n",
    "        pmf /= pmf.sum()\n",
    "        return pmf, pdf\n",
    "    \n",
    "\n",
    "    def _compute_GEV_pmf(self, xi, mu, sigma):\n",
    "        # assert values are within the valid range for GEV\n",
    "        xi = max(xi, -0.5 + 1e-12)  # clip xi to avoid numerical issues\n",
    "        sigma = max(sigma, 1e-12)  # ensure sigma is positive\n",
    "        pdf = genextreme.pdf(self.data.baseline_log_grid, xi, loc=mu, scale=sigma)\n",
    "        pdf /= jnp.trapezoid(pdf, x=self.data.baseline_log_grid)\n",
    "        pmf = pdf * self.data.log_dx\n",
    "        pmf /= pmf.sum()  # normalize raw PMF\n",
    "        return pmf, pdf\n",
    "\n",
    "\n",
    "    def _estimate_from_mle(self):\n",
    "        log_mu = self.predicted_param_dict[self.target_stn]['log_uar_mean_actual']\n",
    "        log_sigma = self.predicted_param_dict[self.target_stn]['log_uar_std_actual']\n",
    "        return self._compute_lognorm_pmf(log_mu, log_sigma)\n",
    "\n",
    "\n",
    "    # def _estimate_from_observed_lmoments_gev(self):\n",
    "    #     # compute the GEV parameters from the L-moments\n",
    "    #     xi = self.data.LN_param_dict['logx_lmom_xi'][self.target_stn]['actual']\n",
    "    #     loc = self.data.LN_param_dict['logx_lmom_loc'][self.target_stn]['actual']\n",
    "    #     scale = self.data.LN_param_dict['logx_lmom_scale'][self.target_stn]['actual']\n",
    "    #     return self._compute_GEV_pmf(xi, loc, scale)\n",
    "    \n",
    "\n",
    "    def _estimate_from_predicted_log_params(self):\n",
    "        mu = self.predicted_param_dict[self.target_stn]['log_uar_mean_mean_predicted']\n",
    "        sigma = self.predicted_param_dict[self.target_stn]['log_uar_std_mean_predicted']\n",
    "        return self._compute_lognorm_pmf(mu, sigma)\n",
    "        \n",
    "    \n",
    "    def _estimate_from_predicted_linear_mom(self):\n",
    "        mean_x = self.predicted_param_dict[self.target_stn]['uar_mean_mean_predicted']\n",
    "        sd_x = self.predicted_param_dict[self.target_stn]['uar_std_mean_predicted']\n",
    "        v = np.log(1 + (sd_x / mean_x) ** 2)\n",
    "        mu = np.log(mean_x) - 0.5 * v\n",
    "        return self._compute_lognorm_pmf(mu, np.sqrt(v))\n",
    "    \n",
    "\n",
    "    def _estimate_LN_from_randomly_drawn_params(self):\n",
    "        # randomly draw from the predicted parameters\n",
    "        random_idx = np.random.choice(len(self.predicted_param_df))\n",
    "        random_stn_idx = self.predicted_param_df.index[random_idx]\n",
    "        mu_random =self.predicted_param_dict[random_stn_idx]['log_uar_mean_mean_predicted']\n",
    "        sigma_random = self.predicted_param_dict[random_stn_idx]['log_uar_std_mean_predicted']\n",
    "        return self._compute_lognorm_pmf(mu_random, sigma_random)\n",
    "\n",
    "\n",
    "    def run_estimators(self):\n",
    "        results = {}\n",
    "        fns = [\n",
    "            self._estimate_from_mle, \n",
    "            self._estimate_from_predicted_log_params,\n",
    "            self._estimate_from_predicted_linear_mom, \n",
    "            self._estimate_LN_from_randomly_drawn_params,\n",
    "            # self._estimate_from_observed_lmoments_gev,\n",
    "            # self._estimate_from_predicted_lmoments_gev, \n",
    "            # self._estimate_LMOM_gev_from_randomly_drawn_params\n",
    "            ]\n",
    "        labels = ['MLE', 'PredictedLog', 'PredictedMOM', 'RandomDraw', \n",
    "                  #'ObsLMomentsGEV', 'PredictedLMomentsGEV', 'LMomentsGEVRandomDraw',\n",
    "                  ]\n",
    "        for fn, label in zip(fns, labels):\n",
    "            pmf, pdf = fn()\n",
    "            _, pmf_posterior = self.data._compute_posterior_with_laplace_prior(pmf)            \n",
    "            if 'Moments' in label:\n",
    "                # assert no nan values in the pmf\n",
    "                assert not np.any(np.isnan(pmf)), f'PMF contains NaN values for {label}: {pmf[:10]}'\n",
    "\n",
    "            results[label] = {'pmf_posterior': pmf_posterior.tolist(), 'pmf': pmf.tolist()}\n",
    "\n",
    "            estimation_metrics = self.data.eval_metrics._evaluate_fdc_metrics_from_pmf(pmf_posterior, self.data.baseline_pmf)\n",
    "            results[label]['eval'] = estimation_metrics\n",
    "\n",
    "            # compute the bias\n",
    "            bias_metrics = self.data.eval_metrics._evaluate_fdc_metrics_from_pmf(pmf_posterior, pmf)\n",
    "            results[label]['bias'] = bias_metrics\n",
    "                \n",
    "        # compute the bias from the eps\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd61235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMFDCEstimator:\n",
    "    def __init__(self, ctx, target_stn, data, *args, **kwargs):\n",
    "        # super().__init__(*args, **kwargs)\n",
    "        self.ctx = ctx\n",
    "        self.target_stn = target_stn\n",
    "        self.data = data\n",
    "        # self.data = data\n",
    "        self.LSTM_forcings_folder = self.ctx.LSTM_forcings_folder\n",
    "        self.LSTM_ensemble_result_folder = self.ctx.LSTM_ensemble_result_folder\n",
    "        self.df = self._load_ensemble_result()\n",
    "        self.df = self._filter_for_complete_years()\n",
    "        self.sim_cols = sorted([c for c in self.df.columns if c.startswith('streamflow_sim_')])\n",
    "        self.kde = KDEEstimator(self.data.baseline_log_grid, self.data.log_dx)\n",
    "\n",
    "\n",
    "    def _load_ensemble_result(self):\n",
    "        fpath = os.path.join(self.LSTM_ensemble_result_folder, f'{self.target_stn}_ensemble.csv')\n",
    "        df = pd.read_csv(fpath)\n",
    "        # rename 'Unnamed: 0' to 'time' and set to index\n",
    "        df.rename(columns={'Unnamed: 0': 'time'}, inplace=True)\n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "        df.set_index('time', inplace=True)\n",
    "        return df\n",
    "    \n",
    "\n",
    "    def _filter_for_complete_years(self):\n",
    "        # Convert to datetime only if necessary\n",
    "        if self.df.empty:\n",
    "            return pd.DataFrame()\n",
    "        date_column = 'time'\n",
    "        self.df.reset_index(inplace=True)\n",
    "        if not np.issubdtype(self.df[date_column].dtype, np.datetime64):\n",
    "            self.df = self.df.copy()\n",
    "            self.df[date_column] = pd.to_datetime(self.df[date_column])\n",
    "\n",
    "        # Filter out missing values first\n",
    "        valid_data = self.df.copy().dropna()\n",
    "\n",
    "        # Extract year and month\n",
    "        valid_data['year'] = valid_data[date_column].dt.year\n",
    "        valid_data['month'] = valid_data[date_column].dt.month\n",
    "        valid_data['day'] = valid_data[date_column].dt.day\n",
    "        \n",
    "        # Count total and missing days per year-month group\n",
    "        month_counts = valid_data.groupby(['year', 'month'])['day'].nunique()\n",
    "        \n",
    "        # Identify complete months (at least 20 observations)\n",
    "        complete_months = (month_counts >= 20)\n",
    "\n",
    "        # count how many complete months per year\n",
    "        complete_month_counts = complete_months.groupby(level=0).sum()\n",
    "        \n",
    "        complete_years = complete_month_counts[complete_month_counts == 12]\n",
    "        self.complete_years = list(complete_years.index.values)\n",
    "\n",
    "        valid_data = valid_data[valid_data['year'].isin(complete_years.index)].copy()\n",
    "        # drop the year column\n",
    "        return valid_data.drop(columns=['year', 'month', 'day'])\n",
    "    \n",
    "    \n",
    "    def _load_LSTM_forcing_file(self):\n",
    "        # retrieve LSTM forcing data\n",
    "        # read the forcing data from the LSTM forcing file\n",
    "        # and return a dataframe with the same index as the LSTM results\n",
    "        ldf = pd.read_csv(os.path.join(self.met_forcings_folder, f'{self.target_stn}_forcing.csv'))\n",
    "        ldf.rename(columns={'Unnamed: 0': 'time'}, inplace=True)\n",
    "        ldf['time'] = pd.to_datetime(ldf['time'])\n",
    "        ldf.set_index('time', inplace=True)\n",
    "        ldf = ldf.loc[self.stn_df.index]\n",
    "        # convert to unit area runoff (L/s/km2)\n",
    "        ldf['uar'] = 1000 * ldf['discharge'] / self.target_da\n",
    "        return ldf\n",
    "\n",
    "    \n",
    "    def _plot_pmfs(self, pmf_time, pmf_freq, line_dash='solid'):\n",
    "        # plot using bokeh\n",
    "        f = figure(title=self.target_stn, width=600, height=400)\n",
    "        f.line(self.data.baseline_log_grid, pmf_time, line_width=2, color='blue', legend_label='Time Ensemble', line_dash=line_dash)\n",
    "        # f.line(self.data.baseline_log_grid, pmf1, line_width=2, color='red', legend_label='T_MeanLinEns PMF', line_dash=line_dash)\n",
    "        f.line(self.data.baseline_log_grid, pmf_freq, line_width=2, color='purple', legend_label='Frequency Ensemble', line_dash=line_dash)\n",
    "        f.line(self.data.baseline_log_grid, self.ctx.baseline_pmf, line_width=2, color='green', legend_label='Observed', line_dash=line_dash)\n",
    "        f.xaxis.axis_label = 'Log UAR (L/s/km2)'\n",
    "        f.yaxis.axis_label = 'PMF'\n",
    "        f.legend.location = 'top_left'\n",
    "        f.legend.background_fill_alpha = 0.25\n",
    "        f.legend.click_policy = 'hide'\n",
    "        f = dpf.format_fig_fonts(f, font_size=14)\n",
    "        show(f)\n",
    "\n",
    "\n",
    "    def _compute_time_ensemble_pmf(self):\n",
    "        data = self.df[self.sim_cols].copy()\n",
    "        temporal_ensemble_log = data.mean(axis=1) # this is still in log space\n",
    "        self.temporal_ensemble = np.exp(temporal_ensemble_log.values)\n",
    "        pmf, _ = self.kde.compute(self.temporal_ensemble, self.data.target_da)\n",
    "        _, pmf_posterior = self.data._compute_posterior_with_laplace_prior(pmf)\n",
    "        return (pmf, pmf_posterior)\n",
    "\n",
    "\n",
    "    def _compute_frequency_ensemble_pmf(self):\n",
    "        data = self.df[self.sim_cols].copy()\n",
    "        data.dropna(inplace=True)\n",
    "        # compute the frequency ensemble PMF\n",
    "        # initialize a len(data) x n_sim_cols array\n",
    "        pmfs = np.column_stack([\n",
    "            self.kde.compute(np.exp(data[c].values), self.data.target_da)[0]\n",
    "            for c in self.sim_cols\n",
    "        ])\n",
    "        # average the pmfs over the ensemble \n",
    "        pmf = pmfs.mean(axis=1)\n",
    "        assert len(pmf) == len(self.data.baseline_log_grid), f'len(pmfs) = {len(pmfs)} != len(baseline_log_grid) = {len(self.data.baseline_log_grid)}' \n",
    "        _, pmf_posterior = self.data._compute_posterior_with_laplace_prior(pmf)\n",
    "        return (pmf, pmf_posterior)\n",
    "\n",
    "\n",
    "    def _compute_ensemble_distribution_estimate(self, ensemble_type):\n",
    "        if ensemble_type == 'time':\n",
    "            pmf, pmf_posterior = self._compute_time_ensemble_pmf()\n",
    "        elif ensemble_type == 'frequency':\n",
    "            pmf, pmf_posterior = self._compute_frequency_ensemble_pmf()\n",
    "        else:\n",
    "            raise ValueError(f'Unknown ensemble type: {ensemble_type}')\n",
    "        \n",
    "        # compute the divergence measures\n",
    "        result = {}\n",
    "        result['pmf'] = pmf.tolist()\n",
    "        result['pmf_posterior'] = pmf_posterior.tolist()\n",
    "        result['eval'] = self.data.eval_metrics._evaluate_fdc_metrics_from_pmf(pmf_posterior, self.data.baseline_pmf)\n",
    "        result['bias'] = self.data.eval_metrics._evaluate_fdc_metrics_from_pmf(pmf_posterior, pmf)\n",
    "        return result\n",
    "\n",
    "\n",
    "    def run_estimators(self):\n",
    "        # met_forcing = self._load_LSTM_forcing_file()  # Load LSTM forcing data\n",
    "        results = {}\n",
    "        for ensemble_type in ['time', 'frequency']:\n",
    "            print(f'     Processing {ensemble_type} ensemble for {self.target_stn}')\n",
    "            result = self._compute_ensemble_distribution_estimate(ensemble_type)\n",
    "            results[ensemble_type] = result\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0ecf64f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Using all stations in the catchment data with a baseline PMF (validated): 1097\n",
      "    ...overlap dict loaded from data/record_overlap_dict.json\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "target_cols = [\n",
    "    'mean_uar', 'sd_uar', \n",
    "    'mean_logx', 'sd_logx', \n",
    "]\n",
    "\n",
    "# from utils import FDCEstimationContext\n",
    "attr_df_fpath = os.path.join('data', f'catchment_attributes_with_runoff_stats.csv')\n",
    "LSTM_forcings_folder = '/home/danbot/neuralhydrology/data/BCUB_catchment_mean_met_forcings_20250320'\n",
    "LSTM_ensemble_result_folder = '/home/danbot/code/neuralhydrology/data/ensemble_results_20250514'\n",
    "baseline_distribution_folder = os.path.join('data', 'results', 'baseline_distributions')\n",
    "# parameter_prediction_results_folder = os.path.join('data', 'parameter_prediction_results')\n",
    "\n",
    "methods = ('parametric', 'lstm', 'knn',)\n",
    "methods = ('knn',)\n",
    "exclude_pre_1980_data = False  # use only stations with data 1980-present concurrent with Daymet\n",
    "daymet_start_date = '1950-01-01'  # default start date for Daymet data\n",
    "k_nearest = 10\n",
    "if exclude_pre_1980_data:\n",
    "    daymet_start_date = '1980-01-01'\n",
    "\n",
    "processed = []\n",
    "ESTIMATOR_CLASSES = {\n",
    "    'parametric': ParametricFDCEstimator,\n",
    "    'lstm': LSTMFDCEstimator,\n",
    "    'knn': kNNEstimator,\n",
    "    # add others here\n",
    "}\n",
    "input_data = {\n",
    "    'attr_df_fpath': attr_df_fpath,\n",
    "    'LSTM_forcings_folder': LSTM_forcings_folder,\n",
    "    'LSTM_ensemble_result_folder': LSTM_ensemble_result_folder,\n",
    "    'LSTM_concurrent_network': exclude_pre_1980_data,  # use only stations with data 1980-present concurrent with Daymet\n",
    "    'daymet_start_date': daymet_start_date,\n",
    "    # 'parameter_prediction_results_folder': parameter_prediction_results_folder,\n",
    "    'predicted_param_dict': predicted_param_dict,\n",
    "    'divergence_measures': ['DKL', 'EMD'],\n",
    "    'baseline_pmf_stations': pmf_stations,\n",
    "    'eps': 1e-12,\n",
    "    'min_flow': 1e-4,\n",
    "    'n_grid_points': 2**12,\n",
    "    'min_record_length': 5,\n",
    "    'minimum_days_per_month': 20,\n",
    "    'parametric_target_cols': target_cols,\n",
    "    'all_official_ids': station_ids,\n",
    "    'daymet_concurrent_stations': daymet_concurrent_stations,\n",
    "    'baseline_distribution_folder': baseline_distribution_folder,\n",
    "    'prior_strength': 1e-2,  # prior strength for the Laplace fit\n",
    "}\n",
    "\n",
    "context = FDCEstimationContext(**input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25d6acab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing FDCs...\n",
      "Estimating FDC for 05AA008...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 10.58s for temporal ensemble, 0.99s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 5.76s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 9.93s for temporal ensemble, 0.32s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 7.92s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "Estimating FDC for 05AA022...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 9.87s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 6.45s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 9.28s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 6.34s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "Estimating FDC for 05AA023...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 9.33s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 6.99s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 9.57s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 8.17s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "Estimating FDC for 05AA035...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 11.22s for temporal ensemble, 0.33s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 6.89s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 8.27s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 7.56s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "Estimating FDC for 05AD003...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 9.91s for temporal ensemble, 0.32s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 4.65s for temporal ensemble, 0.32s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 10.31s for temporal ensemble, 0.32s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 8.00s for temporal ensemble, 0.80s for frequency ensemble.\n",
      "Estimating FDC for 05BA001...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 10.12s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 6.83s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 10.23s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 7.08s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "Estimating FDC for 05BB001...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 10.48s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 6.34s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 9.41s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 6.64s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "Estimating FDC for 05BF016...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 9.21s for temporal ensemble, 0.28s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 5.13s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 10.99s for temporal ensemble, 0.28s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 5.70s for temporal ensemble, 0.28s for frequency ensemble.\n",
      "Estimating FDC for 05BF017...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 10.10s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 7.42s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 9.58s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 4.45s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "Estimating FDC for 05BF018...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 8.91s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 5.50s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 9.34s for temporal ensemble, 0.28s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 2.80s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "Processed 10/1097 stations in 40.55 seconds per station\n",
      "Estimating FDC for 05BF019...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 6.84s for temporal ensemble, 0.28s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 4.52s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 10.34s for temporal ensemble, 0.28s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 6.88s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "Estimating FDC for 05BG006...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 11.90s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 7.65s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 9.16s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 8.02s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "Estimating FDC for 05BH015...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 9.91s for temporal ensemble, 0.28s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 6.78s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 9.91s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 7.94s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "Estimating FDC for 05BJ004...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 10.64s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 7.97s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 10.62s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 7.89s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "Estimating FDC for 05BJ010...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 9.83s for temporal ensemble, 0.28s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 8.12s for temporal ensemble, 0.28s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 8.86s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 7.31s for temporal ensemble, 1.30s for frequency ensemble.\n",
      "Estimating FDC for 05BL012...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 10.42s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 6.96s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 9.28s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 5.79s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "Estimating FDC for 05BL014...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 8.89s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 7.65s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 10.72s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 6.39s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "Estimating FDC for 05BL022...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 9.67s for temporal ensemble, 0.28s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 8.39s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 11.73s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 7.09s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "Estimating FDC for 05CA009...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 10.19s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 5.96s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 9.72s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 7.16s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "Estimating FDC for 05CB001...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 11.30s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 8.01s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 10.59s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 10.32s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "Processed 20/1097 stations in 41.69 seconds per station\n",
      "Estimating FDC for 05CB004...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 11.59s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 7.79s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 10.63s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 6.31s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "Estimating FDC for 05CC001...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 10.25s for temporal ensemble, 0.28s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 8.13s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 10.00s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 6.97s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "Estimating FDC for 05CC007...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 10.76s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 9.22s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 10.64s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 8.07s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "Estimating FDC for 05DA007...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 10.21s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 7.13s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 10.67s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 8.65s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "Estimating FDC for 05DA009...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 10.50s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 9.10s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 11.77s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 6.87s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "Estimating FDC for 05DA010...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 8.54s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 6.92s for temporal ensemble, 1.88s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 11.05s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 8.31s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "Estimating FDC for 05DB002...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 10.72s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 8.15s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 9.75s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 7.28s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "Estimating FDC for 05DB006...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 10.23s for temporal ensemble, 0.28s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 5.66s for temporal ensemble, 0.33s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 10.59s for temporal ensemble, 0.34s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 8.90s for temporal ensemble, 0.33s for frequency ensemble.\n",
      "Estimating FDC for 05DC006...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 10.69s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 8.37s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 10.91s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 10.70s for temporal ensemble, 0.33s for frequency ensemble.\n",
      "Estimating FDC for 05DC012...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 12.09s for temporal ensemble, 0.32s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 8.82s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 10.92s for temporal ensemble, 0.32s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 7.51s for temporal ensemble, 0.34s for frequency ensemble.\n",
      "Processed 30/1097 stations in 42.90 seconds per station\n",
      "Estimating FDC for 05DD009...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 11.52s for temporal ensemble, 0.28s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 8.38s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 7.84s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 7.96s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "Estimating FDC for 07AA001...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 11.61s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 4.41s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 10.20s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 7.15s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "Estimating FDC for 07AA002...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 9.88s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 6.07s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 13.10s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 7.46s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "Estimating FDC for 07AD002...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 9.71s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 7.96s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 9.54s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 7.66s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "Estimating FDC for 07AF002...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 11.73s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 6.41s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 12.41s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 8.51s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "Estimating FDC for 07AG003...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 12.25s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 6.20s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 10.48s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 5.79s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "Estimating FDC for 07AG007...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 10.80s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 8.21s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 10.56s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 10.02s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "Estimating FDC for 07BB002...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 10.74s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 6.01s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 11.70s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 9.36s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "Estimating FDC for 07BB003...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 7.91s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 6.60s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 11.66s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 9.18s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "Estimating FDC for 07BC002...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 8.67s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 7.25s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 9.32s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 8.47s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "Processed 40/1097 stations in 43.17 seconds per station\n",
      "Estimating FDC for 07EA002...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 12.37s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 9.02s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 11.04s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 8.28s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "Estimating FDC for 07EA004...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 11.59s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 11.08s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 11.07s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 5.43s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "Estimating FDC for 07EA005...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 10.78s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 5.60s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 11.35s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 9.33s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "Estimating FDC for 07EA007...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 11.21s for temporal ensemble, 0.33s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 9.21s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 10.77s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 5.66s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "Estimating FDC for 07EB002...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 10.88s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 5.86s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 8.43s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 7.01s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "Estimating FDC for 07EC002...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 13.01s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 7.40s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 11.42s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 9.86s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "Estimating FDC for 07EC003...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 10.74s for temporal ensemble, 0.32s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 12.27s for temporal ensemble, 0.32s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 9.56s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 7.89s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "Estimating FDC for 07EC004...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 11.06s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 6.85s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 8.36s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 5.69s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "Estimating FDC for 07ED001...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 12.30s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 9.35s for temporal ensemble, 0.32s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 11.41s for temporal ensemble, 0.33s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 8.05s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "Estimating FDC for 07ED003...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 12.44s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 10.32s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 11.68s for temporal ensemble, 0.32s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 7.74s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "Processed 50/1097 stations in 43.91 seconds per station\n",
      "Estimating FDC for 07EE007...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 12.77s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 8.23s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 10.64s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 7.72s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "Estimating FDC for 07EE009...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 11.76s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 9.78s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 15.11s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 9.82s for temporal ensemble, 0.32s for frequency ensemble.\n",
      "Estimating FDC for 07EE010...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 12.27s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 10.34s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 12.70s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 10.94s for temporal ensemble, 0.32s for frequency ensemble.\n",
      "Estimating FDC for 07EF004...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 13.63s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 9.69s for temporal ensemble, 0.32s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 14.02s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 10.62s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "Estimating FDC for 07FA003...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 13.08s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 6.11s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 9.23s for temporal ensemble, 0.34s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 8.78s for temporal ensemble, 0.32s for frequency ensemble.\n",
      "Estimating FDC for 07FA005...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 11.17s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 7.83s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 13.05s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 6.12s for temporal ensemble, 0.33s for frequency ensemble.\n",
      "Estimating FDC for 07FA006...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 11.10s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 9.55s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 12.76s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 11.52s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "Estimating FDC for 07FB001...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 13.99s for temporal ensemble, 0.30s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 9.10s for temporal ensemble, 0.38s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 13.85s for temporal ensemble, 0.37s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 11.24s for temporal ensemble, 0.35s for frequency ensemble.\n",
      "Estimating FDC for 07FB002...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n",
      "    ...spatial_dist ID1 took 13.12s for temporal ensemble, 0.29s for frequency ensemble.\n",
      "    ...spatial_dist ID2 took 7.83s for temporal ensemble, 0.31s for frequency ensemble.\n",
      "    ...attribute_dist ID1 took 13.61s for temporal ensemble, 0.32s for frequency ensemble.\n",
      "    ...attribute_dist ID2 took 7.62s for temporal ensemble, 0.33s for frequency ensemble.\n",
      "Estimating FDC for 07FB003...\n",
      "    ...initializing nearest neighbours with minimum concurrent record.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mEstimating FDC for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     11\u001b[39m runner = FDCEstimatorRunner(stn, context, methods, k_nearest, target_cols, ESTIMATOR_CLASSES)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_selected\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m processed.append(stn)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(processed) % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mFDCEstimatorRunner.run_selected\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     64\u001b[39m     estimator = EstimatorClass(\n\u001b[32m     65\u001b[39m         \u001b[38;5;28mself\u001b[39m.ctx, \u001b[38;5;28mself\u001b[39m.stn_id, \u001b[38;5;28mself\u001b[39m.data\n\u001b[32m     66\u001b[39m     )\n\u001b[32m     67\u001b[39m     \u001b[38;5;28mself\u001b[39m.data.eval_metrics = eval_metrics\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     result = \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_estimators\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m     \u001b[38;5;28mself\u001b[39m._save_result(result)\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/distribution_estimation/docs/notebooks/utils/knn_estimator.py:451\u001b[39m, in \u001b[36mkNNEstimator.run_estimators\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_estimators\u001b[39m(\u001b[38;5;28mself\u001b[39m):              \n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize_nearest_neighbour_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    452\u001b[39m     \u001b[38;5;66;03m# set the baseline pdf by kde\u001b[39;00m\n\u001b[32m    453\u001b[39m     \u001b[38;5;28mself\u001b[39m.target_kde = KDEEstimator(\u001b[38;5;28mself\u001b[39m.data.baseline_log_grid, \u001b[38;5;28mself\u001b[39m.data.log_dx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/distribution_estimation/docs/notebooks/utils/knn_estimator.py:157\u001b[39m, in \u001b[36mkNNEstimator._initialize_nearest_neighbour_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28mself\u001b[39m.nbr_dfs = defaultdict(\u001b[38;5;28;01mlambda\u001b[39;00m: defaultdict(\u001b[38;5;28mdict\u001b[39m))\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tree_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mspatial_dist\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mattribute_dist\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m     nbr_df, nbr_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retrieve_nearest_nbr_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m     effective_k = \u001b[38;5;28mself\u001b[39m._compute_effective_k(nbr_df, max_k=\u001b[38;5;28mself\u001b[39m.k_nearest)\n\u001b[32m    159\u001b[39m     \u001b[38;5;28mself\u001b[39m.nbr_dfs[tree_type] = {\n\u001b[32m    160\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mnbr_df\u001b[39m\u001b[33m'\u001b[39m: nbr_df,\n\u001b[32m    161\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mnbr_data\u001b[39m\u001b[33m'\u001b[39m: nbr_data,\n\u001b[32m    162\u001b[39m         \u001b[33m'\u001b[39m\u001b[33meffective_k\u001b[39m\u001b[33m'\u001b[39m: effective_k,\n\u001b[32m    163\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/distribution_estimation/docs/notebooks/utils/knn_estimator.py:121\u001b[39m, in \u001b[36mkNNEstimator._retrieve_nearest_nbr_data\u001b[39m\u001b[34m(self, tree_type)\u001b[39m\n\u001b[32m    118\u001b[39m sorted_nbrs = \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mzip\u001b[39m(nbr_ids, distances), key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[32m1\u001b[39m])        \n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (nbr_id, dist) \u001b[38;5;129;01min\u001b[39;00m sorted_nbrs:\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     df = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mretrieve_timeseries_discharge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbr_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(df, pd.DataFrame) \u001b[38;5;129;01mor\u001b[39;00m df.empty:\n\u001b[32m    123\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Skip bad or empty DataFrames\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/distribution_estimation/docs/notebooks/utils/fdc_data.py:71\u001b[39m, in \u001b[36mStationData.retrieve_timeseries_discharge\u001b[39m\u001b[34m(self, stn)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mretrieve_timeseries_discharge\u001b[39m(\u001b[38;5;28mself\u001b[39m, stn):\n\u001b[32m     70\u001b[39m     watershed_id = \u001b[38;5;28mself\u001b[39m.ctx.official_id_dict[stn]\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     df = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdischarge\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43msel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwatershed\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwatershed_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdischarge\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m.reset_index()\n\u001b[32m     72\u001b[39m     df = df.set_index(\u001b[33m'\u001b[39m\u001b[33mtime\u001b[39m\u001b[33m'\u001b[39m)[[\u001b[33m'\u001b[39m\u001b[33mdischarge\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m     73\u001b[39m     \u001b[38;5;66;03m# regardless of whether the INPUT data is clipped to \u001b[39;00m\n\u001b[32m     74\u001b[39m     \u001b[38;5;66;03m# match the LSTM input data, the target data should be clipped\u001b[39;00m\n\u001b[32m     75\u001b[39m     \u001b[38;5;66;03m# such that the target distribution is held constant\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/data_analysis/lib/python3.12/site-packages/xarray/core/dataarray.py:3963\u001b[39m, in \u001b[36mDataArray.to_dataframe\u001b[39m\u001b[34m(self, name, dim_order)\u001b[39m\n\u001b[32m   3960\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3961\u001b[39m     ordered_dims = ds._normalize_dim_order(dim_order=dim_order)\n\u001b[32m-> \u001b[39m\u001b[32m3963\u001b[39m df = \u001b[43mds\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_to_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mordered_dims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3964\u001b[39m df.columns = [name \u001b[38;5;28;01mif\u001b[39;00m c == unique_name \u001b[38;5;28;01melse\u001b[39;00m c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df.columns]\n\u001b[32m   3965\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/data_analysis/lib/python3.12/site-packages/xarray/core/dataset.py:7087\u001b[39m, in \u001b[36mDataset._to_dataframe\u001b[39m\u001b[34m(self, ordered_dims)\u001b[39m\n\u001b[32m   7081\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxarray\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextension_array\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PandasExtensionArray\n\u001b[32m   7083\u001b[39m columns_in_order = [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.variables \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dims]\n\u001b[32m   7084\u001b[39m non_extension_array_columns = [\n\u001b[32m   7085\u001b[39m     k\n\u001b[32m   7086\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m columns_in_order\n\u001b[32m-> \u001b[39m\u001b[32m7087\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_extension_array_dtype(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m)\n\u001b[32m   7088\u001b[39m ]\n\u001b[32m   7089\u001b[39m extension_array_columns = [\n\u001b[32m   7090\u001b[39m     k\n\u001b[32m   7091\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m columns_in_order\n\u001b[32m   7092\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_extension_array_dtype(\u001b[38;5;28mself\u001b[39m.variables[k].data)\n\u001b[32m   7093\u001b[39m ]\n\u001b[32m   7094\u001b[39m extension_array_columns_different_index = [\n\u001b[32m   7095\u001b[39m     k\n\u001b[32m   7096\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m extension_array_columns\n\u001b[32m   7097\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m.variables[k].dims) != \u001b[38;5;28mset\u001b[39m(ordered_dims.keys())\n\u001b[32m   7098\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/data_analysis/lib/python3.12/site-packages/xarray/core/variable.py:430\u001b[39m, in \u001b[36mVariable.data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    428\u001b[39m     duck_array = \u001b[38;5;28mself\u001b[39m._data.array\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._data, indexing.ExplicitlyIndexed):\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m     duck_array = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_duck_array(\u001b[38;5;28mself\u001b[39m._data):\n\u001b[32m    432\u001b[39m     duck_array = \u001b[38;5;28mself\u001b[39m._data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/data_analysis/lib/python3.12/site-packages/xarray/core/indexing.py:845\u001b[39m, in \u001b[36mMemoryCachedArray.get_duck_array\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m845\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ensure_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    846\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.array.get_duck_array()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/data_analysis/lib/python3.12/site-packages/xarray/core/indexing.py:842\u001b[39m, in \u001b[36mMemoryCachedArray._ensure_cached\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    841\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_ensure_cached\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m842\u001b[39m     \u001b[38;5;28mself\u001b[39m.array = as_indexable(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/data_analysis/lib/python3.12/site-packages/xarray/core/indexing.py:799\u001b[39m, in \u001b[36mCopyOnWriteArray.get_duck_array\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    798\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m799\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/data_analysis/lib/python3.12/site-packages/xarray/core/indexing.py:654\u001b[39m, in \u001b[36mLazilyIndexedArray.get_duck_array\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    650\u001b[39m     array = apply_indexer(\u001b[38;5;28mself\u001b[39m.array, \u001b[38;5;28mself\u001b[39m.key)\n\u001b[32m    651\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    652\u001b[39m     \u001b[38;5;66;03m# If the array is not an ExplicitlyIndexedNDArrayMixin,\u001b[39;00m\n\u001b[32m    653\u001b[39m     \u001b[38;5;66;03m# it may wrap a BackendArray so use its __getitem__\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m654\u001b[39m     array = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;66;03m# self.array[self.key] is now a numpy array when\u001b[39;00m\n\u001b[32m    657\u001b[39m \u001b[38;5;66;03m# self.array is a BackendArray subclass\u001b[39;00m\n\u001b[32m    658\u001b[39m \u001b[38;5;66;03m# and self.key is BasicIndexer((slice(None, None, None),))\u001b[39;00m\n\u001b[32m    659\u001b[39m \u001b[38;5;66;03m# so we need the explicit check for ExplicitlyIndexed\u001b[39;00m\n\u001b[32m    660\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, ExplicitlyIndexed):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/data_analysis/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:103\u001b[39m, in \u001b[36mNetCDF4ArrayWrapper.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mindexing\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexplicit_indexing_adapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexing\u001b[49m\u001b[43m.\u001b[49m\u001b[43mIndexingSupport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mOUTER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/data_analysis/lib/python3.12/site-packages/xarray/core/indexing.py:1023\u001b[39m, in \u001b[36mexplicit_indexing_adapter\u001b[39m\u001b[34m(key, shape, indexing_support, raw_indexing_method)\u001b[39m\n\u001b[32m   1001\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Support explicit indexing by delegating to a raw indexing method.\u001b[39;00m\n\u001b[32m   1002\u001b[39m \n\u001b[32m   1003\u001b[39m \u001b[33;03mOuter and/or vectorized indexers are supported by indexing a second time\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1020\u001b[39m \u001b[33;03mIndexing result, in the form of a duck numpy-array.\u001b[39;00m\n\u001b[32m   1021\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1022\u001b[39m raw_key, numpy_indices = decompose_indexer(key, shape, indexing_support)\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m result = \u001b[43mraw_indexing_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_key\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtuple\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m numpy_indices.tuple:\n\u001b[32m   1025\u001b[39m     \u001b[38;5;66;03m# index the loaded duck array\u001b[39;00m\n\u001b[32m   1026\u001b[39m     indexable = as_indexable(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/data_analysis/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:116\u001b[39m, in \u001b[36mNetCDF4ArrayWrapper._getitem\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.datastore.lock:\n\u001b[32m    115\u001b[39m         original_array = \u001b[38;5;28mself\u001b[39m.get_array(needs_lock=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         array = \u001b[43mgetitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    118\u001b[39m     \u001b[38;5;66;03m# Catch IndexError in netCDF4 and return a more informative\u001b[39;00m\n\u001b[32m    119\u001b[39m     \u001b[38;5;66;03m# error message.  This is most often called when an unsorted\u001b[39;00m\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# indexer is used before the data is loaded from disk.\u001b[39;00m\n\u001b[32m    121\u001b[39m     msg = (\n\u001b[32m    122\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe indexing operation you are attempting to perform \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    123\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mis not valid on netCDF4.Variable object. Try loading \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    124\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myour data into memory first by calling .load().\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    125\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "processed = []\n",
    "t0 = time()\n",
    "process_fdcs = True\n",
    "if process_fdcs:\n",
    "    print('Processing FDCs...')\n",
    "    for stn in [s for s in context.official_ids if s in daymet_concurrent_stations]:\n",
    "        if stn == '12414900': # this station has no data in the LSTM ensemble results\n",
    "            print(f'    ...skipping {stn} due to naming issue.')\n",
    "            continue\n",
    "        print(f'Estimating FDC for {stn}...')\n",
    "        runner = FDCEstimatorRunner(stn, context, methods, k_nearest, target_cols, ESTIMATOR_CLASSES)\n",
    "        runner.run_selected()\n",
    "        processed.append(stn)\n",
    "        if len(processed) % 10 == 0:\n",
    "            t1 = time()\n",
    "            elapsed = t1 - t0\n",
    "            unit_time = elapsed / len(processed)\n",
    "            print(f'Processed {len(processed)}/{len(context.official_ids)} stations in {unit_time:.2f} seconds per station')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d7d309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
