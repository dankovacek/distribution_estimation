{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference Distributions\n",
    "\n",
    "In this notebook we compute the reference distributions for each catchment in the sample to represent the baseline period of record (POR) flow duration curve for validation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"bd7eaeaf-4f4f-4cbf-9290-3d083dc23570\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"bd7eaeaf-4f4f-4cbf-9290-3d083dc23570\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.7.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.7.3.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"bd7eaeaf-4f4f-4cbf-9290-3d083dc23570\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "\n",
    "from scipy.stats import laplace\n",
    "\n",
    "from pathlib import Path\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "import numpy as np\n",
    "output_notebook()\n",
    "\n",
    "import data_processing_functions as dpf\n",
    "BASE_DIR = os.getcwd()\n",
    "\n",
    "# update this to the path where you stored `HYSETS_2023_update_QC_stations.nc`\n",
    "HYSETS_DIR = Path('/home/danbot/code/common_data/HYSETS')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing overview\n",
    "\n",
    "Note that these steps are optional and the end results of these pre-processing steps are provided in the open data repository.\n",
    "\n",
    "### get updated data sources and validate catchment attributes\n",
    "\n",
    "1) Extract catchment attributes using updated catchment geometries where available (optional, updated catchment geometries are saved in `data/BCUB_watershed_bounds_updated.geojson`).\n",
    "2) Process climate indices for HYSETS catchments in the study region (optional, pre-processed attributes are contained in `BCUB_watershed_attributes_updated.csv`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import HYSETS catchment attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the HYSETS attributes data\n",
    "hysets_df = pd.read_csv(HYSETS_DIR / 'HYSETS_watershed_properties.txt', sep=';')\n",
    "ws_id_dict = hysets_df.set_index('Official_ID')['Watershed_ID'].to_dict()\n",
    "da_dict = hysets_df.set_index('Official_ID')['Drainage_Area_km2'].to_dict()\n",
    "official_id_dict = {row['Official_ID']: row['Watershed_ID'] for _, row in hysets_df.iterrows()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the pre-filtered stations within the study region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Found 1024 catchments in the BCUB region with runoff statistics.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danbot/code/data_analysis/lib/python3.12/site-packages/pyogrio/raw.py:198: RuntimeWarning: driver CSV does not support open option DTYPE\n",
      "  return ogr_read(\n"
     ]
    }
   ],
   "source": [
    "# import the BCUB (study) region boundary\n",
    "bcub_df = gpd.read_file(os.path.join('data', f'catchment_attributes_with_runoff_stats.csv'), dtype={'official_id': str})\n",
    "bcub_df['official_id'] = bcub_df['official_id'].astype(str)\n",
    "# map the Hysets watershed IDs to the BCUB watershed IDs\n",
    "# create a dict to map HYSETS watershed IDs to the Official station IDs\n",
    "bcub_df['watershedID'] = bcub_df['official_id'].apply(lambda x: official_id_dict.get(x, None))\n",
    "print(f'   Found {len(bcub_df)} catchments in the BCUB region with runoff statistics.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import streamflow timeseries\n",
    "\n",
    "\n",
    "```{note}\n",
    "At the top of `data_processing_functions.py`, update the `STREAMFLOW_DIR` variable to match where the HYSETS streamflow time series are stored.  \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "streamflow = xr.open_dataset(HYSETS_DIR / 'HYSETS_2023_update_QC_stations.nc')\n",
    "\n",
    "# Promote 'watershedID' to a coordinate on 'watershed'\n",
    "streamflow = streamflow.assign_coords(watershedID=(\"watershed\", streamflow[\"watershedID\"].data))\n",
    "\n",
    "# Set 'watershedID' as index\n",
    "streamflow = streamflow.set_index(watershed=\"watershedID\")\n",
    "\n",
    "# Select only watershedIDs present in bcub_df\n",
    "valid_ids = [int(wid) for wid in bcub_df['watershedID'].values if wid in streamflow.watershed.values]\n",
    "ds = streamflow.sel(watershed=valid_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_timeseries_discharge(stn):\n",
    "    watershed_id = official_id_dict[stn]\n",
    "    da = da_dict[stn]\n",
    "    try:\n",
    "        df = ds['discharge'].sel(watershed=str(watershed_id)).to_dataframe(name='discharge').reset_index()\n",
    "    except KeyError:\n",
    "        print(f\"Warning: Station {stn} not found in dataset under watershedID {watershed_id}.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = df.set_index('time')[['discharge']]\n",
    "    df.dropna(inplace=True)\n",
    "    df['discharge'] = np.clip(df['discharge'], 1e-4, None)\n",
    "    df.rename(columns={'discharge': stn}, inplace=True)\n",
    "    df[f'{stn}_uar'] = 1000 * df[stn] / da\n",
    "    df[f'{stn}_mm'] = df[stn] * (24 * 3.6 / da)\n",
    "    df['log_x'] = np.log(df[f'{stn}_uar'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USGS station IDs are integers but they are stored in the dataset with the unfortunate characteristic that different stations can have identifiers that are substrings of each other.  We have to add a few extra lines of code to ensure we get the correct file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm all watershed IDs exist in ds\n",
    "bcub_ws_ids = bcub_df['watershedID'].values\n",
    "ds_ids = ds.watershed.values  # After selection via sel(watershed=valid_ids)\n",
    "\n",
    "missing = [wid for wid in bcub_ws_ids if wid not in ds_ids]\n",
    "n_total = len(bcub_ws_ids)\n",
    "n_missing = len(missing)\n",
    "\n",
    "assert n_missing == 0, f\"{n_missing} / {n_total} watershedIDs missing from dataset. First few missing: {missing[:5]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Period of Record (POR) probability distribution functions for each station record\n",
    "\n",
    "In order to compare distributions fairly without data leakage in the subsequent prediction steps, we will first define a \"global evaluation grid\", or the support over which all PDFs will be evaluated.  We'll do this by setting a global expected range of flow on a unit area basis. Since we clipped the lowest flows to 1e-4 $m^3/s$ at import, and we know the minimum drainage area in the dataset is 0.7 $\\text{km}^2$, we can set the minimum to $10^{-4} and find the minimum and maximum unit area runoff in the dataset and leave some margin.  \n",
    "\n",
    "Since we'll be computing KL divergences, we will also assume a prior distribution based on the heavy tailed Laplace distribution using mean and standard deviation (log) unit area runoff predicted (out of sample) from catchment attributes.  This prior will be assumed on the donor/proxy PMF to avoid division by zero where the support gets very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the predicted mean and standard deviation from the previous notebook (Predict Runoff Statistics)\n",
    "predicted_params = pd.read_csv('data/results/parameter_prediction_results/mean_parameter_predictions.csv', index_col=0)\n",
    "mean_param_dict = predicted_params['mean_logx_mean_predicted'].to_dict()\n",
    "sd_param_dict = predicted_params['sd_logx_mean_predicted'].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a global range of expected (unit area) runoff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max streamflow for 05014000: 1342.85 L/s/km²\n",
      "   Min streamflow for 05014000: 6.57e-01 L/s/km² (DA=8.62 km²)\n",
      "Max streamflow for 05014500: 1447.25 L/s/km²\n",
      "   Min streamflow for 05014500: 1.24e-03 L/s/km² (DA=80.81 km²)\n",
      "   Min streamflow for 05BK001: 3.84e-04 L/s/km² (DA=260.51 km²)\n",
      "Max streamflow for 05BL022: 1964.19 L/s/km²\n",
      "   Min streamflow for 05CB001: 3.88e-05 L/s/km² (DA=2578.34 km²)\n",
      "   Min streamflow for 07FC001: 6.43e-06 L/s/km² (DA=15560.60 km²)\n",
      "Max streamflow for 08FF003: 2243.89 L/s/km²\n",
      "Max streamflow for 08FF006: 2783.70 L/s/km²\n",
      "Max streamflow for 08GA065: 3644.79 L/s/km²\n",
      "Max streamflow for 08HA069: 8374.38 L/s/km²\n",
      "\n",
      " Global min = 6.43e-06 L/s/km², max = 8374.38 L/s/km²\n"
     ]
    }
   ],
   "source": [
    "def process_station(stn, da_dict):\n",
    "    df = retrieve_timeseries_discharge(stn)\n",
    "    da = da_dict[stn]\n",
    "    uar = 1000 * df[stn] / da  # L/s/km²\n",
    "    return stn, uar.min(), uar.max()\n",
    "\n",
    "def parallel_min_max(bcub_stations, da_dict, processes=None):\n",
    "    processes = processes or 18\n",
    "    with Pool(processes=processes) as pool:\n",
    "        fn = partial(process_station, da_dict=da_dict)\n",
    "        results = pool.map(fn, bcub_stations)\n",
    "\n",
    "    global_min = float('inf')\n",
    "    global_max = float('-inf')\n",
    "    results = [res for res in results if res[1]]\n",
    "    for stn, min_uar, max_uar in results:\n",
    "        if max_uar > global_max:\n",
    "            global_max = max_uar\n",
    "            print(f'Max streamflow for {stn}: {max_uar:.2f} L/s/km²')\n",
    "        if min_uar < global_min:\n",
    "            global_min = min_uar\n",
    "            print(f'   Min streamflow for {stn}: {min_uar:.2e} L/s/km² (DA={da_dict[stn]:.2f} km²)')\n",
    "\n",
    "    print(f\"\\n Global min = {global_min:.2e} L/s/km², max = {global_max:.2f} L/s/km²\")\n",
    "\n",
    "# Usage\n",
    "parallel_min_max(bcub_df['official_id'], da_dict)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_grid(global_min, global_max, n_grid_points=2**12):\n",
    "    # self.baseline_log_grid = np.linspace(np.log(adjusted_min_uar), np.log(max_uar), self.n_grid_points)\n",
    "    baseline_log_grid = np.linspace(\n",
    "        global_min, global_max, n_grid_points\n",
    "    )\n",
    "    baseline_lin_grid = np.exp(baseline_log_grid)\n",
    "    log_dx = np.gradient(baseline_log_grid)\n",
    "    max_step_size = baseline_lin_grid[-1] - baseline_lin_grid[-2]\n",
    "    print(f'    max step size = {max_step_size:.1f} L/s/km^2 for n={n_grid_points:.1e} grid points')\n",
    "    min_step_size = baseline_lin_grid[1] - baseline_lin_grid[0]\n",
    "    print(f'    min step size = {min_step_size:.2e} L/s/km^2 for n={n_grid_points:.1e} grid points')\n",
    "    return baseline_lin_grid, baseline_log_grid, log_dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    max step size = 61.7 L/s/km^2 for n=4.1e+03 grid points\n",
      "    min step size = 6.20e-10 L/s/km^2 for n=4.1e+03 grid points\n"
     ]
    }
   ],
   "source": [
    "# get the maximum streamflow from the streamflow dataset\n",
    "# max_streamflow = ds['discharge'].max().values.item()\n",
    "# max_streamflow = # it's actually 19400 in the dataset\n",
    "baseline_lin_grid, baseline_log_grid, log_dx = set_grid(np.log(1e-7), np.log(1e4), n_grid_points=2**12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prior_from_laplace_fit(predicted_uar, loc, scale, target_da, baseline_log_grid, log_dx, n_cols=1, scale_factor=1.05, recursion_depth=0, max_depth=100):\n",
    "    \"\"\"\n",
    "    Fit a Laplace distribution to the simulation and define a \n",
    "    pdf across a pre-determined \"global\" range to avoid data\n",
    "    leakage.  \"Normalize\" by setting the total prior mass to\n",
    "    integrate to a factor related to the number of observations.\n",
    "\n",
    "    The location of the Laplace distribution is the median, \n",
    "    and the scale is the mean absolute deviation (MAD).\n",
    "    By Jensen's Inequality, the MAD is less than or equal to the standard deviation.\n",
    "    Here we just use the predicted log-mean and log-standard deviation\n",
    "    as an approximation of the Laplace distribution parameters.\n",
    "    \"\"\"\n",
    "    # assert no nan values\n",
    "    assert np.isfinite(predicted_uar).all(), f'NaN values in predicted_uar: {predicted_uar}'\n",
    "    # assert all positive values\n",
    "    # assert np.all(predicted_uar > 0), f'Negative values in predicted_uar: {np.min(predicted_uar)}'\n",
    "    # replace anything <= 0 with 1e-4 scaled by the drainage area\n",
    "    predicted_uar = np.where(predicted_uar <= 0, 1000 * 1e-4 / target_da, predicted_uar)\n",
    "    assert np.all(predicted_uar > 0), f'Negative values in predicted_uar: {np.min(predicted_uar)}'\n",
    "    # print('min/max: ', np.min(predicted_uar), np.max(predicted_uar))\n",
    "    # loc, scale = laplace.fit(np.log(predicted_uar))\n",
    "\n",
    "    # Apply scale factor in case of recursion\n",
    "    if scale <= 0:\n",
    "        original_scale = scale\n",
    "        scale = scale_factor ** recursion_depth\n",
    "        print(f'   Adjusting scale from {original_scale:.3f} to {scale:.3f} for recursion depth {recursion_depth}')\n",
    "\n",
    "    prior_pdf = laplace.pdf(baseline_log_grid, loc=loc, scale=scale)\n",
    "    prior_check = np.trapezoid(prior_pdf, x=baseline_log_grid)\n",
    "    prior_pdf /= prior_check\n",
    "\n",
    "    # Check for zeros\n",
    "    if np.any(prior_pdf == 0) | np.any(np.isnan(prior_pdf)):\n",
    "        # Prevent scale from being too small\n",
    "        if recursion_depth >= max_depth:\n",
    "            # set a very small prior\n",
    "            prior_pdf = np.ones_like(baseline_log_grid)\n",
    "            err_msg = f\"Recursion limit reached. Scale={scale}, setting default prior to 1 pseudo-count uniform distribution.\"\n",
    "            print(err_msg)\n",
    "            return prior_pdf\n",
    "            # raise ValueError(err_msg)\n",
    "        # print(f\"Recursion {recursion_depth}: Zero values detected. Increasing scale to {scale:.6f}\")\n",
    "        return compute_prior_from_laplace_fit(predicted_uar, target_da, baseline_log_grid, log_dx, n_cols=n_cols, scale_factor=scale_factor, recursion_depth=recursion_depth + 1)\n",
    "\n",
    "    second_check = np.trapezoid(prior_pdf, x=baseline_log_grid)\n",
    "    assert np.isclose(second_check, 1, atol=2e-4), f'prior check != 1, {second_check:.6f} N={len(predicted_uar)} {predicted_uar}'\n",
    "    assert np.min(prior_pdf) > 0, f'min prior == 0, scale={scale:.5f}'\n",
    "\n",
    "    # convert prior PDF to PMF (pseudo-count mass function)\n",
    "    prior_pmf = prior_pdf * log_dx\n",
    "\n",
    "    # scale the number of pseudo-counts based on years of record  (365 / n_observations)\n",
    "    # and number of models in the ensemble (given by n_cols)\n",
    "    prior_pseudo_counts = prior_pmf * (365 / (len(predicted_uar) * n_cols))\n",
    "    \n",
    "    # return weighted_prior_pdf\n",
    "    return prior_pseudo_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(u):\n",
    "    return np.exp(-0.5 * u**2) / np.sqrt(2 * np.pi)\n",
    "\n",
    "def measurement_error_bandwidth_function(x):\n",
    "    error_points = np.array([1e-4, 1e-3, 1e-2, 1e-1, 1., 1e1, 1e2, 1e3, 1e4, 1e5])\n",
    "    error_values = np.array([1.0, 0.5, 0.2, 0.1, 0.1, 0.1, 0.1, 0.15, 0.2, 0.25])\n",
    "    return np.interp(x, error_points, error_values, left=1.0, right=0.25)\n",
    "\n",
    "\n",
    "def adaptive_bandwidths(uar, da):\n",
    "    flow_data = uar * da / 1000\n",
    "    unique_q = np.unique(flow_data)\n",
    "    error_model = measurement_error_bandwidth_function(unique_q)\n",
    "    unique_UAR = (1000 / da) * unique_q\n",
    "    upper_err_UAR = unique_UAR * (1 + error_model)\n",
    "    err_widths_UAR = np.log(upper_err_UAR) - np.log(unique_UAR)\n",
    "\n",
    "    if len(unique_UAR) < 2:\n",
    "        noise_bounds = (unique_UAR * (1 - error_model), unique_UAR * (1 + error_model))\n",
    "        flow_data += np.random.uniform(*noise_bounds, size=flow_data.shape)\n",
    "        unique_q = np.unique(flow_data)\n",
    "        unique_UAR = (1000 / da) * unique_q\n",
    "\n",
    "    log_midpoints = np.log((unique_UAR[:-1] + unique_UAR[1:]) / 2)\n",
    "    left = unique_UAR[0] - (log_midpoints[0] - unique_UAR[0])\n",
    "    right = unique_UAR[-1] + (unique_UAR[-1] - log_midpoints[-1])\n",
    "    log_midpoints = np.concatenate(([left], log_midpoints, [right]))\n",
    "    log_diffs = np.diff(log_midpoints) / 2 / 1.15\n",
    "\n",
    "    # use the error-based bandwidth except where the log difference \n",
    "    # between unique values is greater \n",
    "    bw_vals = np.where(log_diffs > err_widths_UAR, log_diffs, err_widths_UAR)\n",
    "    idx = np.searchsorted(unique_UAR, uar, side='right') - 1\n",
    "    return bw_vals[idx]\n",
    "\n",
    "\n",
    "class KDEEstimator:\n",
    "    def __init__(self, log_grid, dx, mean_param_dict, sd_param_dict, set_global_prior=False):\n",
    "        self.log_grid = np.asarray(log_grid, dtype=np.float32)\n",
    "        self.dx = np.asarray(dx, dtype=np.float32)\n",
    "        self.mean_param_dict = mean_param_dict\n",
    "        self.sd_param_dict = sd_param_dict\n",
    "        self.set_prior_from_laplace_fit = set_global_prior\n",
    "\n",
    "    def compute(self, stn, uar_data, da, n_years, ):\n",
    "        uar_data = np.asarray(uar_data, dtype=np.float32)\n",
    "        bw_values = adaptive_bandwidths(uar_data, da)\n",
    "        log_data = np.log(uar_data)\n",
    "\n",
    "        H = bw_values[:, None]\n",
    "        U = (self.log_grid[None, :] - log_data[:, None]) / H\n",
    "        K = gaussian_kernel(U) / H\n",
    "\n",
    "        pdf = K.sum(axis=0) / len(log_data)\n",
    "        pdf /= np.trapezoid(pdf, x=self.log_grid)\n",
    "        pmf = pdf * self.dx\n",
    "        pmf /= np.sum(pmf)\n",
    "\n",
    "        mu, sd = self.mean_param_dict[stn], self.sd_param_dict[stn]\n",
    "        posterior_pmf = pmf\n",
    "        posterior_pdf = pdf\n",
    "        if self.set_prior_from_laplace_fit:\n",
    "            # Compute prior counts from Laplace fit\n",
    "            # This is a global prior based on the mean and standard deviation of the log-mean\n",
    "            # and log-standard deviation of the UAR data.\n",
    "            # The prior is computed across a pre-defined \"global\" range to avoid data leakage.\n",
    "            # The prior counts are scaled by the number of observations and the number of columns in the ensemble.\n",
    "            laplace_prior_counts = compute_prior_from_laplace_fit(uar_data, mu, sd, da, self.log_grid, self.dx)\n",
    "            posterior_counts = pmf * len(uar_data) + (1.0 /  n_years) * laplace_prior_counts\n",
    "            posterior_pmf = posterior_counts / np.sum(posterior_counts)\n",
    "            posterior_pdf = posterior_pmf / self.dx\n",
    "\n",
    "        return (posterior_pmf, posterior_pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering stations with at least 5 complete years of data: 1024/1024 stations.\n"
     ]
    }
   ],
   "source": [
    "# set the minimum record length\n",
    "min_record_years = 5\n",
    "# choose if you want to set a global prior distribution based on the \n",
    "# Laplace fit to (out of sample) predicted location and scale\n",
    "set_global_prior = False\n",
    "validated_stations = bcub_df[bcub_df['n_complete_years'].astype(float) >= min_record_years]['official_id'].values\n",
    "print(f'Filtering stations with at least {min_record_years} complete years of data: {len(validated_stations)}/{len(bcub_df)} stations.')\n",
    "\n",
    "# theres a problem with finding '0212414900' in the data, drop it\n",
    "validated_stations = [stn for stn in validated_stations if stn != '0212414900']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024 stations with at least 2 complete years of data also in the predicted parameter dict.\n",
      "1024 stations meeting minimum complete period of record.\n"
     ]
    }
   ],
   "source": [
    "# from kde_estimator import KDEEstimator\n",
    "pdf_path = Path(os.getcwd()) / 'data' / 'results' / 'baseline_distributions' / f'bcub_pdfs.csv'\n",
    "pmf_path = Path(os.getcwd()) / 'data' / 'results' / 'baseline_distributions' / f'bcub_pmfs.csv'\n",
    "\n",
    "if os.path.exists(pdf_path):\n",
    "    pmf_df = pd.read_csv(pmf_path, index_col='q')\n",
    "    pdf_df = pd.read_csv(pdf_path, index_col='q')\n",
    "else:\n",
    "    # compute the PDF and PMF for each station\n",
    "    log_dx = np.gradient(baseline_log_grid)\n",
    "    print(len(validated_stations), 'stations with at least 2 complete years of data also in the predicted parameter dict.')\n",
    "    validated_stations = [stn for stn in validated_stations if stn in mean_param_dict and stn in sd_param_dict]\n",
    "    assert np.all([stn in mean_param_dict for stn in validated_stations]), \"Not all stations have predicted mean parameters.\"\n",
    "    assert np.all([stn in sd_param_dict for stn in validated_stations]), \"Not all stations have predicted standard deviation parameters.\"\n",
    "    estimator = KDEEstimator(baseline_log_grid, log_dx, mean_param_dict, sd_param_dict)\n",
    "\n",
    "    def compute_pdf_and_pmf_from_kde(input_tuple):\n",
    "        stn, n_years = input_tuple\n",
    "        df = retrieve_timeseries_discharge(stn)\n",
    "        # convert the streamflow to L/s/km²\n",
    "        df[f'{stn}_uar'] = 1000 * df[stn] / da_dict[stn]\n",
    "        \n",
    "        # compute the PDF and PMF\n",
    "        pmf, pdf = estimator.compute(stn, df[f'{stn}_uar'].values, da_dict[stn], n_years)\n",
    "        return (stn, pmf, pdf)\n",
    "    \n",
    "    print(len(validated_stations), 'stations meeting minimum complete period of record.')\n",
    "    years_by_stn = [len(complete_year_dict[stn]) for stn in validated_stations]\n",
    "    inputs = list(zip(validated_stations, years_by_stn))\n",
    "    with Pool(14) as pool:\n",
    "        results = pool.map(compute_pdf_and_pmf_from_kde, inputs)\n",
    "\n",
    "    # concatenate the results\n",
    "    stations, pmfs, pdfs = zip(*results)\n",
    "    pdf_df = pd.DataFrame(np.matrix(pdfs).T, columns=stations)\n",
    "    pmf_df = pd.DataFrame(np.matrix(pmfs).T, columns=stations)\n",
    "    pdf_df['q'] = baseline_lin_grid\n",
    "    pmf_df['q'] = baseline_lin_grid\n",
    "\n",
    "    # save the pdf and pmf files\n",
    "    pdf_df.set_index('q', inplace=True)\n",
    "    pmf_df.set_index('q', inplace=True)\n",
    "    pdf_df.to_csv(pdf_path)\n",
    "    pmf_df.to_csv(pmf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the entropy of each PMF\n",
    "\n",
    "```{note}\n",
    "The remainder of this notebook is not part of the analysis presented in the accompanying paper.\n",
    "```\n",
    "\n",
    "The entropy of the distribution represents the randomness or disorder of the system, and it is given by the sum of log probabilities of the system states:\n",
    "\n",
    "$$H(X) = \\sum_{i=1}^N -\\log p_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the entropy of the posterior distribution for each station\n",
    "bits = list(range(2, 13)) # set a range that is both too low and too high for the data\n",
    "entropy_output_folder = Path(os.getcwd()) / 'data' / 'results' / 'entropy_results'\n",
    "if not entropy_output_folder.exists():\n",
    "    entropy_output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "states = [2**b for b in bits]\n",
    "eps = 1e-12 # set a small epsilon to avoid numerical issues\n",
    "for s in states:\n",
    "    q_values = pmf_df.index.values\n",
    "    # resample the PMF by q_values to the number of states\n",
    "    resampled_q = np.linspace(np.log(q_values.min()-eps), np.log(q_values.max()+eps), s)\n",
    "    resampled_q = np.exp(resampled_q)  # convert back to original scale\n",
    "    \n",
    "    # create a new DataFrame with the resampled PMF\n",
    "    pmf_resampled = pd.DataFrame(index=resampled_q, columns=pmf_df.columns)\n",
    "    n = 0\n",
    "    for stn in pmf_df.columns:\n",
    "        n += 1\n",
    "        pmf = pmf_df[stn].values\n",
    "        bin_numbers = np.digitize(q_values, resampled_q)\n",
    "        df = pd.DataFrame({'q': q_values, 'pmf': pmf, 'bin': bin_numbers})\n",
    "        df = df.groupby('bin').sum().reset_index()\n",
    "        pmf_resampled[stn] = df['pmf'].values\n",
    "        assert np.isclose(df['pmf'].sum(), 1), f'PMF for {stn} does not sum to 1: {np.sum(pmf):.6f}'\n",
    "    bits = int(np.log2(s))\n",
    "    pmf_resampled.index.name = 'q'\n",
    "    pmf_resampled.to_csv(entropy_output_folder / f'bcub_pmf_resampled_{bits}bits.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.palettes import RdYlGn\n",
    "\n",
    "def logspace_edges_from_linear_centers(centers):\n",
    "    \"\"\"Given linear-space bin centers from a log-uniform KDE, return log-space edges in linear space.\"\"\"\n",
    "    log_centers = np.log(centers)\n",
    "    dlog = log_centers[1] - log_centers[0]\n",
    "    log_edges = np.concatenate([\n",
    "        [log_centers[0] - dlog / 2],\n",
    "        log_centers + dlog / 2\n",
    "    ])\n",
    "    return np.exp(log_edges)  # return edges in linear space\n",
    "\n",
    "\n",
    "pmf_fig = figure(title=\"BCUB PMF Resampled\", width=750, height=400, x_axis_type='log')\n",
    "entropy_distributions = {}\n",
    "for s in states:\n",
    "    bits = int(np.log2(s))\n",
    "    pmf_resampled = pd.read_csv(entropy_output_folder / f'bcub_pmf_resampled_{bits}bits.csv', index_col=0)\n",
    "    pmf = np.percentile(pmf_resampled, 50, axis=1)\n",
    "\n",
    "    # get the percentile value corresponding to the mean PMF over all rows\n",
    "    mean_pmf = np.mean(pmf_resampled, axis=1)\n",
    "    mean_percentile = np.percentile(mean_pmf, 50)\n",
    "\n",
    "    linear_centers = pmf_resampled.index.astype(float).values\n",
    "    edges = logspace_edges_from_linear_centers(linear_centers)\n",
    "    dx = np.diff(edges)  # linear bin widths corresponding to log-space bins\n",
    "    pdf = pmf / dx  # convert PMF to PDF\n",
    "\n",
    "    entropy = -np.sum(pmf * np.log2(pmf + 1e-12))  # add a small epsilon to avoid log(0)\n",
    "    \n",
    "    # print(len(dx), len(edges), len(pmf))\n",
    "    # print(asdf)\n",
    "    bits = np.log2(s)\n",
    "    ratio = entropy / bits\n",
    "    # add an edge at the end to close the PMF\n",
    "    \n",
    "    pmf_fig.quad(\n",
    "        top=pdf,\n",
    "        bottom=0,\n",
    "        left=edges[:-1],\n",
    "        right=edges[1:],\n",
    "        fill_color=RdYlGn[11][states.index(s) % len(RdYlGn[11])],\n",
    "        line_color=None,\n",
    "        alpha=0.7,\n",
    "        legend_label=f'{bits:.0f}b (H={entropy:.1f} {100*ratio:.0f}%)'\n",
    "    )\n",
    "    pmf_fig.legend.click_policy = 'hide'\n",
    "    pmf_fig.legend.location = 'top_right'\n",
    "    # pmf_fig.xaxis.axis_label = r'$$\\text{Unit Area Runoff } \\frac{L}{s\\cdot \\text{km}^2}$$'\n",
    "    # pmf_fig.yaxis.axis_label = r'$$\\text{PDF } P(X\\leq x)$$'\n",
    "    pmf_fig.xaxis.axis_label = \"Unit Area Runoff (L/s/km²)\"\n",
    "    pmf_fig.yaxis.axis_label = \"Probability Density\"\n",
    "    pmf_fig = dpf.format_fig_fonts(pmf_fig, font_size=14)\n",
    "show(pmf_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_empirical_cdf(values):\n",
    "    \"\"\"Compute the empirical cumulative distribution function (CDF) of the given values.\"\"\"\n",
    "    sorted_values = np.sort(values)\n",
    "    cdf = np.arange(1, len(sorted_values) + 1) / len(sorted_values)\n",
    "    return sorted_values, cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.palettes import Viridis10\n",
    "# plot the CDFs of entropy by number of bits\n",
    "entropy_dist_fig = figure(title=\"Entropy Distributions by Number of Bits\", width=750, height=400)\n",
    "colours = Viridis10\n",
    "for s in states:\n",
    "    bits = int(np.log2(s))\n",
    "    pmf_resampled = pd.read_csv(entropy_output_folder / f'bcub_pmf_resampled_{bits}bits.csv', index_col=0)\n",
    "    # drop the 'q' column\n",
    "    # pmf = pmf_resampled.drop(columns=['q']).values\n",
    "    # broadcast the computation of entropy across all columns\n",
    "    # to compute the entropy for each column\n",
    "    sample_entropy = -np.sum(pmf_resampled.values * np.log2(pmf_resampled.values), axis=0)  # add a small epsilon to avoid log(0)\n",
    "    mean_entropy = np.mean(sample_entropy)\n",
    "    x, cdf = compute_empirical_cdf(sample_entropy)\n",
    "\n",
    "    entropy_dist_fig.line(x, cdf, line_width=3, color=colours[states.index(s) % len(colours)],\n",
    "                          legend_label=f'{bits:.0f}b (H={mean_entropy:.1f})')\n",
    "entropy_dist_fig.legend.location = 'bottom_right'\n",
    "entropy_dist_fig.legend.background_fill_alpha = 0.5\n",
    "entropy_dist_fig.xaxis.axis_label = \"Entropy (bits)\"\n",
    "entropy_dist_fig.yaxis.axis_label = \"Cumulative Probability\"\n",
    "entropy_dist_fig.legend.click_policy = 'hide'\n",
    "entropy_dist_fig = dpf.format_fig_fonts(entropy_dist_fig, font_size=14)\n",
    "show(entropy_dist_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logspace_edges_from_linear_centers(centers):\n",
    "    \"\"\"Given linear-space bin centers from a log-uniform KDE, return log-space edges in linear space.\"\"\"\n",
    "    log_centers = np.log(centers)\n",
    "    dlog = log_centers[1] - log_centers[0]\n",
    "    log_edges = np.concatenate([\n",
    "        [log_centers[0] - dlog / 2],\n",
    "        log_centers + dlog / 2\n",
    "    ])\n",
    "    return np.exp(log_edges)  # return edges in linear space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.palettes import Viridis10 as colours\n",
    "pmf_fig = figure(title=\" PDF by Percentile\", width=1000, height=400, x_axis_type='log')\n",
    "percentiles = 2, 10, 25, 50, 75, 90, 95, 96, 98, 99\n",
    "for pct in percentiles:\n",
    "    pmf_resampled = pd.read_csv(entropy_output_folder / f'bcub_pmf_resampled_8bits.csv', index_col=0)\n",
    "    pmf = np.percentile(pmf_resampled, pct, axis=1)\n",
    "\n",
    "    linear_centers = pmf_resampled.index.astype(float).values\n",
    "    edges = logspace_edges_from_linear_centers(linear_centers)\n",
    "    dx = np.diff(edges)  # linear bin widths corresponding to log-space bins\n",
    "    pdf = pmf / dx  # convert PMF to PDF\n",
    "    # compute the area under the PDF\n",
    "    area = np.trapezoid(pdf, x=edges[:-1])\n",
    "    pdf = pdf / area  # normalize the PDF to have unit area\n",
    "    area = np.trapezoid(pdf, x=edges[:-1])\n",
    "    assert np.isclose(area, 1), f'Area under PDF for {pct}th percentile does not equal 1: {area:.6f}'\n",
    "    # print(asdf)\n",
    "    mask = pmf > 0\n",
    "    entropy = -np.sum(pmf[mask] * np.log2(pmf[mask]))  \n",
    "    \n",
    "    # print(len(dx), len(edges), len(pmf))\n",
    "    # print(asdf)\n",
    "    bits = np.log2(s)\n",
    "    ratio = entropy / bits\n",
    "    # add an edge at the end to close the PMF\n",
    "    \n",
    "    pmf_fig.line(\n",
    "        x=edges[:-1],\n",
    "        y=pdf,\n",
    "        line_width=4,\n",
    "        color=colours[percentiles.index(pct) % len(colours)],\n",
    "        legend_label=f'{pct}th Percentile (H={entropy:.1f} {100*ratio:.0f}%)'\n",
    "    )\n",
    "    pmf_fig.legend.click_policy = 'hide'\n",
    "    pmf_fig.legend.location = 'top_right'\n",
    "    # pmf_fig.xaxis.axis_label = r'$$\\text{Unit Area Runoff } \\frac{L}{s\\cdot \\text{km}^2}$$'\n",
    "    # pmf_fig.yaxis.axis_label = r'$$\\text{PDF } P(X\\leq x)$$'\n",
    "    pmf_fig.xaxis.axis_label = \"Unit Area Runoff (L/s/km²)\"\n",
    "    pmf_fig.yaxis.axis_label = \"Probability Density\"\n",
    "    pmf_fig.legend.background_fill_alpha = 0.5\n",
    "    pmf_fig.add_layout(pmf_fig.legend[0], 'right')\n",
    "    pmf_fig = dpf.format_fig_fonts(pmf_fig, font_size=14)\n",
    "show(pmf_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import LogColorMapper, ColorBar, FixedTicker, ColumnDataSource, LinearColorMapper\n",
    "from bokeh.palettes import Viridis256\n",
    "\n",
    "percentiles = np.linspace(0.01, 99.9, 500)\n",
    "exceedance_probs = 1 - (percentiles / 100)  # Convert percentiles to exceedance probabilities\n",
    "pmf_resampled = pd.read_csv(entropy_output_folder / 'bcub_pmf_resampled_12bits.csv', index_col=0)\n",
    "assert np.allclose(pmf_resampled.sum(), 1), \"PMF does not sum to 1 across all stations.\"\n",
    "linear_centers = pmf_resampled.index.astype(float).values\n",
    "edges = logspace_edges_from_linear_centers(linear_centers)\n",
    "dx = np.diff(edges)\n",
    "x_vals = edges[:-1]\n",
    "log_x = np.log10(x_vals)\n",
    "\n",
    "z_matrix = []\n",
    "for pct in percentiles:\n",
    "    pmf = np.percentile(pmf_resampled, pct, axis=1)\n",
    "    # pmf /= np.sum(pmf)\n",
    "    z_matrix.append(pmf)\n",
    "\n",
    "z_image = np.flipud(np.array(z_matrix))\n",
    "\n",
    "# Use log color mapping for dynamic range of PDF values\n",
    "mapper = LogColorMapper(palette=Viridis256, low=z_image[z_image > 0].min(), high=z_image.max())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with x-axis in log space (via manual transformation)\n",
    "p = figure(\n",
    "    title=\"PMF across Large Sample of Watersheds\",\n",
    "    # x_range=(log_x.min(), log_x.max()),\n",
    "    x_range=(x_vals.min(), x_vals.max()),\n",
    "    y_range=(0.01, 99.9),\n",
    "    # y_range=(np.min(exceedance_probs), np.max(exceedance_probs)),\n",
    "    width=1000,\n",
    "    height=500,\n",
    "    x_axis_type=\"log\",\n",
    ")\n",
    "# exceedance_probs = 1 - (percentiles / 100)\n",
    "# p.yaxis.axis_label = \"Exceedance Probability\"\n",
    "\n",
    "p.image(\n",
    "    image=[z_image],\n",
    "    x=x_vals.min(),\n",
    "    y=1,\n",
    "    dw=x_vals.max() - x_vals.min(),\n",
    "    dh=98,\n",
    "    color_mapper=mapper\n",
    ")\n",
    "\n",
    "# Axis labels\n",
    "p.xaxis.axis_label = \"Unit Area Runoff (L/s/km²)\"\n",
    "p.yaxis.axis_label = \"Percentile\"\n",
    "\n",
    "# Color bar\n",
    "color_bar = ColorBar(title=\"Probability Mass\",  color_mapper=mapper, label_standoff=12)\n",
    "p.add_layout(color_bar, 'right')\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pmf = pmf_resampled.mean(axis=1)  # Mean PMF per bin\n",
    "\n",
    "# Create a new figure for the mean PDF\n",
    "mean_pdf_fig = figure(title=\"Mean PDF of Unit Area Runoff\", width=1000, height=400, x_axis_type='log')\n",
    "mean_pdf_fig.line(x=x_vals, y=mean_pmf, line_width=2, color='blue', legend_label='Mean PDF')\n",
    "mean_pdf_fig.xaxis.axis_label = \"Unit Area Runoff (L/s/km²)\"\n",
    "mean_pdf_fig.yaxis.axis_label = \"Probability Density\"\n",
    "mean_pdf_fig.legend.location = 'top_right'\n",
    "mean_pdf_fig.legend.click_policy = 'hide'\n",
    "mean_pdf_fig = dpf.format_fig_fonts(mean_pdf_fig, font_size=14)\n",
    "show(mean_pdf_fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "In the subsequent chapters, we'll use the KL divergence computed for each station in a gradient boosting model to test their predictability from catchment attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
